{
  "hash": "0a0e9f758e9056891a2000cd343dd783",
  "result": {
    "engine": "knitr",
    "markdown": "# Probability & Simulation {#sim .incomplete-chapter}\n\n<div class=\"right meme\"><img src=\"images/memes/sim.jpg\"\n     alt=\"Morpheus from The Matrix. Top text: What if I told you; Bottom text: Ur in a simulation, inside a simulation, inside another simulation, inside a taco, inside a taco cat, inside a Taco Bell, inside an Arby's. Inside another simulation\" /></div>\n\n## Intended Learning Outcomes {#sec-ilo-sim - .ilo}\n\n- [ ] Generate and plot data randomly sampled from common distributions \n- [ ] Generate related variables from a multivariate distribution\n- [ ] Define the following statistical terms: [p-value](#p-value), [alpha](#alpha), [power](#power), smallest effect size of interest ([SESOI](#sesoi)), [false positive](#false-pos) (type I error), [false negative](#false-neg) (type II error), confidence interval ([CI](#conf-inf))\n- [ ] Test sampled distributions against a null hypothesis \n- [ ] Calculate power using iteration and a sampling function\n\n\n## Setup {#sec-setup-sim -}\n\n1. Open your `reprores` project \n1. Create a new quarto file called `09-sim.qmd`\n1. Update the YAML header \n1. Replace the setup chunk with the one below: \n\n\n\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#‎| label: setup\n#‎| include: false\nlibrary(tidyverse) # for data wrangling\n# devtools::install_github(\"debruine/faux\") \nlibrary(faux)      # data simulation\nlibrary(plotly)    # create a 3D plot to visualise correlations\n```\n````\n\n::: {.cell-output .cell-output-error}\n\n```\nError in library(plotly): there is no package called 'plotly'\n```\n\n\n:::\n\n````{.cell-code}\n```{{r}}\n# MASS::mvrnorm() is used without loading MASS\n\nset.seed(8675309) # makes sure random numbers are reproducible\n```\n````\n:::\n\n\n\n\nSimulating data is a very powerful way to test your understanding of statistical concepts. We are going to use <a href='https://psyteachr.github.io/glossary/s#simulation' target='_blank' class='glossary' title='Generating data from summary parameters'>simulations</a> to learn the basics of <a href='https://psyteachr.github.io/glossary/p#probability' target='_blank' class='glossary' title='A number between 0 and 1 where 0 indicates impossibility of the event and 1 indicates certainty'>probability</a>.\n\n## Univariate Distributions\n\nFirst, we need to understand some different ways data might be distributed and how to simulate data from these distributions. A <a href='https://psyteachr.github.io/glossary/u#univariate' target='_blank' class='glossary' title='Relating to a single variable.'>univariate</a> distribution is the distribution of a single variable.\n\n### Uniform Distribution {#uniform}\n\nThe <a href='https://psyteachr.github.io/glossary/u#uniform-distribution' target='_blank' class='glossary' title='A distribution where all numbers in the range have an equal probability of being sampled'>uniform distribution</a> is the simplest distribution. All numbers in the range have an equal probability of being sampled.\n\n::: {.try data-latex=\"\"}\nTake a minute to think of things in your own research that are uniformly distributed.\n:::\n\n#### Continuous distribution\n\n`runif(n, min=0, max=1)` \n\nUse `runif()` to sample from a continuous uniform distribution.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu <- runif(100000, min = 0, max = 1)\n\n# plot to visualise\nggplot() + \n  geom_histogram(aes(u), binwidth = 0.05, boundary = 0,\n                 fill = \"white\", colour = \"black\")\n```\n\n::: {.cell-output-display}\n![](images/figures/runif-1.png){width=100%}\n:::\n:::\n\n\n\n\n#### Discrete\n\n`sample(x, size, replace = FALSE, prob = NULL)`\n\nUse `sample()` to sample from a <a href='https://psyteachr.github.io/glossary/d#discrete' target='_blank' class='glossary' title='Data that can only take certain values, such as integers.'>discrete</a> distribution.\n\nYou can use `sample()` to simulate events like rolling dice or choosing from a deck of cards. The code below simulates rolling a 6-sided die 10000 times. We set `replace` to `TRUE` so that each event is independent. See what happens if you set `replace` to `FALSE`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrolls <- sample(1:6, 10000, replace = TRUE)\n\n# plot the results\nggplot() + \n  geom_histogram(aes(rolls), binwidth = 1, \n                 fill = \"white\", color = \"black\")\n```\n\n::: {.cell-output-display}\n![Distribution of dice rolls.](images/figures/sample-replace-1.png){width=100%}\n:::\n:::\n\n\n\n\nYou can also use sample to sample from a list of named outcomes.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npet_types <- c(\"cat\", \"dog\", \"ferret\", \"bird\", \"fish\")\nsample(pet_types, 10, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"ferret\" \"cat\"    \"dog\"    \"fish\"   \"dog\"    \"bird\"   \"fish\"   \"cat\"   \n [9] \"bird\"   \"ferret\"\n```\n\n\n:::\n:::\n\n\n\n\nFerrets are a much less common pet than cats and dogs, so our sample isn't very realistic. You can set the probabilities of each item in the list with the `prob` argument.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npet_types <- c(\"cat\", \"dog\", \"ferret\", \"bird\", \"fish\")\npet_prob <- c(0.3, 0.4, 0.1, 0.1, 0.1)\nsample(pet_types, 10, replace = TRUE, prob = pet_prob)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"cat\"    \"dog\"    \"dog\"    \"cat\"    \"cat\"    \"bird\"   \"bird\"   \"ferret\"\n [9] \"cat\"    \"dog\"   \n```\n\n\n:::\n:::\n\n\n\n\n\n### Binomial Distribution {#binomial}\n\nThe <a href='https://psyteachr.github.io/glossary/b#binomial-distribution' target='_blank' class='glossary' title='The distribution of data where each observation can have one of two outcomes, like success/failure, yes/no or head/tails.'>binomial distribution</a> is useful for modelling binary data, where each observation can have one of two outcomes, like success/failure, yes/no or head/tails. \n\n\n`rbinom(n, size, prob)`\n\nThe `rbinom` function will generate a random binomial distribution.\n\n* `n` = number of observations\n* `size` = number of trials\n* `prob` = probability of success on each trial\n\nCoin flips are a typical example of a binomial distribution, where we can assign heads to 1 and tails to 0.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 20 individual coin flips of a fair coin\nrbinom(20, 1, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 20 individual coin flips of a baised (0.75) coin\nrbinom(20, 1, 0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0\n```\n\n\n:::\n:::\n\n\n\n\nYou can generate the total number of heads in 1 set of 20 coin flips by setting `size` to 20 and `n` to 1.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(1, 20, 0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 16\n```\n\n\n:::\n:::\n\n\n\n\nYou can generate more sets of 20 coin flips by increasing the `n`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(10, 20, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 12  6  7  7  9 14 10 13  7  6\n```\n\n\n:::\n:::\n\n\n\n\nYou should always check your randomly generated data to check that it makes sense. For large samples, it's easiest to do that graphically. A histogram is usually the best choice for plotting binomial data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflips <- rbinom(1000, 20, 0.5)\n\nggplot() +\n  geom_histogram(\n    aes(flips), \n    binwidth = 1, \n    fill = \"white\", \n    color = \"black\"\n  )\n```\n\n::: {.cell-output-display}\n![](images/figures/sim_flips-1.png){width=100%}\n:::\n:::\n\n\n\n\n::: {.try data-latex=\"\"}\nRun the simulation above several times, noting how the histogram changes. Try changing the values of `n`, `size`, and `prob`.\n:::\n\n\n### Normal Distribution {#normal}\n\n`rnorm(n, mean, sd)`\n\nWe can simulate a <a href='https://psyteachr.github.io/glossary/n#normal-distribution' target='_blank' class='glossary' title='A symmetric distribution of data where values near the centre are most probable.'>normal distribution</a> of size `n` if we know the `mean` and standard deviation (`sd`). A density plot is usually the best way to visualise this type of data if your `n` is large.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndv <- rnorm(1e5, 10, 2)\n\n# proportions of normally-distributed data \n# within 1, 2, or 3 SD of the mean\nsd1 <- .6827 \nsd2 <- .9545\nsd3 <- .9973\n\nggplot() +\n  geom_density(aes(dv), fill = \"white\") +\n  geom_vline(xintercept = mean(dv), color = \"red\") +\n  geom_vline(xintercept = quantile(dv, .5 - sd1/2), color = \"darkgreen\") +\n  geom_vline(xintercept = quantile(dv, .5 + sd1/2), color = \"darkgreen\") +\n  geom_vline(xintercept = quantile(dv, .5 - sd2/2), color = \"blue\") +\n  geom_vline(xintercept = quantile(dv, .5 + sd2/2), color = \"blue\") +\n  geom_vline(xintercept = quantile(dv, .5 - sd3/2), color = \"purple\") +\n  geom_vline(xintercept = quantile(dv, .5 + sd3/2), color = \"purple\") +\n  scale_x_continuous(\n    limits = c(0,20), \n    breaks = seq(0,20)\n  )\n```\n\n::: {.cell-output-display}\n![](images/figures/rnorm-1.png){width=100%}\n:::\n:::\n\n\n\n\n::: {.info data-latex=\"\"}\nRun the simulation above several times, noting how the density plot changes. What do the vertical lines represent? Try changing the values of `n`, `mean`, and `sd`.\n:::\n\n\n### Poisson Distribution {#poisson}\n\nThe <a href='https://psyteachr.github.io/glossary/p#poisson-distribution' target='_blank' class='glossary' title='A distribution that models independent events happening over a unit of time'>Poisson distribution</a> is useful for modelling events, like how many times something happens over a unit of time, as long as the events are independent (e.g., an event having happened in one time period doesn't make it more or less likely to happen in the next).\n\n`rpois(n, lambda)`\n\nThe `rpois` function will generate a random Poisson distribution.\n\n* `n` = number of observations\n* `lambda` = the mean number of events per observation\n\nLet's say we want to model how many texts you get each day for a whole. You know that you get an average of 20 texts per day. So we set `n = 365` and `lambda = 20`. Lambda is a <a href='https://psyteachr.github.io/glossary/p#parameter' target='_blank' class='glossary' title='A quantity characterizing a population.'>parameter</a> that describes the Poisson distribution, just like mean and standard deviation are parameters that describe the normal distribution.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntexts <- rpois(n = 365, lambda = 20)\n\nggplot() +\n  geom_histogram(\n    aes(texts), \n    binwidth = 1, \n    fill = \"white\", \n    color = \"black\"\n  )\n```\n\n::: {.cell-output-display}\n![](images/figures/rpois-1.png){width=100%}\n:::\n:::\n\n\n\n\nSo we can see that over a year, you're unlikely to get fewer than 5 texts in a day, or more than 35 (although it's not impossible).\n\n## Multivariate Distributions {#mvdist}\n\n### Bivariate Normal {#bvn}\n\nA <a href='https://psyteachr.github.io/glossary/b#bivariate-normal' target='_blank' class='glossary' title='Two normally distributed vectors that have a specified correlation with each other.'>bivariate normal</a> distribution is two normally distributed vectors that have a specified relationship, or <a href='https://psyteachr.github.io/glossary/c#correlation' target='_blank' class='glossary' title='The relationship two vectors have to each other.'>correlation</a> to each other.\n\nWhat if we want to sample from a population with specific relationships between variables? We can sample from a bivariate normal distribution using `mvrnorm()` from the `MASS` package. \n\n::: {.warning data-latex=\"\"}\nDon't load MASS with the `library()` function because it will create a conflict with the `select()` function from dplyr and you will always need to preface it with `dplyr::`. Just use `MASS::mvrnorm()`.\n:::\n\nYou need to know how many observations you want to simulate (`n`) the means of the two variables (`mu`) and you need to calculate a <a href='https://psyteachr.github.io/glossary/c#covariance-matrix' target='_blank' class='glossary' title='Parameters showing how a set of vectors vary and covary.'>covariance matrix</a> (`sigma`) from the correlation between the variables (`rho`) and their standard deviations (`sd`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn   <- 1000 # number of random samples\n# name the mu values to give the resulting columns names\nmu     <- c(x = 10, y = 20) # the means of the samples\nsd <- c(5, 6)   # the SDs of the samples\n\nrho <- 0.5  # population correlation between the two variables\n\n# correlation matrix\ncor_mat <- matrix(c(  1, rho, \n                    rho,   1), 2) \n\n# create the covariance matrix\nsigma <- (sd %*% t(sd)) * cor_mat\n\n# sample from bivariate normal distribution\nbvn <- MASS::mvrnorm(n, mu, sigma) \n```\n:::\n\n\n\n\nPlot your sampled variables to check everything worked like you expect. It's easiest to convert the output of `mvnorm` into a tibble in order to use it in ggplot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbvn |>\n  as_tibble() |>\n  ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) + \n    geom_smooth(method = \"lm\") +\n    geom_density2d()\n```\n\n::: {.cell-output-display}\n![](images/figures/graph-bvn-1.png){width=100%}\n:::\n:::\n\n\n\n\n### Multivariate Normal {#mvnorm}\n\nYou can generate more than 2 correlated variables, but it gets a little trickier to create the correlation matrix.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn      <- 200 # number of random samples\nmu     <- c(x = 10, y = 20, z = 30) # the means of the samples\nsd <- c(8, 9, 10)   # the SDs of the samples\n\nrho1_2 <- 0.5 # correlation between x and y\nrho1_3 <- 0   # correlation between x and z\nrho2_3 <- 0.7 # correlation between y and z\n\n# correlation matrix\ncor_mat <- matrix(c(     1, rho1_2, rho1_3, \n                    rho1_2,      1, rho2_3,\n                    rho1_3, rho2_3,      1), 3) \n\nsigma <- (sd %*% t(sd)) * cor_mat\nbvn3 <- MASS::mvrnorm(n, mu, sigma)\n\ncor(bvn3) # check correlation matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          x         y         z\nx 1.0000000 0.5850764 0.1108543\ny 0.5850764 1.0000000 0.7234038\nz 0.1108543 0.7234038 1.0000000\n```\n\n\n:::\n:::\n\n\n\n\n\nYou can use the `plotly` library to make a 3D graph.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#set up the marker style\nmarker_style = list(\n    color = \"#ff0000\", \n    line = list(\n      color = \"#444\", \n      width = 1\n    ), \n    opacity = 0.5,\n    size = 5\n  )\n\n# convert bvn3 to a tibble, plot and add markers\nbvn3 |>\n  as_tibble() |>\n  plot_ly(x = ~x, y = ~y, z = ~z, marker = marker_style) |>\n  add_markers()\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in add_markers(plot_ly(as_tibble(bvn3), x = ~x, y = ~y, z = ~z, : could not find function \"add_markers\"\n```\n\n\n:::\n:::\n\n\n\n\n### Faux {#sec-faux}\n\nAlternatively, you can use the package [faux](https://debruine.github.io/faux/) to generate any number of correlated variables. It also has a function for checking the parameters of your new simulated data (`check_sim_stats()`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbvn3 <- rnorm_multi(\n  n = n, \n  vars = 3,\n  mu = mu, \n  sd = sd,\n  r = c(rho1_2, rho1_3, rho2_3),\n  varnames = c(\"x\", \"y\", \"z\")\n)\n\ncheck_sim_stats(bvn3)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|   n|var |    x|    y|    z|  mean|   sd|\n|---:|:---|----:|----:|----:|-----:|----:|\n| 200|x   | 1.00| 0.55| 0.00|  9.67| 8.43|\n| 200|y   | 0.55| 1.00| 0.64| 20.31| 8.73|\n| 200|z   | 0.00| 0.64| 1.00| 30.57| 9.40|\n\n</div>\n:::\n:::\n\n\n\n\nYou can also use faux to simulate data for factorial designs. Set up the between-subject and within-subject factors as lists with the levels as (named) vectors. Means and standard deviations can be included as vectors or data frames. The function calculates sigma for you, structures your dataset, and outputs a plot of the design.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb <- list(pet = c(cat = \"Cat Owners\",\n                  dog = \"Dog Owners\"))\nw <- list(time = c(\"morning\",\n                   \"noon\",\n                   \"night\"))\nmu <- data.frame(\n  cat    = c(10, 12, 14),\n  dog    = c(10, 15, 20),\n  row.names = w$time\n)\nsd <- c(3, 3, 3, 5, 5, 5)\n\npet_data <- sim_design(\n  within = w, \n  between = b,\n  n = 100, \n  mu = mu,\n  sd = sd, \n  r = .5)\n```\n\n::: {.cell-output-display}\n![](images/figures/faux-sim-design-1.png){width=100%}\n:::\n:::\n\n\n\n\nYou can use the `check_sim_stats()` function, but you need to set the argument `between` to a vector of all the between-subject factor columns.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_sim_stats(pet_data, between = \"pet\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|pet |   n|var     | morning| noon| night|  mean|   sd|\n|:---|---:|:-------|-------:|----:|-----:|-----:|----:|\n|cat | 100|morning |    1.00| 0.50|  0.53| 10.53| 3.31|\n|cat | 100|noon    |    0.50| 1.00|  0.52| 12.11| 2.96|\n|cat | 100|night   |    0.53| 0.52|  1.00| 14.21| 3.15|\n|dog | 100|morning |    1.00| 0.57|  0.36|  9.67| 4.86|\n|dog | 100|noon    |    0.57| 1.00|  0.45| 14.67| 5.89|\n|dog | 100|night   |    0.36| 0.45|  1.00| 19.96| 5.42|\n\n</div>\n:::\n:::\n\n\n\n\nSee the [faux website](https://debruine.github.io/faux/) for more detailed tutorials.\n\n\n## Statistical terms {#stat-terms}\n\nLet's review some important statistical terms before we review tests of distributions. \n\n### Effect {#effect}\n\nThe <a href='https://psyteachr.github.io/glossary/e#effect' target='_blank' class='glossary' title='Some measure of your data, such as the mean value, or the number of standard deviations the mean differs from a chance value.'>effect</a> is some measure of your data. This will depend on the type of data you have and the type of statistical test you are using. For example, if you flipped a coin 100 times and it landed heads 66 times, the effect would be 66/100. You can then use the exact binomial test to compare this effect to the <a href='https://psyteachr.github.io/glossary/n#null-effect' target='_blank' class='glossary' title='An outcome that does not show an otherwise expected effect.'>null effect</a> you would expect from a fair coin (50/100) or to any other effect you choose. The <a href='https://psyteachr.github.io/glossary/e#effect-size' target='_blank' class='glossary' title='The difference between the effect in your data and the null effect (usually a chance value)'>effect size</a> refers to the difference between the effect in your data and the null effect (usually a chance value).\n\n<div class=\"right meme\"><img src=\"images/memes/p-value.jpg\"\n  alt=\"Top text: Don't know what a p-value is, Bottom text: at this point I'm too afraid to ask\"/></div>\n\n### P-value {#p-value}\n\nThe <a href='https://psyteachr.github.io/glossary/p#p-value' target='_blank' class='glossary' title='The probability of seeing an effect at least as extreme as what you have, if the real effect was the value you are testing against (e.g., a null effect)'>p-value</a> of a test is the probability of seeing an effect at least as extreme as what you have, if the real effect was the value you are testing against (e.g., a null effect). So if you used a binomial test to test against a chance probability of 1/6 (e.g., the probability of rolling 1 with a 6-sided die), then a p-value of 0.17 means that you could expect to see effects at least as extreme as your data 17% of the time just by chance alone. \n\n### Alpha {#alpha}\n\nIf you are using null hypothesis significance testing (<a href='https://psyteachr.github.io/glossary/n#nhst' target='_blank' class='glossary' title='Null Hypothesis Signficance Testing'>NHST</a>), then you need to decide on a cutoff value (<a href='https://psyteachr.github.io/glossary/a#alpha' target='_blank' class='glossary' title='(stats) The cutoff value for making a decision to reject the null hypothesis; (graphics) A value between 0 and 1 used to control the levels of transparency in a plot'>alpha</a>) for making a decision to reject the null hypothesis. We call p-values below the alpha cutoff <a href='https://psyteachr.github.io/glossary/s#significant' target='_blank' class='glossary' title='The conclusion when the p-value is less than the critical alpha.'>significant</a>. In psychology, alpha is traditionally set at 0.05, but there are good arguments for [setting a different criterion in some circumstances](http://daniellakens.blogspot.com/2019/05/justifying-your-alpha-by-minimizing-or.html). \n\n### False Positive/Negative {#false-pos}\n\nThe probability that a test concludes there is an effect when there is really no effect (e.g., concludes a fair coin is biased) is called the <a href='https://psyteachr.github.io/glossary/f#false-positive' target='_blank' class='glossary' title='When a test concludes there is an effect when there really is no effect'>false positive</a> rate (or <a href='https://psyteachr.github.io/glossary/t#type-i-error' target='_blank' class='glossary' title='A false positive; When a test concludes there is an effect when there really is no effect'>Type I Error</a> Rate). The <a href='https://psyteachr.github.io/glossary/a#alpha' target='_blank' class='glossary' title='(stats) The cutoff value for making a decision to reject the null hypothesis; (graphics) A value between 0 and 1 used to control the levels of transparency in a plot'>alpha</a> is the false positive rate we accept for a test. The probability that a test concludes there is no effect when there really is one (e.g., concludes a biased coin is fair) is called the <a href='https://psyteachr.github.io/glossary/f#false-negative' target='_blank' class='glossary' title='When a test concludes there is no effect when there really is an effect'>false negative</a> rate (or <a href='https://psyteachr.github.io/glossary/t#type-ii-error' target='_blank' class='glossary' title='A false negative; When a test concludes there is no effect when there really is an effect'>Type II Error</a> Rate). The <a href='https://psyteachr.github.io/glossary/b#beta' target='_blank' class='glossary' title='The false negative rate we accept for a statistical test.'>beta</a> is the false negative rate we accept for a test.\n\n::: {.info data-latex=\"\"}\nThe false positive rate is not the overall probability of getting a false positive, but the probability of a false positive *under the null hypothesis*. Similarly, the false negative rate is the probability of a false negative *under the alternative hypothesis*. Unless we know the probability that we are testing a null effect, we can't say anything about the overall probability of false positives or negatives. If 100% of the hypotheses we test are false, then all significant effects are false positives, but if all of the hypotheses we test are true, then all of the positives are true positives and the overall false positive rate is 0.\n:::\n\n### Power and SESOI {#power}\n\n<a href='https://psyteachr.github.io/glossary/p#power' target='_blank' class='glossary' title='The probability of rejecting the null hypothesis when it is false.'>Power</a> is equal to 1 minus beta (i.e., the <a href='https://psyteachr.github.io/glossary/t#true-positive' target='_blank' class='glossary' title='When a test concludes there is an effect when there is really is an effect'>true positive</a> rate), and depends on the effect size, how many samples we take (n), and what we set alpha to. For any test, if you specify all but one of these values, you can calculate the last. The effect size you use in power calculations should be the smallest effect size of interest (<a href='https://psyteachr.github.io/glossary/s#sesoi' target='_blank' class='glossary' title='Smallest Effect Size of Interest: the smallest effect that is theoretically or practically meaningful'>SESOI</a>). See [@TOSTtutorial](https://doi.org/10.1177/2515245918770963) for a tutorial on methods for choosing an SESOI. \n\n::: {.try data-latex=\"\"}\nLet's say you want to be able to detect at least a 15% difference from chance (50%) in a coin's fairness, and you want your test to have a 5% chance of false positives and a 10% chance of false negatives. What are the following values?\n\n* alpha = <input class='webex-solveme nospaces' size='4' data-answer='[\"0.05\",\".05\",\"5%\"]'/>\n* beta = <input class='webex-solveme nospaces' size='4' data-answer='[\"0.1\",\"0.10\",\".1\",\".10\",\"10%\"]'/>\n* false positive rate = <input class='webex-solveme nospaces' size='4' data-answer='[\"0.05\",\".05\",\"5%\"]'/>\n* false negative rate = <input class='webex-solveme nospaces' size='4' data-answer='[\"0.1\",\"0.10\",\".1\",\".10\",\"10%\"]'/>\n* power = <input class='webex-solveme nospaces' size='4' data-answer='[\"0.9\",\"0.90\",\".9\",\".90\",\"90%\"]'/>\n* SESOI = <input class='webex-solveme nospaces' size='4' data-answer='[\"0.15\",\".15\",\"15%\"]'/>\n:::\n\n### Confidence Intervals {#conf-int}\n\nThe <a href='https://psyteachr.github.io/glossary/c#confidence-interval' target='_blank' class='glossary' title='A type of interval estimate used to summarise a given statistic or measurement where a proportion of intervals calculated from the sample(s) will contain the true value of the statistic.'>confidence interval</a> is a range around some value (such as a mean) that has some probability of containing the parameter, if you repeated the process many times. Traditionally in psychology, we use 95% confidence intervals, but you can calculate CIs for any percentage.\n\n::: {.info data-latex=\"\"}\nA 95% CI does *not* mean that there is a 95% probability that the true mean lies within this range, but that, if you repeated the study many times and calculated the CI this same way every time, you'd expect the true mean to be inside the CI in 95% of the studies. This seems like a subtle distinction, but can lead to some misunderstandings. See [@Morey2016](https://link.springer.com/article/10.3758/s13423-015-0947-8) for more detailed discussion.\n:::\n\n## Tests\n\n### Exact binomial test {#exact-binom}\n\n`binom.test(x, n, p)`\n\nYou can test a binomial distribution against a specific probability using the exact binomial test.\n\n* `x` = the number of successes\n* `n` = the number of trials\n* `p` = hypothesised probability of success\n\nHere we can test a series of 10 coin flips from a fair coin and a biased coin against the hypothesised probability of 0.5 (even odds).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10\nfair_coin <- rbinom(1, n, 0.5)\nbiased_coin <- rbinom(1, n, 0.6)\n\nbinom.test(fair_coin, n, p = 0.5)\nbinom.test(biased_coin, n, p = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tExact binomial test\n\ndata:  fair_coin and n\nnumber of successes = 6, number of trials = 10, p-value = 0.7539\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.2623781 0.8784477\nsample estimates:\nprobability of success \n                   0.6 \n\n\n\tExact binomial test\n\ndata:  biased_coin and n\nnumber of successes = 5, number of trials = 10, p-value = 1\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.187086 0.812914\nsample estimates:\nprobability of success \n                   0.5 \n```\n\n\n:::\n:::\n\n\n\n\n::: {.info data-latex=\"\"}\nRun the code above several times, noting the p-values for the fair and biased coins. Alternatively, you can [simulate coin flips](http://shiny.psy.gla.ac.uk/debruine/coinsim/) online and build up a graph of results and p-values. \n\n* How does the p-value vary for the fair and biased coins?\n* What happens to the confidence intervals if you increase n from 10 to 100?\n* What criterion would you use to tell if the observed data indicate the coin is fair or biased?\n* How often do you conclude the fair coin is biased (false positives)? \n* How often do you conclude the biased coin is fair (false negatives)?\n:::\n\n\n#### Sampling function {#sampling-binom}\n\nTo estimate these rates, we need to repeat the sampling above many times. A <a href='https://psyteachr.github.io/glossary/f#function' target='_blank' class='glossary' title='A named section of code that can be reused.'>function</a> is ideal for repeating the exact same procedure over and over. Set the arguments of the function to variables that you might want to change. Here, we will want to estimate power for:\n\n* different sample sizes (`n`)\n* different effects (`bias`)\n* different hypothesised probabilities (`p`, defaults to 0.5)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_binom_test <- function(n, bias, p = 0.5) {\n  # simulate 1 coin flip n times with the specified bias\n  coin <- rbinom(1, n, bias)\n  # run a binomial test on the simulated data for the specified p\n  btest <- binom.test(coin, n, p)\n  # return the p-value of this test\n  btest$p.value\n}\n```\n:::\n\n\n\n\nOnce you've created your function, test it a few times, changing the values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_binom_test(100, 0.6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3682016\n```\n\n\n:::\n:::\n\n\n\n\n#### Calculate power {#calc-power-binom}\n\nThen you can use the `replicate()` function to run it many times and save all the output values. You can calculate the <a href='https://psyteachr.github.io/glossary/p#power' target='_blank' class='glossary' title='The probability of rejecting the null hypothesis when it is false.'>power</a> of your analysis by checking the proportion of your simulated analyses that have a p-value less than your <a href='https://psyteachr.github.io/glossary/a#alpha' target='_blank' class='glossary' title='(stats) The cutoff value for making a decision to reject the null hypothesis; (graphics) A value between 0 and 1 used to control the levels of transparency in a plot'>alpha</a> (the probability of rejecting the null hypothesis when the null hypothesis is true).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_reps <- replicate(1e4, sim_binom_test(100, 0.6))\n\nalpha <- 0.05 # this does not always have to be 0.05\n\nmean(my_reps < alpha)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.459\n```\n\n\n:::\n:::\n\n\n\n\n::: {.info data-latex=\"\"}\n`1e4` is just scientific notation for a 1 followed by 4 zeros (`10000`). When you're running simulations, you usually want to run a lot of them. It's a pain to keep track of whether you've typed 5 or 6 zeros (100000 vs 1000000) and this will change your running time by an order of magnitude.\n:::\n\nYou can plot the distribution of p-values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_histogram(\n    aes(my_reps), \n    binwidth = 0.05, \n    boundary = 0,\n    fill = \"white\", \n    color = \"black\"\n  )\n```\n\n::: {.cell-output-display}\n![](images/figures/plot-reps-binom-1.png){width=100%}\n:::\n:::\n\n\n\n\n\n### T-test {#sec-t-test}\n\n`t.test(x, y, alternative, mu, paired)`\n\nUse a t-test to compare the mean of one distribution to a null hypothesis (one-sample t-test), compare the means of two samples (independent-samples t-test), or compare pairs of values (paired-samples t-test).\n\nYou can run a one-sample t-test comparing the mean of your data to `mu`. Here is a simulated distribution with a mean of 0.5 and an SD of 1, creating an effect size of 0.5 SD when tested against a `mu` of 0. Run the simulation a few times to see how often the t-test returns a significant p-value (or run it in the [shiny app](http://shiny.psy.gla.ac.uk/debruine/normsim/)).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_norm <- rnorm(100, 0.5, 1)\nt.test(sim_norm, mu = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  sim_norm\nt = 7.7949, df = 99, p-value = 6.582e-12\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.4734324 0.7967641\nsample estimates:\nmean of x \n0.6350983 \n```\n\n\n:::\n:::\n\n\n\n\nRun an independent-samples t-test by comparing two lists of values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- rnorm(100, 0.5, 1)\nb <- rnorm(100, 0.7, 1)\nt_ind <- t.test(a, b, paired = FALSE)\nt_ind\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  a and b\nt = -1.9481, df = 196.79, p-value = 0.05282\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.539079097  0.003297729\nsample estimates:\nmean of x mean of y \n0.5337330 0.8016237 \n```\n\n\n:::\n:::\n\n\n\n\n::: {.warning data-latex=\"\"}\nThe `paired` argument defaults to `FALSE`, but it's good practice to always explicitly set it so you are never confused about what type of test you are performing.\n:::\n\n#### Sampling function {#sampling-t}\n\nWe can use the `names()` function to find out the names of all the t.test parameters and use this to just get one type of data, like the test statistic (e.g., t-value).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(t_ind)\nt_ind$statistic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"statistic\"   \"parameter\"   \"p.value\"     \"conf.int\"    \"estimate\"   \n [6] \"null.value\"  \"stderr\"      \"alternative\" \"method\"      \"data.name\"  \n        t \n-1.948111 \n```\n\n\n:::\n:::\n\n\n\n\nIf you want to run the simulation many times and record information each time, first you need to turn your simulation into a function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_t_ind <- function(n, m1, sd1, m2, sd2) {\n  # simulate v1\n  v1 <- rnorm(n, m1, sd1)\n  \n  #simulate v2\n  v2 <- rnorm(n, m2, sd2)\n    \n  # compare using an independent samples t-test\n  t_ind <- t.test(v1, v2, paired = FALSE)\n  \n  # return the p-value\n  return(t_ind$p.value)\n}\n```\n:::\n\n\n\n\nRun it a few times to check that it gives you sensible values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_t_ind(100, 0.7, 1, 0.5, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4781169\n```\n\n\n:::\n:::\n\n\n\n\n#### Calculate power {#calc-power-t}\n\nNow replicate the simulation 1000 times.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_reps <- replicate(1e4, sim_t_ind(100, 0.7, 1, 0.5, 1))\n\nalpha <- 0.05\npower <- mean(my_reps < alpha)\npower\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2853\n```\n\n\n:::\n:::\n\n\n\n\n::: {.try data-latex=\"\"}\nRun the code above several times. How much does the power value fluctuate? How many replications do you need to run to get a reliable estimate of power?\n:::\n\nCompare your power estimate from simluation to a power calculation using `power.t.test()`. Here, `delta` is the difference between `m1` and `m2` above.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n = 100, \n             delta = 0.2, \n             sd = 1, \n             sig.level = alpha, \n             type = \"two.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 100\n          delta = 0.2\n             sd = 1\n      sig.level = 0.05\n          power = 0.2902664\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\n\nYou can plot the distribution of p-values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_histogram(\n    aes(my_reps), \n    binwidth = 0.05, \n    boundary = 0,\n    fill = \"white\", \n    color = \"black\"\n  )\n```\n\n::: {.cell-output-display}\n![](images/figures/plot-reps-t-1.png){width=100%}\n:::\n:::\n\n\n\n\n::: {.try data-latex=\"\"}\nWhat do you think the distribution of p-values is \nwhen there is no effect (i.e., the means are identical)? Check this yourself.\n:::\n\n::: {.warning data-latex=\"\"}\nMake sure the `boundary` argument is set to `0` for p-value histograms. See what happens with a null effect if `boundary` is not set.\n:::\n\n\n### Correlation {#correlation}\n\nYou can test if continuous variables are related to each other using the `cor()` function. Let's use `rnorm_multi()` to make a quick table of correlated values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- rnorm_multi(\n  n = 100, \n  vars = 2, \n  r = -0.5,\n  varnames = c(\"x\", \"y\")\n)\n\ncor(dat$x, dat$y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.4405768\n```\n\n\n:::\n:::\n\n\n\n\n::: {.try data-latex=\"\"}\nSet `n` to a large number like 1e6 so that the correlations are less affected by chance. Change the value of the **mean** for `a`, `x`, or `y`. Does it change the correlation between `x` and `y`? What happens when you increase or decrease the **sd**? Can you work out any rules here?\n:::\n\n`cor()` defaults to Pearson's correlations. Set the `method` argument to use Kendall or Spearman correlations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(dat$x, dat$y, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.400072\n```\n\n\n:::\n:::\n\n\n\n\n#### Sampling function {#sampling-cor}\n\nCreate a function that creates two variables with `n` observations and `r` correlation. Use the function `cor.test()` to give you p-values for the correlation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_cor_test <- function(n = 100, r = 0) {\n  dat <- rnorm_multi(\n    n = n, \n    vars = 2, \n    r = r,\n    varnames = c(\"x\", \"y\")\n  )\n\n  ctest <- cor.test(dat$x, dat$y)\n  ctest$p.value\n}\n```\n:::\n\n\n\n\nOnce you've created your function, test it a few times, changing the values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_cor_test(50, .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.653965e-09\n```\n\n\n:::\n:::\n\n\n\n\n#### Calculate power {#calc-power-cor}\n\nNow replicate the simulation 1000 times.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_reps <- replicate(1e4, sim_cor_test(50, 0.5))\n\nalpha <- 0.05\npower <- mean(my_reps < alpha)\npower\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.966\n```\n\n\n:::\n:::\n\n\n\n\nCompare to the value calcuated by the pwr package.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr::pwr.r.test(n = 50, r = 0.5)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in loadNamespace(x): there is no package called 'pwr'\n```\n\n\n:::\n:::\n\n\n\n\n\n## Example\n\nThis example uses the [Growth Chart Data Tables](https://www.cdc.gov/growthcharts/data/zscore/zstatage.csv) from the [US CDC](https://www.cdc.gov/growthcharts/zscore.htm). The data consist of height in centimeters for the z-scores of –2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, and 2 by sex (1=male; 2=female) and half-month of age (from 24.0 to 240.5 months).\n\n### Load & wrangle\n\nWe have to do a little data wrangling first. Have a look at the data after you import it and relabel `Sex` to `male` and `female` instead of `1` and `2`. Also convert `Agemos` (age in months) to years. Relabel the column `0` as `mean` and calculate a new column named `sd` as the difference between columns `1` and `0`. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\norig_height_age <- read_csv(\"https://www.cdc.gov/growthcharts/data/zscore/zstatage.csv\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Failed to open\n'https://www.cdc.gov/growthcharts/data/zscore/zstatage.csv': Invalid HTTP\nheader field was received: frame type: 1, stream: 1, name: [set-cookie], value:\n[TS0196e5be=015d0abe87633f1789c6f892bc5b696479311f9b8876703fd13abfcea7608e6eb23064126ac246db1b91e3c5c773d4f6f1c417cc3e;\nPath=/; Domain=.www.cdc.gov; Secure; HttpO\n```\n\n\n:::\n\n::: {.cell-output .cell-output-error}\n\n```\nError in open.connection(structure(4L, class = c(\"curl\", \"connection\"), conn_id = <pointer: 0x127451d10>), : cannot open the connection\n```\n\n\n:::\n\n```{.r .cell-code}\nheight_age <- orig_height_age |>\n  filter(Sex %in% c(1,2)) |>\n  mutate(\n    sex = recode(Sex, \"1\" = \"male\", \"2\" = \"female\"),\n    age = as.numeric(Agemos)/12,\n    sd = `1` - `0`\n  ) |>\n  select(sex, age, mean = `0`, sd)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'orig_height_age' not found\n```\n\n\n:::\n:::\n\n\n\n\n\n### Plot\n\nPlot your new data frame to see how mean height changes with age for boys and girls.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(height_age, aes(age, mean, color = sex)) +\n  geom_smooth(aes(ymin = mean - sd, \n                  ymax = mean + sd),\n              stat=\"identity\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'height_age' not found\n```\n\n\n:::\n:::\n\n\n\n\n### Simulate a population\n\nSimulate 50 random male heights and 50 random female heights for 20-year-olds using the `rnorm()` function and the means and SDs from the `height_age` table. Plot the data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage_filter <- 20\nm <- filter(height_age, age == age_filter, sex == \"male\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'height_age' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nf <- filter(height_age, age == age_filter, sex == \"female\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'height_age' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nsim_height <- tibble(\n  male = rnorm(50, m$mean, m$sd),\n  female = rnorm(50, f$mean, f$sd)\n) |>\n  gather(\"sex\", \"height\", male:female)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'm' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(sim_height) +\n  geom_density(aes(height, fill = sex), alpha = 0.5) +\n  xlim(125, 225)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'sim_height' not found\n```\n\n\n:::\n:::\n\n\n\n\n::: {.try data-latex=\"\"}\nRun the simulation above several times, noting how the density plot changes. Try changing the age you're simulating.\n:::\n\n### Analyse simulated data\n\nUse the `sim_t_ind(n, m1, sd1, m2, sd2)` function we created above to generate one simulation with a sample size of 50 in each group using the means and SDs of male and female 14-year-olds.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage_filter <- 14\nm <- filter(height_age, age == age_filter, sex == \"male\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'height_age' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nf <- filter(height_age, age == age_filter, sex == \"female\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'height_age' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nsim_t_ind(50, m$mean, m$sd, f$mean, f$sd)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'm' not found\n```\n\n\n:::\n:::\n\n\n\n\n### Replicate simulation\n\nNow replicate this 1e4 times using the `replicate()` function. This function will save the returned p-values in a list (`my_reps`). We can then check what proportion of those p-values are less than our alpha value. This is the power of our test.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_reps <- replicate(1e4, sim_t_ind(50, m$mean, m$sd, f$mean, f$sd))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in FUN(X[[i]], ...): object 'm' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nalpha <- 0.05\npower <- mean(my_reps < alpha)\npower\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.966\n```\n\n\n:::\n:::\n\n\n\n\n### One-tailed prediction\n\nThis design has about 65% power to detect the sex difference in height (with a 2-tailed test). Modify the `sim_t_ind` function for a 1-tailed prediction.\n\nYou could just set `alternative` equal to \"greater\" in the function, but it might be better to add the `alt` argument to your function (giving it the same default value as `t.test`) and change the value of `alternative` in the function to `alt`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_t_ind <- function(n, m1, sd1, m2, sd2, alt = \"two.sided\") {\n  v1 <- rnorm(n, m1, sd1)\n  v2 <- rnorm(n, m2, sd2)\n  t_ind <- t.test(v1, v2, paired = FALSE, alternative = alt)\n  \n  return(t_ind$p.value)\n}\n\nalpha <- 0.05\nmy_reps <- replicate(1e4, sim_t_ind(50, m$mean, m$sd, f$mean, f$sd, \"greater\"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in FUN(X[[i]], ...): object 'm' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(my_reps < alpha)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.966\n```\n\n\n:::\n:::\n\n\n\n\n### Range of sample sizes\n\nWhat if we want to find out what sample size will give us 80% power? We can try trial and error. We know the number should be slightly larger than 50. But you can search more systematically by repeating your power calculation for a range of sample sizes. \n\n::: {.info data-latex=\"\"}\nThis might seem like overkill for a t-test, where you can easily look up sample size calculators online, but it is a valuable skill to learn for when your analyses become more complicated.\n:::\n\nStart with a relatively low number of replications and/or more spread-out samples to estimate where you should be looking more specifically. Then you can repeat with a narrower/denser range of sample sizes and more iterations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make another custom function to return power\npwr_func <- function(n, reps = 100, alpha = 0.05) {\n  ps <- replicate(reps, sim_t_ind(n, m$mean, m$sd, f$mean, f$sd, \"greater\"))\n  mean(ps < alpha)\n}\n\n# make a table of the n values you want to check\npower_table <- tibble(\n  n = seq(20, 100, by = 5)\n) |>\n  # run the power function for each n\n  mutate(power = map_dbl(n, pwr_func))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in `mutate()`:\nℹ In argument: `power = map_dbl(n, pwr_func)`.\nCaused by error in `map_dbl()`:\nℹ In index: 1.\nCaused by error in `FUN()`:\n! object 'm' not found\n```\n\n\n:::\n\n```{.r .cell-code}\n# plot the results\nggplot(power_table, aes(n, power)) +\n  geom_smooth() +\n  geom_point() +\n  geom_hline(yintercept = 0.8)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'power_table' not found\n```\n\n\n:::\n:::\n\n\n\n\nNow we can narrow down our search to values around 55 (plus or minus 5) and increase the number of replications from 1e3 to 1e4.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower_table <- tibble(\n  n = seq(50, 60)\n) |>\n  mutate(power = map_dbl(n, pwr_func, reps = 1e4))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in `mutate()`:\nℹ In argument: `power = map_dbl(n, pwr_func, reps = 10000)`.\nCaused by error in `map_dbl()`:\nℹ In index: 1.\nCaused by error in `FUN()`:\n! object 'm' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(power_table, aes(n, power)) +\n geom_smooth() +\n geom_point() +\n geom_hline(yintercept = 0.8)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'power_table' not found\n```\n\n\n:::\n:::\n\n\n\n\n## Glossary {#sec-glossary-sim -}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term                                                    |definition                                                                                                                                                                                 |\n|:-------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|[alpha](https://psyteachr.github.io/glossary/a#alpha){target='_blank' class='glossary'}|(stats) The cutoff value for making a decision to reject the null hypothesis; (graphics) A value between 0 and 1 used to control the levels of transparency in a plot                      |\n|[beta](https://psyteachr.github.io/glossary/b#beta){target='_blank' class='glossary'}|The false negative rate we accept for a statistical test.                                                                                                                                  |\n|[binomial-distribution](https://psyteachr.github.io/glossary/b#binomial-distribution){target='_blank' class='glossary'}|The distribution of data where each observation can have one of two outcomes, like success/failure, yes/no or head/tails.                                                                  |\n|[bivariate-normal](https://psyteachr.github.io/glossary/b#bivariate-normal){target='_blank' class='glossary'}|Two normally distributed vectors that have a specified correlation with each other.                                                                                                        |\n|[confidence-interval](https://psyteachr.github.io/glossary/c#confidence-interval){target='_blank' class='glossary'}|A type of interval estimate used to summarise a given statistic or measurement where a proportion of intervals calculated from the sample(s) will contain the true value of the statistic. |\n|[correlation](https://psyteachr.github.io/glossary/c#correlation){target='_blank' class='glossary'}|The relationship two vectors have to each other.                                                                                                                                           |\n|[covariance-matrix](https://psyteachr.github.io/glossary/c#covariance-matrix){target='_blank' class='glossary'}|Parameters showing how a set of vectors vary and covary.                                                                                                                                   |\n|[discrete](https://psyteachr.github.io/glossary/d#discrete){target='_blank' class='glossary'}|Data that can only take certain values, such as integers.                                                                                                                                  |\n|[effect](https://psyteachr.github.io/glossary/e#effect){target='_blank' class='glossary'}|Some measure of your data, such as the mean value, or the number of standard deviations the mean differs from a chance value.                                                              |\n|[effect-size](https://psyteachr.github.io/glossary/e#effect-size){target='_blank' class='glossary'}|The difference between the effect in your data and the null effect (usually a chance value)                                                                                                |\n|[false-negative](https://psyteachr.github.io/glossary/f#false-negative){target='_blank' class='glossary'}|When a test concludes there is no effect when there really is an effect                                                                                                                    |\n|[false-positive](https://psyteachr.github.io/glossary/f#false-positive){target='_blank' class='glossary'}|When a test concludes there is an effect when there really is no effect                                                                                                                    |\n|[function](https://psyteachr.github.io/glossary/f#function){target='_blank' class='glossary'}|A named section of code that can be reused.                                                                                                                                                |\n|[nhst](https://psyteachr.github.io/glossary/n#nhst){target='_blank' class='glossary'}|Null Hypothesis Signficance Testing                                                                                                                                                        |\n|[normal-distribution](https://psyteachr.github.io/glossary/n#normal-distribution){target='_blank' class='glossary'}|A symmetric distribution of data where values near the centre are most probable.                                                                                                           |\n|[null-effect](https://psyteachr.github.io/glossary/n#null-effect){target='_blank' class='glossary'}|An outcome that does not show an otherwise expected effect.                                                                                                                                |\n|[p-value](https://psyteachr.github.io/glossary/p#p-value){target='_blank' class='glossary'}|The probability of seeing an effect at least as extreme as what you have, if the real effect was the value you are testing against (e.g., a null effect)                                   |\n|[parameter](https://psyteachr.github.io/glossary/p#parameter){target='_blank' class='glossary'}|A quantity characterizing a population.                                                                                                                                                    |\n|[poisson-distribution](https://psyteachr.github.io/glossary/p#poisson-distribution){target='_blank' class='glossary'}|A distribution that models independent events happening over a unit of time                                                                                                                |\n|[power](https://psyteachr.github.io/glossary/p#power){target='_blank' class='glossary'}|The probability of rejecting the null hypothesis when it is false.                                                                                                                         |\n|[power](https://psyteachr.github.io/glossary/p#power){target='_blank' class='glossary'}|The probability of rejecting the null hypothesis when it is false.                                                                                                                         |\n|[probability](https://psyteachr.github.io/glossary/p#probability){target='_blank' class='glossary'}|A number between 0 and 1 where 0 indicates impossibility of the event and 1 indicates certainty                                                                                            |\n|[sesoi](https://psyteachr.github.io/glossary/s#sesoi){target='_blank' class='glossary'}|Smallest Effect Size of Interest: the smallest effect that is theoretically or practically meaningful                                                                                      |\n|[significant](https://psyteachr.github.io/glossary/s#significant){target='_blank' class='glossary'}|The conclusion when the p-value is less than the critical alpha.                                                                                                                           |\n|[simulation](https://psyteachr.github.io/glossary/s#simulation){target='_blank' class='glossary'}|Generating data from summary parameters                                                                                                                                                    |\n|[true-positive](https://psyteachr.github.io/glossary/t#true-positive){target='_blank' class='glossary'}|When a test concludes there is an effect when there is really is an effect                                                                                                                 |\n|[type-i-error](https://psyteachr.github.io/glossary/t#type-i-error){target='_blank' class='glossary'}|A false positive; When a test concludes there is an effect when there really is no effect                                                                                                  |\n|[type-ii-error](https://psyteachr.github.io/glossary/t#type-ii-error){target='_blank' class='glossary'}|A false negative; When a test concludes there is no effect when there really is an effect                                                                                                  |\n|[uniform-distribution](https://psyteachr.github.io/glossary/u#uniform-distribution){target='_blank' class='glossary'}|A distribution where all numbers in the range have an equal probability of being sampled                                                                                                   |\n|[univariate](https://psyteachr.github.io/glossary/u#univariate){target='_blank' class='glossary'}|Relating to a single variable.                                                                                                                                                             |\n\n\n:::\n:::\n\n\n\n\n## Further Resources {#sec-resources-sim -}\n\n* [Distribution Shiny App](http://shiny.psy.gla.ac.uk/debruine/simulate/) (or run `reprores::app(\"simulate\")`\n* [Simulation tutorials](https://debruine.github.io/data-sim-workshops/) by Lisa DeBruine\n* [Chapter 21: Iteration](http://r4ds.had.co.nz/iteration.html)  of *R for Data Science*\n* [Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/) on Coursera (week 1)\n* [Faux](https://debruine.github.io/faux/) package for data simulation\n* [Simulation-Based Power-Analysis for Factorial ANOVA Designs](https://psyarxiv.com/baxsf) [@lakens_caldwell_2019]\n* [Understanding mixed effects models through data simulation](https://psyarxiv.com/xp5cy/) [@debruine_barr_2019]\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}