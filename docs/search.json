[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Skills for Reproducible Research",
    "section": "",
    "text": "Overview\nThis book provides an overview of skills needed for reproducible and open research using the statistical programming language R and tidyverse packages. It covers reproducible workflows, data visualisation, data tidying and wrangling, archiving, iteration and functions, probability and data simulations.\nWhile this book mainly focuses on technical data skills, reproducible and open research is the reason for learning these skills. The following papers provide a great overview of these concepts if you are not already familiar with them.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Data Skills for Reproducible Research",
    "section": "Resources",
    "text": "Resources\n\nVideos Each chapter has several short video lectures for the main learning outcomes. The videos are captioned and watching with the captioning on is a useful way to learn the jargon of computational reproducibility. If you cannot access YouTube, the videos are available by request. The videos were created in 2020, so a few aspects of the RStudio interface or the book text have changed.\nglossary Coding and statistics both have a lot of specialist terms. Throughout this book, jargon will be linked to the glossary. Each chapter will end with a table of glossary terms relevant to the chapter.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#how-to-learn-data-skills",
    "href": "index.html#how-to-learn-data-skills",
    "title": "Data Skills for Reproducible Research",
    "section": "How to learn data skills",
    "text": "How to learn data skills\n\n\n\nLearning data skills is kind of like having a gym membership (HT to Phil McAleer for the analogy). You’ll be given state-of-the-art equipment to use and instructions for how to use them, but your data skills won’t get any stronger unless you practice.\nData skills do not require you to memorise lots of code. You will be introduced to many different functions, but the main skill to learn is how to efficiently find the information you need. This will require getting used to the structure of help files and cheat sheets, learning how to Google your problem and choose a helpful solution, and learning how to read error messages.\n\n\n\nLearning to code involves making a lot of mistakes. These mistakes are completely essential to the process, so try not to feel too frustrated. Many of the chapter exercises will give you broken code to fix so you get experience seeing what common errors look like. As you become a more experienced coder, you might not make fewer errors, but you’ll recover from them much faster.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#i-found-a-bug",
    "href": "index.html#i-found-a-bug",
    "title": "Data Skills for Reproducible Research",
    "section": "I found a bug!",
    "text": "I found a bug!\nThis book is a work in progress, so you might find errors. Please help me fix them! The best way is to open an issue on github that describes the error, but you can also email Lisa.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#other-resources",
    "href": "index.html#other-resources",
    "title": "Data Skills for Reproducible Research",
    "section": "Other Resources",
    "text": "Other Resources\n\nRStudio Cheat Sheets\nImproving Pedagogy through Registered Reports\n\nLearning Statistics with R by Navarro\n\nR for Data Science by Grolemund and Wickham\n\nImproving your statistical inferences on Coursera\nswirl\nR for Reproducible Scientific Analysis\ncodeschool.com\ndatacamp\nStyle guide for R programming",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Data Skills for Reproducible Research",
    "section": "References",
    "text": "References\n\n\n\n\nCorker, K. S. (2021). An open science workflow for more credible, rigorous research.\n\n\nCrüwell, S., Doorn, J. van, Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven easy steps to open science: An annotated reading list. Zeitschrift für Psychologie, 227(4), 237.\n\n\nKathawalla, U.-K., Silverstein, P., & Syed, M. (2021). Easing into open science: A guide for graduate students and their advisors. Collabra: Psychology, 7(1). https://doi.org/10.1525/collabra.18684\n\n\nParsons, S., Azevedo, F., others, & Aczel, B. (2022). A community-sourced glossary of open scholarship terms. Nature Human Behaviour, 6(3), 312–318. https://doi.org/10.1038/s41562-021-01269-4",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-ilo-intro",
    "href": "01-intro.html#sec-ilo-intro",
    "title": "1  Introduction",
    "section": "",
    "text": "Install R and RStudio\nInstall add-on packages\nGet help for packages and functions\nCreate objects by writing and running code in the console",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-functions-intro",
    "href": "01-intro.html#sec-functions-intro",
    "title": "1  Introduction",
    "section": "Functions used",
    "text": "Functions used\n\nbuilt-in (you can always use these without loading any packages)\n\nbase:: .rs.restartR(), as.Date(), library(), paste(), sample(), Sys.Date()\n\nutils:: help(), install.packages(), vignette()\n\nstats:: sample(), rnorm()\n\n\n\nother (you need to load each package to use these)\n\nbeepr:: beepr::beep()\n\ndevtools:: devtools::install_github()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-setup-intro",
    "href": "01-intro.html#sec-setup-intro",
    "title": "1  Introduction",
    "section": "Setup",
    "text": "Setup\nDownload the RStudio IDE Cheatsheet",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-intro-installing-r",
    "href": "01-intro.html#sec-intro-installing-r",
    "title": "1  Introduction",
    "section": "\n1.1 Installing R and RStudio",
    "text": "1.1 Installing R and RStudio\nR is a programming language that you will write code in and RStudio is a program that makes working in R easier.\n\nAppendix A has technical details on installing R and RStudio on your computer. Once you have installed R and RStudio, come back to this chapter. If you already had R and/or RStudio installed, we recommend updating to the latest version before you work through this course. Section A.2 has more details on how to do that. Here, we’ll concentrate on introducing you to RStudio’s interface and getting it configured.\n\n\n1.1.1 RStudio\nWhen you installed R, that gave your computer the ability to process the R programming language, and also installed an app called “R”. We will never use that app. Instead, we will use RStudio.\n\n\n\n\n\n\nLaunch R though the RStudio IDE\n\n\n\nLaunch  (RStudio.app), not  (R.app).\n\n\nRStudio is an Integrated Development Environment (IDE). Think of it as knowing English and using a plain text editor like NotePad to write a book versus using a word processor like Microsoft Word. You could do it, but it would be much harder without things like spell-checking and formatting and you wouldn’t be able to use some of the advanced features that Word has developed. In a similar way, you can use R without R Studio but we wouldn’t recommend it. RStudio serves as a text editor, file manager, spreadsheet viewer, and more. The key thing to remember is that although you will do all of your work using RStudio for this course, you are actually using two pieces of software, which means that from time to time, both of them may have separate updates.\nRStudio is arranged with four window panes.\n\n\n\n\n\nFigure 1.1: The RStudio IDE\n\n\nBy default, the upper left pane is the source pane, where you view, write, and edit code from files and view data tables in a spreadsheet format. When you first open RStudio, this pane won’t display until we open a document or load in some data – don’t worry, we’ll get to that soon.\nThe lower left pane is the console pane, where you can type in commands and view output messages. You can write code in the console to test it out. The code will run and can create objects in the environment, but the code itself won’t be saved. You need to write your code into a script in the source pane to save it, which we’ll cover in Chapter 2.\nThe right panes have several different tabs that show you information about your code. The most used tabs in the upper right pane are the Environment tab and the Help tab. The Environment tab lists some information about the objects that you have defined in your code. We’ll learn more about the Help tab in Section 1.2.5.\nIn the lower right pane, the most used tabs are the Files tab for directory structure, the Plots tab for plots made in a script, the Packages tab for managing add-on packages (see Section 1.2), and the Viewer tab to display reports created by your scripts. You can change the location of panes and what tabs are shown under Tools &gt; Global Options… &gt; Pane Layout.\n\n1.1.2 Reproducibility\nIn this class, you will be learning how to do reproducible research. This involves writing scripts that completely and transparently perform some analysis from start to finish in a way that yields the same result for different people using the same software on different computers. Transparency is a key value of science, as embodied in the “trust but verify” motto.\n\n{fig-alt = “Fry from Futurama squinting; top text: Not sure if I have a bad memory; bottom text: Or a bad memory”}\n\nWhen you do things reproducibly, others can understand and check your work. This benefits science, but there is a selfish reason, too: the most important person who will benefit from a reproducible script is your future self. When you return to an analysis after two weeks of vacation, you will thank your earlier self for doing things in a transparent, reproducible way, as you can easily pick up right where you left off. It might take a little longer to set up the report in the first instance with reproducible methods, but the time it saves you in the long run is invaluable.\n\n\n\n\n\n\nSettings for Reproducibility\n\n\n\nSection A.1.2.2 shows you how to change two important settings in the Global Options to increase reproducibility. Your settings should have:\n\nRestore .RData into workspace at startup: \nChecked\nNot Checked\n\nSave workspace to .RData on exit: \nAlways\nNever\nAsk\n\n\n\n\n\n1.1.3 Themes and accessiblilty\nYou can customise how R Studio looks to make it work for you. You can change the default font, font size, and general appearance of R Studio, including using dark mode.\n\nClick Tools &gt; Global Options &gt; Appearance. Play around with the settings and see what you prefer - you’re going to spend a lot of time with R, it might as well look nice!\n\n\n1.1.4 Sessions\nIf you have the above settings configured correctly, when you open up RStudio and start writing code, loading packages, and creating objects, you will be doing so in a new session and your Environment tab should be completely empty. If you find that your code isn’t working and you can’t figure out why, it might be worth restarting your R session. This will clear the environment and detach all loaded packages - think of it like restarting your phone. There are several ways that you can restart R:\n\nMenu: Session &gt; Restart R\n\n\nCmd-Shift-F10 or Ctl-Shift-F10\n\ntype .rs.restartR() in the console\n\n\nTry each method of restarting R. Additionally, now would be a good time to create a notebook where you can keep a record of useful hints and tips and things to try when your code isn’t working. Add “restart R session” to this notebook as your first item.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-packages",
    "href": "01-intro.html#sec-packages",
    "title": "1  Introduction",
    "section": "\n1.2 Packages and functions",
    "text": "1.2 Packages and functions\nWhen you install R you will have access to a range of functions including options for data wrangling and statistical analysis. The functions that are included in the default installation are typically referred to as base R and you can think of them like the default apps that come pre-loaded on your phone.\nOne of the great things about R, however, is that it is user extensible: anyone can create a new add-on that extends its functionality. There are currently thousands of packages that R users have created to solve many different kinds of problems, or just simply to have fun. For example, there are packages for data visualisation, machine learning, interactive dashboards, web scraping, and playing games such as Sudoku.\nAdd-on packages are not distributed with base R, but have to be downloaded and installed from an archive, in the same way that you would, for instance, download and install PokemonGo on your smartphone. The main repository where packages reside is called CRAN, the Comprehensive R Archive Network.\nThere is an important distinction between installing a package and loading a package.\n\n1.2.1 Installing a package\n\n\n\nThis is done using install.packages(). This is like installing an app on your phone: you only have to do it once and the app will remain installed until you remove it. For instance, if you want to use PokemonGo on your phone, you install it once from the App Store or Play Store; you don’t have to re-install it each time you want to use it. Once you launch the app, it will run in the background until you close it or restart your phone. Likewise, when you install a package, the package will be available (but not loaded) every time you open up R.\n\nInstall the tidyverse package on your system. This is the main package we will use throughout this book for data wrangling, summaries, and visualisation. It is actually a bundle of packages, which we’ll explain further in Section 1.2.4.\n\n\n\nRun in the console\n\ninstall.packages(\"tidyverse\")\n\n\nIf you get a message that says something like package ‘tidyverse’ successfully unpacked and MD5 sums checked, the installation was successful. If you get an error and the package wasn’t installed, check the troubleshooting section of Section A.2.4.\n\n\n\n\n\n\n\nInstall packages from the console only\n\n\n\nNever install a package from inside a script. Only do this from the console pane or the packages tab of the lower right pane.\n\n\nHere are some other packages you’ll want to install for the first chapter.\n\n\n\nRun in the console\n\ninstall.packages(\"beepr\")     # for beeps\ninstall.packages(\"devtools\")  # for installing packages from github\n\n\nOnce you’ve installed the devtools package, you can also install packages from repositories other than CRAN, such as github. The following code installs the development version of a package for guiding computational reproducibility reviews.\n\n\n\nRun in the console\n\n# install compreprev package \ndevtools::install_github(\"debruine/compreprev\")\n\n\n\n1.2.2 Loading a package\nThis is done using the library() function. This is like launching an app on your phone: the functionality is only there where the app is launched and remains there until you close the app or restart. For example, when you run library(devtools) within a session, the functions in the package referred to by devtools will be made available for your R session. The next time you start R, you will need to run library(devtools) again if you want to access that package.\n\nAfter installing the beepr package, you can load it for your current R session as follows:\n\n\n\nRun in the console\n\nlibrary(beepr)\n\n\n\nYou might get some red text when you load a package, this is normal. It is usually warning you that this package has functions that have the same name as other packages you’ve already loaded.\n\n\n\n\n\n\nNote\n\n\n\nYou can use the convention package::function() to indicate in which add-on package a function resides. For instance, if you see readr::read_csv(), that refers to the function read_csv() in the readr add-on package. If the package is loaded using library(), you don’t have to specify the package name before a function unless there is a conflict (e.g., you have two packages loaded that have a function with the same name).\n\n\n\n1.2.3 Using a function\nNow you can run the function beep().\n\n\n\nRun in the console\n\nbeep()\n\n\nA function is a name that refers to some code you can reuse. We’ll start by using functions that are provided for you in packages, but you can also write your own functions. After the function name, there is a pair of parentheses, which contain zero or more arguments. These are options that you can set. In the example above, the sound argument has a default value of 1, which makes a “ping” sound.\n\nTry changing the argument to an integer between 1 and 11.\n\n\n\nRun in the console\n\nbeep(sound = 8)\n\n\n\nIf you type a function into the console pane, it will run as soon as you hit enter. If you put the function in a script or quarto document in the source pane, it won’t run until you run the script, render the file, or run a code chunk. You’ll learn more about this in Chapter 2.\n\n1.2.4 Tidyverse\ntidyverse is a meta-package that loads several packages we’ll be using in almost every chapter in this book:\n\n\nggplot2, for data visualisation (Chapter 3)\n\nreadr, for data import (Appendix G)\n\ntibble, for tables (Section H.2.3)\n\ntidyr, for data tidying (Chapter 6)\n\ndplyr, for data manipulation (Chapter 7)\n\nstringr, for strings (Appendix H)\n\nforcats, for factors (Section 3.1.2)\n\npurrr, for repeating things (Section 8.1)\n\nWhen you install tidyverse, it also installs some other useful packages that you can load individually. You can get the full list using tidyverse_packages(), but the packages we’ll be using in this book are:\n\n\ngooglesheets4, for working with Google spreadsheets\n\nreadxl, for Excel files\n\nlubridate, for working with dates\n\nhms, for working with times\n\nrvest, for web scraping\n\n1.2.5 Function Help\nWhen you load the tidyverse it automatically loads all of the above packages, however, it can be helpful to know which package a function comes from if you need to Google it. If a function is in base R or a loaded package, you can type ?function_name in the console to access the help file. At the top of the help it will give you the function and package name.\nIf the package isn’t loaded, use ?package_name::function_name or specify the package in the help() function. When you aren’t sure what package the function is in, use the shortcut ??function_name.\n\nUse the methods above to get help for the beepr::beep() function.\n\n\n\nSolution\n\n# if the package is loaded\n?beepr\nhelp(\"beepr\")\n\n# works whether or not the package is loaded\n?beepr::beep\nhelp(\"beep\", package=\"beepr\") \n\n# shows a list of potentially matching functions\n??beep\n\n\n\nFunction help is always organised in the same way. For example, look at the help for ?beepr::beep. At the top, it tells you the name of the function and its package in curly brackets, then a short description of the function, followed by a longer description. The Usage section shows the function with all of its arguments. If any of those arguments have default values, they will be shown like function(arg = default). The Arguments section lists each argument with an explanation. There may be a Details section after this with even more detail about the functions. The Examples section is last, and shows examples that you can run in your console window to see how the function works.\n\nUse function help to answer the following questions.\n\nWhat is the first argument to the mean function? \ntrim\nna.rm\nmean\nx\n\nWhat package is read_excel in? \nreadr\nreadxl\nbase\nstats",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-code-basics",
    "href": "01-intro.html#sec-code-basics",
    "title": "1  Introduction",
    "section": "\n1.3 Code Basics",
    "text": "1.3 Code Basics\n\n1.3.1 Arguments\nYou can look up the arguments/options that a function has by using the help documentation. Some arguments are required, and some are optional. Optional arguments will often use a default (normally specified in the help documentation) if you do not enter any value.\n\nAs an example, look at the help documentation for the function sample() which randomly samples items from a list.\n\n\n\nRun in the console\n\n?sample\n\n\n\nThe help documentation for sample() should appear in the bottom right help panel. In the usage section, we see that sample() takes the following form:\n\nsample(x, size, replace = FALSE, prob = NULL)\n\nIn the arguments section, there are explanations for each of the arguments. x is the list of items we want to choose from, size is the number of items we want to choose, replace is whether or not each item may be selected more than once, and prob gives the probability that each item is chosen. In the details section it notes that if no values are entered for replace or prob it will use defaults of FALSE (each item can only be chosen once) and NULL (all items will have equal probability of being chosen). Because there is no default value for x or size, they must be specified otherwise the code won’t run.\n\nLet’s try an example and just change the required arguments x and size to ask R to choose from the set of letters (a built-in vector of the 26 lower-case Latin letters), 5 random values.\n\nsample(x = letters, size = 5)\n\n[1] \"z\" \"v\" \"y\" \"w\" \"j\"\n\n\n\n\n\n\n\n\n\nWhy are my letters different to your letters?\n\n\n\n\n\nsample() generates a random sample. Each time you run the code, you’ll generate a different set of random letters (try it). The function set.seed() controls the random number generator - if you’re using any functions that use randomness (such as sample()), running set.seed() will ensure that you get the same result (in many cases this may not be what you want to do). To get the same numbers we do, run set.seed(1242016) in the console, and then run sample(x = letters, size = 5) again.\n\n\n\nNow we can change the default value for the replace argument to produce a set of letters that is allowed to have duplicates.\n\nset.seed(8675309)\nsample(x = letters, size = 5, replace = TRUE)\n\n[1] \"t\" \"k\" \"j\" \"k\" \"m\"\n\n\nThis time R has still produced 5 random letters, but now this set of letters has two instances of “k”. Always remember to use the help documentation to help you understand what arguments a function requires.\n\n1.3.2 Argument names\nIn the above examples, we have written out the argument names in our code (i.e., x, size, replace), however, this is not strictly necessary. The following two lines of code would both produce the same result (although each time you run sample() it will produce a slightly different result, because it’s random, but they would still work the same):\n\nsample(x = letters, size = 5, replace = TRUE)\nsample(letters, 5, TRUE)\n\nImportantly, if you do not write out the argument names, R will use the default order of arguments. That is, for sample it will assume that the first value you enter is x, the second value is size and the third value is replace.\nIf you write out the argument names, then you can write the arguments in whatever order you like:\n\nsample(size = 5, replace = TRUE, x = letters)\n\nWhen you are first learning R, you may find it useful to write out the argument names as it can help you remember and understand what each part of the function is doing. However, as your skills progress you may find it quicker to omit the argument names and you will also see code examples online that do not use argument names, so it is important to be able to understand which argument each bit of code is referring to (or look up the help documentation to check).\nIn this course, we will always write out the argument names the first time we use each function. However, in subsequent uses they may be omitted.\n\n1.3.3 Tab auto-complete\nOne very useful feature of R Studio is tab auto-complete for functions. If you write the name of the function and then press the tab key, R Studio will show you the arguments that function takes along with a brief description. If you press enter on the argument name it will fill in the name for you, just like auto-complete on your phone. This is incredibly useful when you are first learning R and you should remember to use this feature frequently.\n\n\n\n\n\nFigure 1.2: Tab auto-complete\n\n\n\nUse tab autocomplete to figure out the arguments to rnorm(). Create a vector of 20 numbers from a normal distribution with a mean of 100 and a standard deviation of 10.\n\n\n\nSolution\n\nrnorm(n = 20, mean = 100, sd = 10)\n\n [1] 120.29392 110.65416 109.87220 100.27454 106.72872 105.72067 109.03678\n [8]  84.50448 110.22638 101.50083  93.40036  90.05411 119.72459  95.58198\n[15]  90.99363  98.49412  91.72106 119.85826 100.44005  95.95718\n\n\n\n\n\n1.3.4 Objects\nA large part of your coding will involve creating and manipulating objects. Objects contain stuff. That stuff can be numbers, words, or the result of operations and analyses. You assign content to an object using &lt;- or = (we will use &lt;- in this book).\n\nRun the following code in the console, but change the values of name and age to your own details and change halloween to a holiday or date you care about.\n\n\n\nRun in the console\n\nname &lt;- \"Lisa\"\nage &lt;- 47\ntoday &lt;- Sys.Date()\nhalloween &lt;- as.Date(\"2024-10-31\")\n\n\n\nYou’ll see that four objects now appear in the environment pane:\n\n\nname is character (text) data. In order for R to recognise it as text, it must be enclosed in double quotation marks \" \".\n\nage is numeric data. In order for R to recognise this as a number, it must not be enclosed in quotation marks.\n\ntoday stores the result of the function Sys.Date(). This function returns your computer system’s date. Unlike name and age, which are hard-coded (i.e., they will always return the values you enter), the contents of the object today will change dynamically with the date. That is, if you run that function tomorrow, it will update the date to tomorrow’s date.\n\nhalloween is also a date but it’s hard-coded as a specific date. It’s wrapped within the as.Date() function that tells R to interpret the character string you provide as a date rather than text.\n\n\nTo print the contents of an object, type the object’s name in the console and press enter. Try printing all four objects now.\n\nFinally, a key concept to understand is that objects can interact and you can save the results of those interactions in new object.\n\nEdit and run the following code to create these new objects, and then print the contents of each new object.\n\n\n\nRun in the console\n\ndecade &lt;- age + 10\nfull_name &lt;- paste(name, \"DeBruine\")\nhow_long &lt;- halloween - today",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-help",
    "href": "01-intro.html#sec-help",
    "title": "1  Introduction",
    "section": "\n1.4 Getting help",
    "text": "1.4 Getting help\nYou will feel like you need a lot of help when you’re starting to learn. This won’t really go away; it’s impossible to memorise everything. The goal is to learn enough about the structure of R that you can look things up quickly. This is why we’ll introduce specialised jargon in the glossary for each chapter; it’s easier to google “convert character to numeric in R” than “make numbers in quotes be actual numbers not words”. In addition to the function help described above, here are some additional resources you should use often.\n\n1.4.1 Package reference manuals\nStart up help in a browser by entering help.start() in the console. Click on Packages under Reference to see a list of packages. Scroll down to the readxl package and click on it to see a list of the functions that are available in that package.\n\n1.4.2 Googling\nIf the function help doesn’t help, or you’re not even sure what function you need, try Googling your question. It will take some practice to be able to use the right jargon in your search terms to get what you want. It helps to put “R” or “tidyverse” in the search text, or the name of the relevant package, like “ggplot2”.\n\n1.4.3 AI\nGenerative AI platforms have exploded in popularity, particularly when it comes to coding. Because of this, we have created a companion book AITutoR to show you how to use AI responsibly to support your coding journey.\n\n1.4.4 Vignettes\nMany packages, especially tidyverse ones, have helpful websites with vignettes explaining how to use their functions. Some of the vignettes are also available inside R. You can access them from a package’s help page or with the vignette() function.\n\n\n\nRun in the console\n\n# opens a list of available vignettes\nvignette(package = \"ggplot2\")\n\n# opens a specific vignette in the Help pane\nvignette(\"ggplot2-specs\", package = \"ggplot2\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-exercises-intro",
    "href": "01-intro.html#sec-exercises-intro",
    "title": "1  Introduction",
    "section": "\n1.5 Exercises",
    "text": "1.5 Exercises\n\n1.5.1 Restart R\nRestart R, not RStudio. This should clear the environment tab, but not close your application window.\n\n\n\nSolution\n\n\nMenu: Session &gt; Restart R\n\n\nCmd-Shift-F10 or Ctl-Shift-F10\n\ntype .rs.restartR() in the console\n\n\n\n\n1.5.2 Install a Package\nInstall the faux package from CRAN.\n\n\n\nSolution\n\ninstall.packages(\"faux\")   # for simulating data\n\n\n\n1.5.3 Load a Package\nLoad the faux package.\n\n\n\nSolution\n\nlibrary(faux)\n\n\n\n1.5.4 Use a Function\nRun the make_id() function. Use tab autocomplete to figure out what the arguments are, and make it generate the following:\n\n\n[1] \"P01_control\" \"P02_control\" \"P03_control\" \"P04_control\" \"P05_control\"\n\n\n\n\n\nSolution\n\nmake_id(n = 5, prefix = \"P\", digits = 2, suffix = \"_control\")\n\n# this also works, if you keep the arguments in this order\nmake_id(5, \"P\", 2, \"_control\")\n\n\n\n1.5.5 Get Help\nView the help files for faux::rnorm_multi.\n\n\n\nSolution\n\n?rnorm_multi\n\n\n\n1.5.6 Create an Object\nUse faux::rnorm_multi() to create an object called sim_data. Set the arguments so this is a table with 10 rows and 3 columns. Click on the object name in the Environment tab to view the table, or print it to the console.\n\n\n\nSolution\n\nsim_data &lt;- rnorm_multi(n = 10, vars = 3)\n\nsim_data # prints to the console\n\n\n\n\nX1\nX2\nX3\n\n\n\n-0.6220157\n1.0791335\n0.8261559\n\n\n-1.1765128\n-1.1998364\n0.7951251\n\n\n-0.3289682\n-0.9865283\n1.1043681\n\n\n-2.0446042\n-0.9067866\n-2.9012889\n\n\n-0.2315572\n0.6659664\n0.6554194\n\n\n1.1568557\n1.6956159\n1.0975918\n\n\n-0.3316717\n-0.7358019\n0.9308105\n\n\n-0.3843754\n-0.7209335\n-0.3094711\n\n\n1.0481596\n0.3665779\n-0.1582473\n\n\n-1.3933121\n-1.7554237\n-0.1497308",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-glossary-intro",
    "href": "01-intro.html#sec-glossary-intro",
    "title": "1  Introduction",
    "section": "Glossary",
    "text": "Glossary\nThe glossary at the end of each chapter defines common jargon you might encounter while learning R. This specialised vocabulary can help you to communicate more efficiently and to search for solutions to problems. The terms below link to our PsyTeachR glossary, which contains further information and examples.\n\n\n\n\nterm\ndefinition\n\n\n\nargument\nA variable that provides input to a function.\n\n\nbase-r\nThe set of R functions that come with a basic installation of R, before you add external packages.\n\n\ncharacter\nA data type representing strings of text.\n\n\nchunk\nA section of code in an R Markdown file\n\n\nconflict\nHaving two packages loaded that have a function with the same name.\n\n\ncran\nThe Comprehensive R Archive Network: a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R.\n\n\ndata-wrangling\nThe process of preparing data for visualisation and statistical analysis.\n\n\ndefault-value\nA value that a function uses for an argument if it is skipped.\n\n\nfactor\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\nfunction\nA named section of code that can be reused.\n\n\nide\nIntegrated Development Environment: a program that serves as a text editor, file manager, and provides functions to help you read and write code. RStudio is an IDE for R.\n\n\nnumeric\nA data type representing a real decimal number or integer.\n\n\nobject\nA word that identifies and stores the value of some data for later use.\n\n\npackage\nA group of R functions.\n\n\npanes\nRStudio is arranged with four window “panes”.\n\n\nquarto\nAn open-source scientific and technical publishing system.\n\n\nrender\nTo create a file (usually an image or PDF) or widget from source code\n\n\nreproducible-research\nResearch that documents all of the steps between raw data and results in a way that can be verified.\n\n\nscript\nA plain-text file that contains commands in a coding language, such as R.\n\n\nstring\nA piece of text inside of quotes.\n\n\nvector\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-resources-intro",
    "href": "01-intro.html#sec-resources-intro",
    "title": "1  Introduction",
    "section": "Further Resources",
    "text": "Further Resources\n\nRStudio IDE Cheatsheet\nRStudio Cloud",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-reports.html",
    "href": "02-reports.html",
    "title": "2  Reports",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-ilo-reports",
    "href": "02-reports.html#sec-ilo-reports",
    "title": "2  Reports",
    "section": "",
    "text": "Structure a project\nRender a simple reproducible report with quarto\nCreate code chunks, tables, images, and inline R\nAdd a bibliography and citations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#functions-reports",
    "href": "02-reports.html#functions-reports",
    "title": "2  Reports",
    "section": "Functions used",
    "text": "Functions used\n\nbuilt-in (you can always use these without loading any packages)\n\nbase:: max(), min(), nrow(), str(), summary()\n\nutils:: View()\n\n\n\ntidyverse (you can use all these with library(tidyverse))\n\nreadr:: readr::read_csv(), readr::row_spec()\n\ndplyr:: dplyr::count(), dplyr::filter()\n\nggplot2:: ggplot2::aes(), ggplot2::geom_point(), ggplot2::ggplot(), ggplot2::labs()\n\n\n\nother (you need to load each package to use these)\n\ntinytex:: tinytex::install_tinytex()\n\n\n\n\nDownload the Quarto Cheat Sheet and Markdown Cheat Sheet.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-setup-reports",
    "href": "02-reports.html#sec-setup-reports",
    "title": "2  Reports",
    "section": "Setup",
    "text": "Setup\nFor reference, here are the packages we will use in this chapter. You may need to install them, as explained in Section 1.2.1, if running the code below in the console pane gives you the error Error in library(package_name) : there is no package called ‘package_name’.\n\n\n\nChapter packages\n\nlibrary(tidyverse) # various data manipulation functions\nlibrary(quarto)    # for rendering a report from a script",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#why-use-reproducible-reports",
    "href": "02-reports.html#why-use-reproducible-reports",
    "title": "2  Reports",
    "section": "\n2.1 Why use reproducible reports?",
    "text": "2.1 Why use reproducible reports?\nHave you ever worked on a report, creating a summary table for the demographics, making beautiful plots, getting the analysis just right, and copying all the relevant numbers into your manuscript, only to find out that you forgot to exclude a test run and have to redo everything?\nA reproducible report fixes this problem. Although this requires a bit of extra effort at the start, it will more than pay you back by allowing you to update your entire report with the push of a button whenever anything changes.\nAdditionally, studies show that many, if not most, papers in the scientific literature have reporting errors. For example, more than half of over 250,000 psychology papers published between 1985 and 2013 have at least one value that is statistically incompatible, such as a p-value that is not possible given a t-value and degrees of freedom (Nuijten et al., 2016). Reproducible reports help avoid transcription and rounding errors.\nWe will make reproducible reports following the principles of literate programming. The basic idea is to have the text of the report together in a single document along with the code needed to perform all analyses and generate the tables. The report is then “compiled” from the original format into some other, more portable format, such as HTML or PDF. This is different from traditional cutting and pasting approaches where, for instance, you create a graph in Microsoft Excel or a statistics program like SPSS and then paste it into Microsoft Word.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-projects",
    "href": "02-reports.html#sec-projects",
    "title": "2  Reports",
    "section": "\n2.2 Projects",
    "text": "2.2 Projects\nBefore we write any code, first, we need to get organised. Projects in RStudio are a way to group all the files you need for one project. Most projects include scripts, data files, and output files like the PDF report created by the script or images.\n\n2.2.1 File System\nModern computers tend to hide the file system from users, but we need to understand a little bit about how files are stored on your computer in order to get a script to find your data. Your computer’s file system is like a big box (or directory) that contains both files and smaller boxes, or “subdirectories”. You can specify the location of a file with its name and the names of all the directories it is inside.\nFor example, if Lisa is looking for a file called report.qmdon their Desktop, they can specify the full file path like this: /Users/lisad/Desktop/report.qmd, because the Desktop directory is inside the lisad directory, which is inside the Users directory, which is located at the base of the whole file system. If that file was on your desktop, you would probably have a different path unless your user directory is also called lisad. You can also use the ~ shortcut to represent the user directory of the person who is currently logged in, like this: ~/Desktop/report.qmd.\n\n2.2.2 Default working directory\nFirst, make a new directory (i.e., folder) on your computer where you will keep all of your R projects. Name it something like “R-projects” (avoid spaces and other special characters). Make sure you know how to get to this directory using your computer’s Finder or Explorer.\n\n\n\n\n\n\nAvoid networked drives\n\n\n\n\n\nIf possible, don’t use a network or cloud drive (e.g., OneDrive or Dropbox), as this can sometimes cause problems. If you’re working from a networked drive and you are having issues, a helpful test is to try moving your project folder to the desktop to see if that solves the problem.\n\n\n\nNext, open Tools &gt; Global Options…, navigate to the General pane, and set the “Default working directory (when not in a project)” to this directory. Now, if you’re not working in a project, any files or images you make will be saved in this working directory.\n\n\n\n\n\n\nAvoid long path names\n\n\n\n\n\nOn some versions of Windows 10 and 11, it can cause problems if path names are longer than 260 characters. Set your default working directory to a path with a length well below that to avoid problems when R creates temporary files while rendering a report. If you are having issues, a helpful test is to try moving your project folder to the desktop to see if that solves the problem as this will likely have a much short path name than most other folders on your computer.\n\n\n\nYou can set the working directory to another location manually with menu commands: Session &gt; Set Working Directory &gt; Choose Directory… However, there’s a better way of organising your files by using Projects in RStudio.\n\n2.2.3 Start a Project\nTo create a new project for the work we’ll do in this book:\n\nFile &gt; New Project…\nSelect New Directory\n\nSelect New Project\n\nName the project reprores\n\nSave it inside the default R-projects directory\nClick Create Project\n\n\nRStudio will restart itself and open with this new project directory as the working directory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Starting a new project.\n\n\nClick on the Files tab in the lower right pane to see the contents of the project directory. You will see a file called reprores.Rproj, which is a file that contains all of the project information. When you’re in the Finder/Explorer, you can double-click on it to open up the project.\n\n\n\n\n\n\nDot files\n\n\n\nDepending on your settings, you may also see a directory called .Rproj.user, which contains your specific user settings. You can ignore this and other “invisible” files that start with a full stop.\n\n\n\n\n\n\n\n\nDon’t nest projects\n\n\n\nDon’t ever save a new project inside another project directory. This can cause some hard-to-resolve problems.\n\n\n\n2.2.4 Naming things\nBefore we start creating new files, it’s important to review how to name your files. This might seem a bit pedantic, but following clear naming rules so that both people and computers can easily find things will make your life much easier in the long run. Here are some important principles:\n\nfile and directory names should only contain letters, numbers, dashes, and underscores, with a full stop (.) between the file name and extension (that means no spaces!)\nbe consistent with capitalisation (set a rule to make it easy to remember, like always use lowercase)\nuse underscores (_) to separate parts of the file name, like the title and date, and dashes (-) to separate words in each part (e.g., thesis-analysis_2024-10-31.Rmd)\nname files with a pattern that alphabetises in a sensible order and makes it easy for you to find the file you’re looking for\nprefix a file name with an underscore to move it to the top of the list, or prefix all files with numbers to control their order\n\nFor example, these file names are a mess:\n\nreport.doc\nreport final.doc\nData (Customers) 11-15.xls\nCustomers Data Nov 12.xls\nfinal report2.doc\nproject notes.txt\nVendor Data November 15.xls\n\nHere is one way to structure them so that similar files have the same structure and it’s easy for a human to scan the list or to use code to find relevant files. See if you can figure out what the last one should be.\n\n_project-notes.txt\nreport_v1.doc\nreport_v2.doc\nreport_v3.doc\ndata_customer_2021-11-12.xls\ndata_customer_2021-11-15.xls\n\nvendor-data_2021-11-15.xls\ndata-vendor-2021_11_15.xls\ndata_vendor_2021-11-15.xls\ndata_2021-11-15_vendor.xls",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#naming-practice",
    "href": "02-reports.html#naming-practice",
    "title": "2  Reports",
    "section": "\n2.3 Naming practice",
    "text": "2.3 Naming practice\nThink of other ways to name the files above. Look at some of your own project files and see what you can improve.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-quarto",
    "href": "02-reports.html#sec-quarto",
    "title": "2  Reports",
    "section": "\n2.4 Quarto",
    "text": "2.4 Quarto\nThroughout this course we will use quarto to create reproducible reports with a table of contents, text, tables, images, and code. The text can be written using markdown, which is a way to specify formatting, such as headers, paragraphs, lists, bolding, and links. Code is placed in code chunks.\n\n\n\n\n\n\nQuarto vs R Markdown\n\n\n\nYou may have learned R Markdown in other classes, or see .Rmd files in other people’s projects. Quarto is basically a newer and more general version of R Markdown, with many improvements. The formatting is very similar, and you can often convert R Markdown files by changing the file extension from .Rmd to .qmd with no or very few other changes.\n\n\n\n2.4.1 New document\nTo open a new quarto document, click File &gt; New File &gt; Quarto Document…. You will be prompted to give it a title; title it Reports. You can also change the author name. Keep the output format as HTML. Save the file as 02-reports.qmd.\n\n\n\n\n\n\nSource versus visual editor\n\n\n\n\n\nYou can use the visual editor if you have RStudio version 1.4 or higher. This will be a button at the top of the source pane and the menu options should be very familiar to anyone who has worked with software like Microsoft Word. However, the examples in the rest of this book are shown for the source editor, not the visual editor, so delete the line editor: visual if needed.\nIn the visual editor, you won’t see the hashes that create headers, or the asterisks that create bold and italic text. You also won’t see the backticks that demarcate inline code.\n\n\n\n\n\nFigure 2.2: The example code above shown in the visual editor.\n\n\nIf you try to add the hashes, asterisks and backticks to the visual editor, you will get frustrated as they disappear. If you succeed, your text in the regular editor will be full of backslashes and the code will not run.\n\n\n\n\n2.4.2 Header\nAt the top of the file, you will see some text between a pair of three dashes:\n---\ntitle: \"Reports\"\nauthor: \"Lisa DeBruine\"\nformat: html\n---\nThis is the YAML header, which provides information to quarto about how you want to render a document. Here, it sets the title, author, and format. Add a new line with the date, e.g., date: 2024-10-04.\nYou will learn in Section 2.6.3 how to further customise your document using information in the header.\n\n2.4.3 Markdown\nNow replace all of the text beneath the header with the following text. Make sure to skip a line or two after the three dashes.\n## Basic Markdown\n\nNow I can make:\n\n* headers\n* paragraphs\n* lists\n* [links](https://psyteachr.github.io/reprores-v4/)\nIf you start a line with hashes, it creates a header. One hash makes a document title, two hashes make a document header, three a subheader, and so on. Make sure you leave a blank line before and after a header, and don’t put any spaces or other characters before the first hash.\nPut a blank line between paragraphs of text. Bullet-point list items start with “*” or “-” and numbered list items start with “1.”. Indent list items to make nested lists.\n\n2.4.4 Text Styles\nSee Markdown Basics for a quick reference.\n\nAdd an ordered list of different text styles to your document, like bold, italic, strikethrough, subscript, superscript, code, and a task item.\n\n\n2.4.5 Code chunks\n\nAdd a new level-2 header called “Code Chunks”, skip a line, and add the following text at the end:\n\n```{r}\n# this is a code chunk\n```\n\n\nWhat you have created is a code chunk. In quarto, anything written between lines that start with three backticks is processed as code, and anything written outside is processed as markdown. This makes it easy to combine both text and code in one document. On the default RStudio appearance theme, code chunks are grey and plain text is white, but the actual colours will depend on which theme you have applied.\n\n\n\n\n\n\nCode chunk errors\n\n\n\nWhen you create a new code chunk you should notice that the grey box starts and ends with three backticks ```. One common mistake is to accidentally delete these backticks. Remember, code chunks and text entry are different colours - if the colour of certain parts of your Markdown doesn’t look right, check that you haven’t deleted the backticks.\n\n\n\nInside your code chunk, add the code you created in Section 1.3.4.\n\nname &lt;- \"Lisa\"\nage &lt;- 47\ntoday &lt;- Sys.Date()\nhalloween &lt;- as.Date(\"2024-10-31\")\n\n\n\n\n\n\n\n\nConsole vs scripts\n\n\n\nIn Chapter 1, we asked you to type code into the console. Now, we want you to put code into code chunks in quarto files to make the code reproducible. This way, you can re-run your code any time the data changes to update the report, and you or others can inspect the code to identify and fix any errors.\nHowever, there will still be times that you need to put code in the console instead of in a script, such as when you install a new package. In this book, code chunks will be labelled with whether you should run them in the console or add the code to a script.\n\n\n\n2.4.6 Running code\nWhen you’re working in a quarto document, there are several ways to run your lines of code.\nFirst, you can highlight the code you want to run and then click Run &gt; Run Selected Line(s), however this is tedious and can cause problems if you don’t highlight exactly the code you want to run.\nAlternatively, you can press the green “play” button at the top-right of the code chunk and this will run all lines of code in that chunk.\n\n\n\n\n\nFigure 2.3: Click the green arrow to run all the code in the current chunk.\n\n\nEven better is to learn some of the keyboard shortcuts for RStudio. To run a single line of code, make sure that the cursor is in the line of code you want to run (it can be anywhere) and press Ctrl+Enter or Cmd+Enter. If you want to run all of the code in the code chunk, press Ctrl+Shift+Enter or Cmd+Shift+Enter. Learn these short cuts; they will make your life easier!\n\n\nVideo\n\n\nFigure 2.4: Use the keyboard shortcut to run only highlighted code, or run one line at a time by placing the cursor on a line without highlighting anything.\n\n\n\nRun your code using each of the methods above. You should see the variables name, age, today, and halloween appear in the environment pane.\nRestart R to clear the objects. They should disappear from the environment (see Section A.1.2.2 if they don’t disappear).\nRun you code again, and then change the value of name in the script. When/how does it change in the Environment tab?\n\n\n2.4.7 Inline code\nOne important feature of quarto for reproducible reports is that you can combine text and code to insert values into your writing using inline coding. If you’ve ever had to copy and paste a value or text from one file to another, you’ll know how easy it can be to make mistakes. Inline code avoids this.\n\nAdd a new level-2 header called “Inline Code”, then copy and paste the text below. If you used a different variable name than halloween, you should update this with the name of the object you created, but otherwise don’t change anything else.\nMy name is `r name` and I am `r age` years old. \nIt is `r halloween - today` days until Halloween, \nwhich is my favourite holiday.\n\n\n2.4.8 Rendering your file\nNow we are going to render the file into a document type of our choosing. In this case we’ll create a default html file, but you will learn how to create other files like Word and PDF in Section 2.6.5. To render your file, click the Render button at the top of the source pane.\nThe console pane will open a tab called “Background Jobs”. This is because quarto is not an R package, but a separate application on your computer. You can make this application run with commands from R, or run it from the command line yourself. You may see some text in the Background Jobs window, like “Processing file: 02-reports.qmd” and eventually “Output created: 02-reports.html”. Your rendered html file may pop up in a separate web browser, a pop-up window in RStudio, or in the Viewer tab of the lower right pane, depending on your RStudio settings.\nThat slightly odd bit of text you copied and pasted now appears as a normal sentence with the values pulled in from the objects you created.\n\nMy name is Lisa and I am 47 years old. It is 28 days until Halloween, which is my favourite holiday.\n\n\n\n\n\n\n\nRendering with Code\n\n\n\n\n\nYou can also render by typing the following code into the console. Never put this in a qmd script itself, or it will try to render itself in an infinite loop.\n\n\n\nRun in the console\n\nquarto::quarto_render(\"02-reports.qmd\")\n\n\n\n\n\n\nEdit your file to put the code chunk that defines the objects name, age, today and halloween after the inline text that uses it and render. What happened and why?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#writing-a-report",
    "href": "02-reports.html#writing-a-report",
    "title": "2  Reports",
    "section": "\n2.5 Writing a report",
    "text": "2.5 Writing a report\nWe’re going to write a basic report for this dataset using quarto to show you some more of the features. We’ll be expanding on almost every bit of what we’re about to show you throughout this course; the most important outcome is that you start to get comfortable with how quarto works and what you can use it to do.\n\n2.5.1 Setup Chunk\nMost of your quarto documents should have a setup chunk at the top that loads any necessary libraries and sets default values.\n\nAdd the following just below the YAML header.\n\n```{r}\n#| label: setup\n#‎| include: false\n\nlibrary(tidyverse)\n```\n\n\nThe function library(tidyverse) makes tidyverse functions available to your script. You should always add the packages you need in your setup chunk. Often when you are working on a script, you will realize that you need to load another add-on package. Don’t bury the call to library(package_I_need) way down in the script. Put it in the setup chunk so the user has an overview of what packages are needed.\n\n2.5.2 Chunk Options\nThe chunk execution option label above designates this as the setup chunk, and the include option makes sure that this chunk and any output it produces don’t end up in your rendered document.\nChunk options are structured like #| option: value, and go at the very top of a code chunk. You can also set default values in the YAML header under execute: (see Section 2.6.1 below).\n\n\n\n\n\n\nWarning\n\n\n\nMake sure there are no blank lines, code, or comments before any chunk options, otherwise the options will not be applied.\n\n\n\n2.5.3 Online sources\nNow, rather than using objects we have created from scratch, we will read in a data file. First, let’s try loading data that is stored online.\n\nCreate a new level 2 header called “Data Analysis”, add a code chunk below it, and copy, paste, and run the below code. This code loads some simulated experiment data.\n\nsmalldata &lt;- read_csv(\"https://psyteachr.github.io/reprores/data/smalldata.csv\")\n\n\n\nThe data is stored in a .csv file so we’re going to use the read_csv() function to load it in.\nNote that the url is contained within double quotation marks - it won’t work without this.\nYou should see a message that starts with “Rows: 10 Columns: 4”, you can ignore this for now.\n\n\n\n\n\n\n\nCould not find function\n\n\n\nIf you get an error message that looks like:\n\nError in read_csv(“https://psyteachr.github.io/reprores/data/smalldata.csv”) :\ncould not find function “read_csv”\n\nThis means that you have not loaded tidyverse. Check that library(tidyverse) is in the setup chunk and that you have run the setup chunk.\n\n\nThis dataset is a few lines of simulated data for an experiment with 10 participants, 2 groups (experimental and control) and two dependent measures (pre and post). There are multiple ways to view and check a dataset in R. Do each of the following and make a note of what information each approach seems to give you. If you’d like more information about each of these functions, you can look up the help documentation with ?function:\nClick on the smalldata object in the environment pane, or run each of the following lines of code in the console:\n\n\n\nRun in the console\n\n# different ways to view a data frame\nhead(smalldata)\nsummary(smalldata)\nstr(smalldata)\nView(smalldata)\n\n\n\n2.5.4 Local data files\nMore commonly, you will be working from data files that are stored locally on your computer. But where should you put all of your files? You usually want to have all your scripts and data files for a single project inside one folder on your computer, that project’s working directory, and we have already set up the main directory reproresfor this course.\nYou can organise files in subdirectories inside this main project directory, such as putting all raw data files in a subdirectory called data and saving any image files to a subdirectory called images. Using subdirectories helps avoid one single folder becoming too cluttered, which is important if you’re working on big projects.\nIn your reprores directory, create a new folder named data, download a copy of the data file, and save it in this new subdirectory.\nTo load in data from a local file, again we can use the read_csv() function, but this time rather than specifying a url, give it the subdirectory and file name.\n\nChange the code in your file to the following.\n\nsmalldata &lt;- read_csv(\"data/smalldata.csv\")\n\n\n\n\n\n\n\n\nTab-autocomplete file names\n\n\n\nUse tab auto-complete when typing file names in a code chunk. After you type the first quote, hit tab to see a drop-down menu of the files in your working directory. You can start typing the name of the subdirectory or file to narrow it down. This is really useful for avoiding annoying errors because of typos or files not being where you expect.\n\n\nThings to note:\n\nYou must include the file extension (in this case .csv)\nThe subdirectory folder name (data) and the file name are separated by a forward slash /\n\nPrecision is important, if you have a typo in the file name it won’t be able to find your file; remember that R is case sensitive - SmallData.csv is a completely different file to smalldata.csv as far as R is concerned.\n\n\nRun head(), summary(), str(), and View() on smalldata to confirm that the data is the same as before.\n\n\n2.5.5 Data analysis\nFor this report we’re just going to present some simple stats for two groups: “control” and “exp”. We’ll come back to how to write this kind of code yourself in Chapter 4. For now, see if you can follow the logic of what the code is doing via the code comments.\n\nCreate a new code chunk, then copy, paste and run the following code and then view group_counts by clicking on the object in the environment pane.\n\n# count how many are in each group\ngroup_counts &lt;- count(smalldata, group)\n\n\nBecause each row of the dataset is a participant, this code gives us a nice and easy way of seeing how many participants were in each group; it just counts the number of rows in each group.\n\n\n\n\n\ngroup\nn\n\n\n\ncontrol\n5\n\n\nexp\n5\n\n\n\n\n\n\n\nCopy and paste the text below into the white space below the code chunk that loads in the data. Save the file and then render to view the results.\nThe total number of participants in the **control** condition was 5.\n\nTry and match up the inline code with what is in the group_counts table. Of note:\n\nThe $ sign is used to indicate specific variables (or columns) in an object using the object$variable syntax.\nSquare brackets with a number e.g., [1], indicate a particular observation\nSo group_counts$n[1] asks the inline code to display the first observation of the variable n in the dataset group_counts.\n\n\nAdd another line that reports the total numbers of participants in the experimental condition using inline code. Using either the visual editor or text markups, add in bold and italics so that it matches the others.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe total number of participants in the **experimental** condition was `r group_counts$n[2]`.\n\n\n\n\n\n2.5.6 Code comments\nIn the above code we’ve used code comments and it’s important to highlight how useful these are. You can add comments inside R chunks with the hash symbol (#). R will ignore characters from the hash to the end of the line.\n\n# important numbers\n\nn &lt;- nrow(smalldata) # the total number of participants (number of rows)\npre &lt;- mean(smalldata$pre) # the mean of the pre column\npost &lt;- mean(smalldata$post) # the mean of the post column\n\nIt’s usually good practice to start a code chunk with a comment that explains what you’re doing there, especially if the code is not explained in the text of the report.\nIf you name your objects clearly, you often don’t need to add clarifying comments. For example, if I’d named the three objects above total_participants, mean_pre and mean_post, I would omit the comments. It’s a bit of an art to comment your code well, but try to add comments as you’re working through this book - it will help consolidate your learning and when future you comes to review your code, you’ll thank past you for being so clear.\n\n2.5.7 Images\nAs the saying goes, a picture paints a thousand words, and sometimes you will want to communicate your data using visualisations.\nCreate a code chunk to display a graph of the data in your document after the text we’ve written so far. We’ll use some code that you’ll learn more about in Chapter 3 to make a simple bar chart that represents the sales data – focus on trying to follow how bits of the code map on to the plot that is created.\n\nAdd a new level-3 header called “Visualisation”. Copy and paste the code below into a new chunk. Run the code in your script to see the plot it creates and then render the file to see how it is displayed in your document.\n\nggplot(data = smalldata, \n       mapping = aes(x = pre, \n                     y = post, \n                     color = group)) +\n  geom_point() +\n  labs(x = \"Pre-test Score\",\n       y = \"Post-test Score\")\n\n\n\n\n\n\n\n\nYou can also include images that you did not create in R using the markdown syntax for images. This is very similar to loading data in that you can either use an image that is stored on your computer, or via a url.\nThe general syntax for adding an image in markdown is ![caption](url){#fig-name}. You can leave the caption blank, but must include the square brackets. The curly brackets are optional, and allow you to reference the figure as @fig-name (change the “name” part for each new figure). You can also add other formatting options in the curly brackets, like an image width or CSS styles.\n![The ReproRes logo](images/logos/logo.png){#fig-logo width=\"33%\"}\n\n\n\n\n\nFigure 2.5: The ReproRes logo\n\n\n\n\n\n\n\n\nImage Licenses\n\n\n\n\n\nMost images on Wikipedia are public domain or have an open license. You can search for images by license on Google Images by clicking on the Tools button and choosing “Creative Commons licenses” from the “Usage Rights” menu.\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.8 Tables\nRather than a figure, we might want to display our data in a table.\n\nAdd a new level 3 heading to your document, name the heading “Tables” and then create a new code chunk below this.\n\nsmalldata\n\n\nFirst, let’s see what the table looks like if we don’t make any edits. Simply write the name of the table you want to display in the code chunk (in our case smalldata) and then render to see what it looks like.\n# A tibble: 10 × 4\n   id    group     pre  post\n   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 S01   control  98.5 107. \n 2 S02   control 104.   89.1\n 3 S03   control 105.  124. \n 4 S04   control  92.4  70.7\n 5 S05   control 124.  125. \n 6 S06   exp      97.5 102. \n 7 S07   exp      87.8 126. \n 8 S08   exp      77.2  72.3\n 9 S09   exp      97.0 109. \n10 S10   exp     102.  114. \nThis isn’t very pretty, but we can change the print style.\n\nChange the line format: html in the YAML header to the following.\n---\nformat: \n  html:\n    df-print: kable\n---\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to keep the spaces exactly the same (YAML is very picky about spaces). In YAML, if a key: value pair doesn’t have any sub-options, you can write it on one line, like format: html. But if you want to set any html options, you have to indent it like above.\n\n\n\n2.5.9 Cross references\nYou can automatically number your figures and tables by giving them labels that start with fig- or tbl-, and referring to them in the text like @fig-name or @tbl-name (see quarto cross references for more details).\n\nAdd the following text above the chunk containing the table:\nAll data are shown in @tbl-raw-data.\nAlso, add the two commented lines below to the top of the code chunk:\n#| label: tbl-raw-data\n#| tbl-cap: The raw data from the study.\n\nThese set the figure label so you can reference it in the document, and the table caption. The label must start with “tbl-” to automatically add it to the numbered list of tables. Now, when you render your document, tables will display in “kable” format, which looks much nicer.\nAll data are shown in Table 2.1.\n\n\n\nTable 2.1: The raw data from the study.\n\n\n\n\n\nid\ngroup\npre\npost\n\n\n\nS01\ncontrol\n98.46606\n106.70508\n\n\nS02\ncontrol\n104.39774\n89.09030\n\n\nS03\ncontrol\n105.13377\n123.67230\n\n\nS04\ncontrol\n92.42574\n70.70178\n\n\nS05\ncontrol\n123.53268\n124.95526\n\n\nS06\nexp\n97.48676\n101.61697\n\n\nS07\nexp\n87.75594\n126.30077\n\n\nS08\nexp\n77.15375\n72.31229\n\n\nS09\nexp\n97.00283\n108.80713\n\n\nS10\nexp\n102.32338\n113.74732\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced table customisation\n\n\n\n\n\nIf you’re feeling confident with what we have covered so far, you can also explore the gt package, which is complex, but allows you to create beautiful customised tables. Riding tables with {gt} and {gtExtras} is an outstanding tutorial.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#refining-your-report",
    "href": "02-reports.html#refining-your-report",
    "title": "2  Reports",
    "section": "\n2.6 Refining your report",
    "text": "2.6 Refining your report\n\n2.6.1 Execution defaults\nLet’s finish by tidying up the report and organising our code a bit better.\nYou can set more default options for your document in the YAML header. The help pages for quarto execution options has a full list of options. However, the most useful and common options to change for the purposes of writing reports revolve around whether you want to show your code and the size of your images.\nAdd the code below to your YAML header and then try changing each option from false to true and changing the numeric values then render the file again to see the difference it makes.\n---\nexecute:\n  echo: false     # whether to show code chunks\n  message: false  # whether to show messages from your code\n  warning: false  # whether to show warnings from your code\n  fig-width: 8    # figure width in inches (at 96 dpi)\n  fig-height: 5   # figure height in inches (at 96 dpi)\n---\nYou can also override defaults in a code cell. See quarto code cells help for a full list of options.\n\n\n\n\n\n\nFigure versus output dimensions\n\n\n\n\n\nNote that fig-width and fig-height control the original size and aspect ratio of images generated by R, such as plots. This will affect the relative size of text and other elements in plots. It does not affect the size of existing images at all. However, out-width controls the display size of both existing images and figures generated by R. This is usually set as a percentage of the page width.\n\n```{r}\n#| label: fig-full-100\n#| fig-width: 8\n#| fig-height: 5\n#| out-width: '100%'\n#| fig-cap: A plot with the default values\nggplot2::last_plot()\n```\n\n\n\n\n\n\nFigure 2.6: A plot with the default values\n\n\n\n\n\n```{r}\n#| label: fig-half-100\n#| fig-width: 4\n#| fig-height: 2.5\n#| out-width: '100%'\n#| fig-cap: The same plot with half the default width and height\n\nggplot2::last_plot()\n```\n\n\n\n\n\n\nFigure 2.7: The same plot with half the default width and height\n\n\n\n\n\n```{r}\n#| label: fig-half-50\n#| fig-width: 4\n#| fig-height: 2.5\n#| out-width: '50%'\n#| fig-cap: The same plot as above at half the output width\nggplot2::last_plot()\n```\n\n\n\n\n\n\nFigure 2.8: The same plot as above at half the output width\n\n\n\n\n\n\n\n\n2.6.2 Override defaults\nThese setup options change the behaviour for the entire document, however, you can override the behaviour for individual code chunks.\nFor example, by default you might want to hide your code but there also might be an occasion where you want to show the code you used to analyse your data. You can set echo = FALSE in your setup chunk to make hiding code the default but in the individual code chunk for your plot set echo = TRUE. Try this now and knit the file to see the results.\nAdditionally, you can also override the default image display size or dimensions.\n\n```{r}\n#| label: fig-change-height\n#| fig-width: 10\n#| fig-height: 5\nggplot(data = smalldata, \n       mapping = aes(x = pre, \n                     y = post, \n                     color = group)) +\n  geom_point() +\n  labs(x = \"Pre-test Score\",\n       y = \"Post-test Score\",\n       title = \"Relationship between pre- and post-test by group\")\n```\n\n\n\n\n\n\nFigure 2.9\n\n\n\n\n\n2.6.3 YAML options\nQuarto HTML reference\nFinally, the YAML header is the bit at the very top of your quarto document. You can set several options here as well.\n\n\n\n\n\n\nNote\n\n\n\nUpdate the format section. Try changing the values from false to true to see what the options do.\n---\nformat:\n  html:\n    df-print: paged\n    theme: superhero\n    toc: true\n---\n\n\nThe df-print: paged option prints data frames using rmarkdown::paged_table() automatically. You can use df_print: kable to default to the simple kable style.\nThe built-in bootswatch themes are: default, cerulean, cosmo, darkly, flatly, journal, lumen, paper, readable, sandstone, simplex, spacelab, united, and yeti. You can view and download more themes. Try changing the theme to see which one you like best.\n\n\n\n\n\nFigure 2.10: Light themes in versions 3 and 4.\n\n\n\n\n\n\n\n\nYAML formatting\n\n\n\nYAML headers can be very picky about spaces and semicolons (the rest of R Markdown is much more forgiving). For example, if you put a space before “author”, you will get an error that looks like:\nError in yaml::yaml.load(..., eval.expr = TRUE) : \n  Parser error: while parsing a block mapping at line 1, \n  column 1 did not find expected key at line 2, column 2\nThe error message will tell you exactly where the problem is (the second character of the second line of the YAML header), and it’s usually a matter of fixing typos or making sure that the indenting is exactly right.\n\n\n\n2.6.4 Table of Contents\nThe table of contents is created by setting toc: true. This will use the markdown header structure to create the table of contents. The option toc-depth: 3 means that the table of contents will only display headers up to level 3 (i.e., those that start with three hashes: ###), and toc-expand sets wether the sections are expanded or collapsed.\n\nTry changing the values of the toc settings and re-render.\n---\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    toc-expand: true\n---\nAdd {-} after a header title to remove it from the table of contents, e.g.,\n## Basic Markdown {-}\n\n\n\n\n\n\n\nCaution\n\n\n\nIf your table of contents isn’t showing up correctly, this probably means that your headers are not set up right. Make sure that headers have no spaces before the hashes and at least one space after the hashes. For example, ##Analysis won’t display as a header and be added to the table of contents, but ## Analysis will.\n\n\n\n2.6.5 Formats\nSo far we’ve just rendered to html. To generate PDF reports, you need to install tinytex(Xie, 2022) and run the following code in the console (do not add this to your Rmd file):\n\n\n\nRun in the console\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\n\nOnce you’ve done this, update your YAML heading to add a pdf_document section and knit a PDF document. The options for PDFs are more limited than for HTML documents, so if you just replace html with pdf, you may need to remove some options if you get an error that looks like “Functions that produce HTML output found in document targeting PDF output.”\n---\nformat:\n  pdf:\n    df-print: kable\n    toc: TRUE\n---\nThere are many different formats you can render your document to, from HTML and PDF, to Word, Open Office, and ePub. You can also create websites, books, and presentations with a few small changes. See the quarto documentation for more information.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-bibliography",
    "href": "02-reports.html#sec-bibliography",
    "title": "2  Reports",
    "section": "\n2.7 Bibliography",
    "text": "2.7 Bibliography\nThere are several ways to do in-text references and automatically generate a bibliography in quarto. Quarto files need to link to a BibTex or JSON file (a plain text file with references in a specific format) that contains the references you need to cite. You specify the name of this file in the YAML header, like bibliography: refs.bib and cite references in text using an at symbol and a shortname, like [@tidyverse]. You can also include a Citation Style Language (.csl) file to format your references in, for example, APA style.\n---\nformat:\n  html:\n    toc: true\nbibliography: refs.bib\ncsl: apa.csl\n---\n\n2.7.1 Converting from reference software\nMost reference software like EndNote or Zotero has exporting options that can export to BibTeX format. You just need to check the shortnames in the resulting file.\n\n\n\n\n\n\nWarning\n\n\n\nPlease start using a reference manager consistently through your research career. It will make your life so much easier. Zotero is probably the best one.\n\n\n\n\nIf you don’t already have one, set up a Zotero account\n\nAdd the connector for your web browser (if you’re on a computer you can add browser extensions to)\n\nNavigate to Easing Into Open Science and add this reference to your library with the browser connector\n\nGo to your library and make a new collection called “Open Research” (click on the + icon after My Library)\n\nDrag the reference to Easing Into Open Science into this collection\n\nExport this collection as BibTex\n\n\n\n\n\n\n\nExport a bibliography file from Zotero\n\n\n\nThe exported file should look like this:\n\n@article{kathawalla_easing_2021,\n    title = {Easing {Into} {Open} {Science}: {A} {Guide} for {Graduate} {Students} and {Their} {Advisors}},\n    volume = {7},\n    issn = {2474-7394},\n    shorttitle = {Easing {Into} {Open} {Science}},\n    url = {https://doi.org/10.1525/collabra.18684},\n    doi = {10.1525/collabra.18684},\n    abstract = {This article provides a roadmap to assist graduate students and their advisors to engage in open science practices. We suggest eight open science practices that novice graduate students could begin adopting today. The topics we cover include journal clubs, project workflow, preprints, reproducible code, data sharing, transparent writing, preregistration, and registered reports. To address concerns about not knowing how to engage in open science practices, we provide a difficulty rating of each behavior (easy, medium, difficult), present them in order of suggested adoption, and follow the format of what, why, how, and worries. We give graduate students ideas on how to approach conversations with their advisors/collaborators, ideas on how to integrate open science practices within the graduate school framework, and specific resources on how to engage with each behavior. We emphasize that engaging in open science behaviors need not be an all or nothing approach, but rather graduate students can engage with any number of the behaviors outlined.},\n    number = {1},\n    urldate = {2022-09-07},\n    journal = {Collabra: Psychology},\n    author = {Kathawalla, Ummul-Kiram and Silverstein, Priya and Syed, Moin},\n    month = jan,\n    year = {2021},\n    pages = {18684},\n}\n\n2.7.2 Creating a BibTeX File\nYou can also add references manually.\n\nIn RStudio, go to File &gt; New File... &gt; Text File and save the file as “refs.bib”.\nAdd the line bibliography: refs.bib to your YAML header.\n\n\n2.7.3 Adding references\nYou can add references to a journal article in the following format:\n@article{shortname,\n  author = {Author One and Author Two and Author Three},\n  title = {Paper Title},\n  journal = {Journal Title},\n  volume = {vol},\n  number = {issue},\n  pages = {startpage--endpage},\n  year = {year},\n  doi = {doi}\n}\nSee A complete guide to the BibTeX format for instructions on citing books, technical reports, and more.\nYou can get the reference for an R package using the functions citation() and toBibtex(). You can paste the bibtex entry into your bibliography.bib file. Make sure to add a short name (e.g., “ggplot2”) before the first comma to refer to the reference.\n\ncitation(package=\"ggplot2\") %&gt;% toBibtex()\n\n@Book{,\n  author = {Hadley Wickham},\n  title = {ggplot2: Elegant Graphics for Data Analysis},\n  publisher = {Springer-Verlag New York},\n  year = {2016},\n  isbn = {978-3-319-24277-4},\n  url = {https://ggplot2.tidyverse.org},\n}\n\n\nGoogle Scholar entries have a BibTeX citation option. This is usually the easiest way to get the relevant values if you can’t add a citation through the Zotero browser connector, although you have to add the DOI yourself. You can keep the suggested shortname or change it to something that makes more sense to you.\n\n\n\n\nGet BibTex citations from Google Scholar.\n\n\n\n\n2.7.4 Citing references\nYou can cite references in text like this:\nThis tutorial uses several R packages [@tidyverse;@rmarkdown].\nThis tutorial uses several R packages (Allaire et al., 2018; Wickham, 2017).\nPut a minus in front of the @ if you just want the year:\nKathawalla and colleagues [-@kathawalla_easing_2021] explain how to introduce open research practices into your postgraduate studies.\nKathawalla and colleagues (2021) explain how to introduce open research practices into your postgraduate studies.\n\n2.7.5 Uncited references\nIf you want to add an item to the reference section without citing, it, add it to the YAML header like this:\nnocite: |\n  @kathawalla_easing_2021, @broman2018data, @nordmann2022data\nOr add all of the items in the .bib file like this:\nnocite: '@*'\n\n2.7.6 Citation Styles\nYou can search a list of style files for various journals and download a file that will format your bibliography for a specific journal’s style. You’ll need to add the line csl: filename.csl to your YAML header.\n\nAdd some citations to your refs.bib file, reference them in your text, and render your manuscript to see the automatically generated reference section. Try a few different citation style files.\n\n\n2.7.7 Reference Section\nBy default, the reference section is added to the end of the document. If you want to change the position (e.g., to add figures and tables after the references), include the following where you want the references:\n::: {#refs}\n:::\n\nAdd in-text citations and a reference list to your report.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-reports-summary",
    "href": "02-reports.html#sec-reports-summary",
    "title": "2  Reports",
    "section": "\n2.8 Summary",
    "text": "2.8 Summary\nThis chapter has covered a lot but hopefully now you have a much better idea of what quarto is able to do. Whilst working in quarto and markdown takes longer in the initial set-up stage, once you have a fully reproducible report you can plug in new data each week or month and simply render, reducing duplication of effort, and the human error that comes with it.\nYou can access a working quarto file with the code from the example above to compare to your own code.\nAs you continue to work through the book you will learn how to wrangle and analyse your data and how to use quarto to present it. We’ll slowly build on the available customisation options so over the course of next few weeks, you’ll find your quarto reports start to look more polished and professional.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-exercises-reports",
    "href": "02-reports.html#sec-exercises-reports",
    "title": "2  Reports",
    "section": "\n2.9 Exercises",
    "text": "2.9 Exercises\n\n2.9.1 Create a Project\nCreate a new project called “cv” (Section 2.2).\n\n2.9.2 Create a New Script\nIn the “cv” project, create a new quarto document called “cv.qmd” (Section 2.4.1). Edit the YAML header to print data frames using kable and set a custom theme (Section 2.6.3).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n---\ntitle: \"CV\"\nauthor: \"Me\"\nformat:\n  html:\n    df-print: kable\n    theme: cosmo\n---\n\n\n\n\n2.9.3 Markdown Practice\nWrite a short paragraph describing you and your work or academic aspirations. Include a bullet-point list of links to related websites (Section 2.4.3).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nI am a research psychologist who is interested in open science \nand teaching computational skills.\n\n* [psyTeachR books](https://psyteachr.github.io/)\n* [Google Scholar](https://scholar.google.com/)\n\n\n\n\n2.9.4 Add a Table\nMake a subheading titled “Education” and use the following code to load a small table of your education (Section 2.4.5). Edit it to be relevant to you (you can change the categories entirely if you want).\n\n```{r}\ntibble::tribble(\n  ~degree, ~topic, ~school, ~year,\n  \"BSc\", \"BioPsych/AnthroZoo\", \"University of Michigan\", \"1998\",\n  \"MSc\", \"Biology\", \"University of Michigan\", \"2000\",\n  \"GradCert\", \"Women's Studies\", \"University of Michigan\", \"2000\",\n  \"PhD\", \"Psychology\", \"McMaster University\", \"2004\"\n)\n```\n\n\n\n\ndegree\ntopic\nschool\nyear\n\n\n\nBSc\nBioPsych/AnthroZoo\nUniversity of Michigan\n1998\n\n\nMSc\nBiology\nUniversity of Michigan\n2000\n\n\nGradCert\nWomen’s Studies\nUniversity of Michigan\n2000\n\n\nPhD\nPsychology\nMcMaster University\n2004\n\n\n\n\n\n\n\n2.9.5 Code Execution\nFigure out how to make it so that code chunks don’t show in your rendered document (Section 2.6.1).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can set the execution default to echo: false in the YAML header at the top of the script.\n---\nexecute:\n  echo: false\n---\nTo set visibility for a specific code chunk, put #| echo: false at the top of the code chunk.\n\n\n\n\n2.9.6 Add an Image\nAdd an image of anything relevant (Section 2.5.7).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can add an image from the web using its URL:\n![ReproRes](https://psyteachr.github.io/images/reprores.png){width='200px'}\nOr save an image into your project directory (e.g., in the images folder) and add it using the relative path:\n![ReproRes](images/logos/logo.png){width='200px'}\n\n\n\n\n2.9.7 Use Inline R\nInclude the current date (Section 2.4.7) in a sentence like:\nThis CV was created on 2024-10-03.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis CV was created on `r Sys.Date()`.\n\n\n\n\n2.9.8 Render\nRender this document to html (Section 2.4.8).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nClick on the render button or run the following code in the console. (Do not put it the script!)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquarto::quarto_render(\"cv.qmd\")\n```\n:::\n\n\n\n\n2.9.9 Share\nOnce you’re done, zip up your entire project folder and share it on Teams under the week 2 exercise post. Try downloading someone else’s project and rendering their qmd file.\n\n\n\n\n\n\nImportant\n\n\n\nMake sure any files you reference are inside your project folder, and always use relative paths and not absolute paths when you refer to images or other files in your code. The use of absolute paths is a major source of irreproducibility in published code. The best way to double-check that your code for reproducibility problems is to share your entire project directory with a friend and see if they can render your file.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-glossary-reports",
    "href": "02-reports.html#sec-glossary-reports",
    "title": "2  Reports",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\nterm\ndefinition\n\n\n\nabsolute-path\nA file path that starts with / and is not appended to the working directory\n\n\nchunk\nA section of code in an R Markdown file\n\n\ncomment\nComments are text that R will not run as code. You can annotate .R files or chunks in R Markdown files with comments by prefacing each line of the comment with one or more hash symbols (#).\n\n\ndirectory\nA collection or “folder” of files on a computer.\n\n\nextension\nThe end part of a file name that tells you what type of file it is (e.g., .R or .Rmd).\n\n\nmarkdown\nA way to specify formatting, such as headers, paragraphs, lists, bolding, and links.\n\n\npath\nA string representing the location of a file or directory.\n\n\nproject\nA way to organise related files in RStudio\n\n\nquarto\nAn open-source scientific and technical publishing system.\n\n\nr-markdown\nThe R-specific version of markdown: a way to specify formatting, such as headers, paragraphs, lists, bolding, and links, as well as code blocks and inline code.\n\n\nrelative-path\nThe location of a file in relation to the working directory.\n\n\nrender\nTo create a file (usually an image or PDF) or widget from source code\n\n\nreproducibility\nThe extent to which the findings of a study can be repeated in some other context\n\n\nscript\nA plain-text file that contains commands in a coding language, such as R.\n\n\nworking-directory\nThe filepath where R is currently reading and writing files.\n\n\nyaml\nA structured format for information",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-resources-reports",
    "href": "02-reports.html#sec-resources-reports",
    "title": "2  Reports",
    "section": "Further Resources",
    "text": "Further Resources\n\nQuarto Guide\nMarkdown Basics\n\nProject Structure by Danielle Navarro\n\nHow to name files by Jenny Bryan\n\ngt for customised tables",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "02-reports.html#sec-references-reports",
    "href": "02-reports.html#sec-references-reports",
    "title": "2  Reports",
    "section": "References",
    "text": "References\n\n\n\n\nAllaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins,\nA., Wickham, H., Cheng, J., & Chang, W. (2018). Rmarkdown:\nDynamic documents for r. https://CRAN.R-project.org/package=rmarkdown\n\n\nCorker, K. S. (2021). An open science workflow for more credible,\nrigorous research.\n\n\nCrüwell, S., Doorn, J. van, Etz, A., Makel, M. C., Moshontz, H.,\nNiebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M.\n(2019). Seven easy steps to open science: An annotated reading list.\nZeitschrift für Psychologie, 227(4), 237.\n\n\nDeBruine, L. M., & Barr, D. J. (2019). Understanding mixed\neffects models through data simulation. https://doi.org/10.31234/osf.io/xp5cy\n\n\nKathawalla, U.-K., Silverstein, P., & Syed, M. (2021). Easing into\nopen science: A guide for graduate students and their advisors.\nCollabra: Psychology, 7(1). https://doi.org/10.1525/collabra.18684\n\n\nLakens, Daniel, & Caldwell, A. R. (2019). Simulation-based\npower-analysis for factorial ANOVA designs. https://doi.org/10.31234/osf.io/baxsf\n\n\nLakens, Daniël, Scheel, A. M., & Isager, P. M. (2018). Equivalence\ntesting for psychological research: A tutorial. Advances in Methods\nand Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963\n\n\nMorey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., &\nWagenmakers, E.-J. (2016). The fallacy of placing confidence in\nconfidence intervals. Psychonomic Bulletin &\nReview, 23(1), 103–123. https://doi.org/10.3758/s13423-015-0947-8\n\n\nNordmann, E., & DeBruine, L. (2023). Applied data skills\n(Version 3.0). https://doi.org/10.5281/zenodo.6365077\n\n\nNordmann, E., McAleer, P., Toivo, W., Paterson, H., & DeBruine, L.\nM. (2021). Data visualisation using R, for researchers\nwho don’t use R. PsyArXiv. https://doi.org/10.31234/osf.io/4huvw\n\n\nNuijten, M. B., Hartgerink, C. H., Van Assen, M. A., Epskamp, S., &\nWicherts, J. M. (2016). The prevalence of statistical reporting errors\nin psychology (1985–2013). Behavior Research Methods,\n48(4), 1205–1226.\n\n\nParsons, S., Azevedo, F., others, & Aczel, B. (2022). A\ncommunity-sourced glossary of open scholarship terms. Nature Human\nBehaviour, 6(3), 312–318. https://doi.org/10.1038/s41562-021-01269-4\n\n\nWickham, H. (2017). Tidyverse: Easily install and load the\n’tidyverse’. https://CRAN.R-project.org/package=tidyverse\n\n\nWickham, H. (2022). Tidyverse: Easily install and load the\ntidyverse. https://CRAN.R-project.org/package=tidyverse\n\n\nXie, Y. (2022). Tinytex: Helper functions to install and maintain\nTeX live, and compile LaTeX documents. https://github.com/rstudio/tinytex",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reports</span>"
    ]
  },
  {
    "objectID": "03-viz.html",
    "href": "03-viz.html",
    "title": "3  Data Visualisation",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#sec-ilo-viz",
    "href": "03-viz.html#sec-ilo-viz",
    "title": "3  Data Visualisation",
    "section": "",
    "text": "Identify categorical versus continuous data\nCreate plots in layers using ggplot\nChoose appropriate plots for data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#sec-functions-viz",
    "href": "03-viz.html#sec-functions-viz",
    "title": "3  Data Visualisation",
    "section": "Functions used",
    "text": "Functions used\n\nbuilt-in (you can always use these without loading any packages)\n\nbase:: , as.numeric(), c(), factor(), mean(), seq(),\ngrDevices:: rgb()\n\n\n\ntidyverse (you can use all these with library(tidyverse))\n\nreadr:: readr::col_character(), readr::col_datetime(), readr::col_double(), readr::col_factor(), readr::col_integer(), readr::cols(), readr::read_csv()\n\nlubridate:: lubridate::now(), lubridate::today()\n\ndplyr:: dplyr::count(), dplyr::glimpse(),\nggplot2:: aes(), coord_cartesian(), element_blank(), facet_wrap(), geom_bar(), geom_boxplot(), geom_col(), geom_histogram(), geom_jitter(), geom_point(), geom_smooth(), ggplot(), ggsave(), ggtitle(), guides(), scale_fill_manual(), scale_x_continuous(), scale_x_date(), scale_x_discrete(), scale_y_continuous(), spec(), stat_summary(), theme(), theme_bw(), theme_minimal(), theme_set()\n\n\n\nother (you need to load each package to use these)\n\nggthemes:: ggthemes::theme_gdocs()\n\npatchwork:: patchwork::plot_layout()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#sec-setup-viz",
    "href": "03-viz.html#sec-setup-viz",
    "title": "3  Data Visualisation",
    "section": "Set-up",
    "text": "Set-up\n\nOpen your reprores project\nCreate a new quarto file called 03-dataviz.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\nlibrary(tidyverse) # includes ggplot2\nlibrary(patchwork) # for multi-part plots\nlibrary(ggthemes)  # for plot themes\nlibrary(lubridate) # for manipulating dates\n```\n\nIf you get the message Error in library(x) : there is no package called ‘x’, please refer to Section 1.2.1.\nWe’d recommend making a new code chunk for each different activity, and using the white space to make notes on any errors you make, things you find interesting, or questions you’d like to ask the course team.\nDownload the ggplot2 cheat sheet.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#variable-types",
    "href": "03-viz.html#variable-types",
    "title": "3  Data Visualisation",
    "section": "\n3.1 Variable types",
    "text": "3.1 Variable types\nIf a spreadsheet is in a tidy data format, each row is an observation, each column is a variable, and the information in each cell is a single value. We’ll learn more about how to get our data into this format in Chapter 6, but to get started we’ll use datasets with the right format.\nFor example, the table below lists pets owned by members of the psyTeachR team. Each row is an observation of one pet. There are 6 variables for each pet, their name, owner, species, birthdate, weight (in kg), and rating (on a 5-point scale from “very evil” to “very good”).\n\n\n\n\n\nname\nowner\nspecies\nbirthdate\nweight\nrating\n\n\n\nDarwin\nLisa\nferret\n1998-04-02\n1.2\na little evil\n\n\nOy\nLisa\nferret\nNA\n2.9\nvery good\n\n\nKhaleesi\nEmily\ncat\n2014-10-01\n4.5\nvery good\n\n\nBernie\nPhil\ndog\n2017-06-01\n32.0\nvery good\n\n\n\n\n\n\nVariables can be classified as continuous (numbers) or categorical (labels). When you’re plotting data, it’s important to know what kind of variables you have, which can help you decide what types of plots are most appropriate. Each variable also has a data type, such as numeric (numbers), character (text), or logical (TRUE/FALSE values). Some plots can only work on some data types. Additionally, Appendix H has more details, as this concept will be relevant repeatedly.\n\n\n\n\n\nFigure 3.1: Data types are like the categories when you format cells in Excel.\n\n\n\n3.1.1 Continuous\nContinuous variables are properties you can measure, like weight. You can use continuous variables in mathematical operations, like calculating the sum total of a column of prices or the average number of social media likes per day. They may be rounded to the nearest whole number, but it should make sense to have a measurement halfway between.\nContinuous variables always have a numeric data type. They are either integers like 42 or doubles like 3.14159.\n\n3.1.2 Categorical\nCategorical variables are properties you can count, like the species of pet. Categorical variables can be &lt;a href=‘https://psyteachr.github.io/glossary/n#nominal’ target=’_blank’ class=‘glossary’ title=‘Categorical variables that don’t have an inherent order, such as types of animal.’&gt;nominal, where the categories don’t really have an order, like cats, dogs and ferrets (even though ferrets are obviously best), or ordinal, where they have a clear order but the distance between the categories isn’t something you could exactly equate, like points on a Likert rating scale. Even if a data table uses numbers like 1-7 to represent ordinal variables, you shouldn’t treat them like continuous variables.\nCategorical data can have a character data type, also called strings. These are made by putting text inside of quotes. That text can be letters, punctuation, or even numbers. For example, \"January\" is a character string, but so is \"1\" if you put it in quotes. The character data type is best for variables that can have a lot of different values that you can’t predict ahead of time.\nCategorical data can also be factors, a specific type of integer that lets you specify the category names and their order. This is useful for making plots display with categories in the order you want (otherwise they default to alphabetical order). The factor data type is best for categories that have a specific number of levels.\n\n\n\n\n\n\nDo not factor numbers\n\n\n\nIf you factor numeric data, it gets converted to the integers 1 to the number of unique values, no matter what the values are. Additionally, you can no longer use the values as numbers, such as calculating the mean.\n\n\n\nExample\n\nx &lt;- c(-3, 0, .5)  # numeric vector\nf &lt;- factor(x)     # convert to factor\nx == as.numeric(f) # does not convert back to numeric \n\n\n[1] FALSE FALSE FALSE\n\n\n\n\n\nYou cannot average a factor\n\nm &lt;- mean(f)\n\n\nWarning in mean.default(f): argument is not numeric or logical: returning NA\n\n\n\n\nSometimes people represent categorical variables with numbers that correspond to names, like 0 = “no” and 1 = “yes”, but values in between don’t have a clear interpretation. If you have control over how the data are recorded, it’s better to use the character names for clarity. You’ll learn how to recode columns in Chapter 7.\n\n3.1.3 Dates and times\nDates and times are a special case of variable. They can act like categorical or continuous variables, and there are special ways to plot them. Dates and times can be hard to work with, but the lubridate package provides functions to help you with this.\n\n# the current date\nlubridate::today()\n\n[1] \"2024-10-11\"\n\n\n\n# the current date and time in the GMT timezone\nlubridate::now(tzone = \"GMT\")\n\n[1] \"2024-10-11 09:35:27 GMT\"\n\n\n\nComing back to the pets dataset, what type of variable is in each column? You can use the function glimpse() to show a list of the column names, their data types, and the first few values in each column - here is the output of running glimpse() on the pets dataset.\n\nglimpse(pets)\n\nRows: 4\nColumns: 6\n$ name      &lt;chr&gt; \"Darwin\", \"Oy\", \"Khaleesi\", \"Bernie\"\n$ owner     &lt;chr&gt; \"Lisa\", \"Lisa\", \"Emily\", \"Phil\"\n$ species   &lt;fct&gt; ferret, ferret, cat, dog\n$ birthdate &lt;date&gt; 1998-04-02, NA, 2014-10-01, 2017-06-01\n$ weight    &lt;dbl&gt; 1.2, 2.9, 4.5, 32.0\n$ rating    &lt;fct&gt; a little evil, very good, very good, very good\n\n\n\n\n\n\n\n\n\nColumn\nVariable type\nData type\n\n\n\nname\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate\n\n\nowner\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate\n\n\nspecies\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate\n\n\nbirthdate\n\ncontinuous\nnominal\nordinal\ndate\n\ncontinuous\nnominal\nordinal\ndate\n\n\nweight\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate\n\n\nrating\n\ncontinuous\nnominal\nordinal\ndate\n\nnumeric\ncharacter\nfactor\ndate",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#sec-plots",
    "href": "03-viz.html#sec-plots",
    "title": "3  Data Visualisation",
    "section": "\n3.2 Building plots",
    "text": "3.2 Building plots\nThere are multiple approaches to data visualisation in R; in this course we will use the popular package ggplot2, which is part of the larger tidyverse collection of packages. A grammar of graphics (the “gg” in “ggplot”) is a standardised way to describe the components of a graphic. ggplot2 uses a layered grammar of graphics, in which plots are built up in a series of layers. It may be helpful to think about any plot as having multiple elements that sit semi-transparently over each other. A good analogy is old Disney movies where artists would create a background and then add moveable elements on top of the background via transparencies.\nFigure 3.2 displays the evolution of a simple scatterplot using this layered approach. First, the plot space is built (layer 1); the variables are specified (layer 2); the type of visualisation (known as a geom) that is desired for these variables is specified (layer 3) - in this case geom_point() is called to visualise individual data points; a second geom is added to include a line of best fit (layer 4), the axis labels are edited for readability (layer 5), and finally, a theme is applied to change the overall appearance of the plot (layer 6).\n\n\n\n\n\n\n\nFigure 3.2: Evolution of a layered plot\n\n\n\n\nImportantly, each layer is independent and independently customisable. For example, the size, colour and position of each component can be adjusted, or one could, for example, remove the first geom (the data points) to only visualise the line of best fit, simply by removing the layer that draws the data points (Figure 3.3). The use of layers makes it easy to build up complex plots step-by-step, and to adapt or extend plots from existing code.\n\n\n\n\n\n\n\nFigure 3.3: Final plot with scatterplot layer removed.\n\n\n\n\n\n3.2.1 Loading data\nLet’s build up the plot above, layer by layer. First we need to get the data. We’ll learn how to load data from different sources in Appendix G, but this time we’ll use the same method as we did in Section 2.5.3 and load it from an online source.\n\nLoad the survey data by copying the following into a code block in your script, and running the code.\n\nsurvey_data &lt;- read_csv(\"https://psyteachr.github.io/reprores-v4/data/survey_data.csv\")\n\n\nWhen you load the data, read_csv() will produce a message that gives you information about the data it has imported and what assumptions it has made. The “column specification” tells you what each column is named and what type of data R has categorised each variable as. The abbreviation “chr” is for character columns, “dbl” is for double columns, and “dttm” is a date/time column.\nThis data is simulated data for a call centre customer satisfaction survey. The first thing you should do when you need to plot data is to get familiar with what all of the rows (observations) and columns (variables) mean. Sometimes this is obvious, and sometimes it requires help from the data provider. Here, each row represents one call to the centre.\n\n\ncaller_id is a unique ID for each caller\n\nemployee_id is a unique ID for each employee taking calls\n\ncall_start is the date and time that the call arrived\n\nwait_time is the number of seconds the caller had to wait\n\ncall_time is the number of seconds the call lasted after the employee picked up\n\nissue_category is whether the issue was tech, sales, returns, or other\n\nsatisfaction is the customer satisfaction rating on a scale from 1 (very unsatisfied) to 5 (very satisfied)\n\nUnless you specify the column types, data importing functions will just guess the types and usually default to double for columns with numbers and character for columns with letters.\n\nUse the function spec() to find out all of the column types and edit them if needed.\n\nspec(survey_data)\n\ncols(\n  caller_id = col_character(),\n  employee_id = col_character(),\n  call_start = col_datetime(format = \"\"),\n  wait_time = col_double(),\n  call_time = col_double(),\n  issue_category = col_character(),\n  satisfaction = col_double()\n)\n\n\n\nLet’s set issue_category as a factor and set the order of the levels. By default, R will order the levels of a factor alphanumerically, however in many cases you will want or need to set your own order. For example, in this data, it makes most sense for the category “other” to come at the end of the list. After you update the column types, you have to re-import the data by adjusting the read_csv() code to set the col_types argument to the new column types.\n\nUpdate your data import code block to look like the one below.\n\n# updated column types\nsurvey_col_types &lt;- cols(\n  caller_id = col_character(),\n  employee_id = col_character(),\n  call_start = col_datetime(format = \"\"),\n  wait_time = col_double(),\n  call_time = col_double(),\n  issue_category = col_factor(levels = c(\"tech\", \"sales\", \"returns\", \"other\")),\n  satisfaction = col_integer()\n)\n\n# re-import data with correct column  types\nsurvey_data &lt;- read_csv(\"https://psyteachr.github.io/ads-v2/data/survey_data.csv\",\n                        col_types = survey_col_types)\n\n\n\n\n\n\n\n\nDefine objects before you use them\n\n\n\nBecause read_csv() is going to use the object survey_col_types, you must create survey_col_types before you run the adjusted read_csv() code. If you ever need to adjust your code, try to think about the order that the code will run in if you start from scratch and make sure it’s organised appropriately.\n\n\n\n3.2.2 Plot setup\n\n3.2.2.1 Default theme\nPlots in this book use the black-and-white theme, not the default grey theme, so set your default theme to the same so your plots will look like the examples below. At the top of your script, in the setup chunk after you’ve loaded the tidyverse package, add the following code and run it. You’ll learn more ways to customise your theme in Section 3.2.3.4 and Section J.3.\n\nAdd this code to your setup chunk. It needs to run before you make any plot.\n\ntheme_set(theme_bw()) # set the default theme\n\n\n\n3.2.2.2 Data\nEvery plot starts with the ggplot() function and a data table. If your data are not loaded or you have a typo in your code, this will give you an error message. It’s best to check your plot after each step, so that you can figure out where errors are more easily.\n\nCreate a blank plot.\n\nggplot(data = survey_data)\n\n\n\n\n\n\n\n\n\n3.2.2.3 Mapping\nThe next argument to ggplot() is the mapping. This tells the plot which columns in the data should be represented by, or “mapped” to, different aspects of the plot, such as the x-axis, y-axis, line colour, object fill, or line style. These aspects, or “aesthetics”, are listed inside the aes() function.\n\nSet the arguments x and y to the names of the columns you want to be plotted on those axes. Here, we want to plot the wait time on the x-axis and the call time on the y-axis.\n\n# set up the plot with mapping\nggplot(\n  data = survey_data, \n  mapping = aes(x = wait_time, y = call_time)\n)\n\n\n\n\n\n\n\n\nIn the example above, we wrote out the names of the arguments data and mapping, but in practice, almost everyone omits them. Just make sure you put the data and mapping in the right order.\n\nggplot(survey_data,  aes(x = wait_time, y = call_time))\n\n\n3.2.2.4 Geoms\nNow we can add our plot elements in layers. These are referred to as geoms and their functions start with geom_. You add layers onto the base plot created by ggplot() with a plus (+).\n\nAdd a scatterplot to your plot using geom_point().\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point() # scatterplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation of the +\n\n\n\nSomewhat annoyingly, the plus has to be on the end of the previous line, not at the start of the next line. If you do make this mistake, it will run the first line of code to produce the base layer but then you will get the following error message rather than adding on geom_point().\n\nggplot(survey_data, aes(x = wait_time, y = call_time))\n\n\n\n\n\n\n+ geom_point() # scatterplot\n\nError:\n! Cannot use `+` with a single argument.\nℹ Did you accidentally put `+` on a new line?\n\n\n\n\n\n3.2.2.5 Multiple geoms\nPart of the power of ggplot2 is that you can add more than one geom to a plot by adding on extra layers and so it quickly becomes possible to make complex and informative visualisations. Importantly, the layers display in the order you set them up. The code below uses the same geoms to produce a scatterplot with a line of best fit, but orders them differently.\n\n# Points first\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point() + # scatterplot\n  geom_smooth(method = lm) # line of best fit\n\n# Line first\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_smooth(method = lm) + # line of best fit\n  geom_point() # scatterplot\n\n\n\n\n\n\n\n\nFigure 3.4: Points first versus line first.\n\n\n\n\n\nAdd another geom to your plot. Add another code block and copy the plot code, changing the order of geoms. Do they look different?\n\n\n3.2.2.6 Plot objects\nJust like you can save numbers and data tables to objects, you can also save the output of ggplot(). The code below produces the same plots we created above but saves them to objects named point_first and line_first. If you run just this code, the plots won’t display like they have done before. Instead, you’ll see the object names appear in the environment pane.\n\npoint_first &lt;- \n  ggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point() + # scatterplot\n  geom_smooth(method = lm) # line of best fit\n  \nline_first &lt;-\n  ggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_smooth(method = lm) + # line of best fit\n  geom_point() # scatterplot\n\nTo view the plots, call the objects by name. This will output each plot separately.\n\npoint_first # view first plot\nline_first # view second plot\n\n\nSave your two plots to objects with appropriate names.\n\n\n3.2.2.7 Combining plots\nOne of the reasons to save your plots to objects is so that you can combine multiple plots using functions from the patchwork package. The code below produces the plot above by combining the two plots with + and then specifying that we want the plots produced on a single row with the nrow argument in plot_layout().\n\n# add plots together in 1 row\npoint_first + line_first + plot_layout(nrow = 1)\n\n\n\n\n\n\nFigure 3.5: Combining plots with patchwork.\n\n\n\n\n\nCombine your plots using plot_layout(). Try changing the value of nrow to 2.\n\n\n3.2.3 Customising plots\nThere are nearly endless ways to customise ggplots. We’ll cover a few of the basic customisations here.\n\n3.2.3.1 Styling geoms\nWe should definitely put the line in front of the points, but the points are still a bit dark. If you want to change the overall style of a geom, you can set the arguments colour, alpha, shape, size and linetype inside the geom function. There are many different values that you can set these to; Appendix J gives details of these.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2, # 20% transparency\n             shape = 18,  # solid diamond\n             size = 2) + \n  # setting method & formula avoids an annoying message\n  geom_smooth(method = lm,   # method of linear model (lm)\n              formula = y~x, # formula used to draw line\n              colour = rgb(0, .5, .8),\n              linetype = 3) \n\n\n\n\n\n\nFigure 3.6: Changing geom styles.\n\n\n\n\n\nPlay around with different values above and figure out what the default values are for shape and size.\n\n\n\n\n\n\n\nSetting aesthetics overall versus by category\n\n\n\nThis method is only for changing the style of all the shapes made with that geom. If you want, for example, points to have different colours depending on which issue category they are from, you set the argument colour = issue_category inside the aes() function for the mapping. You can customise the colours used with scale_ functions, which you will learn about below and in Appendix J.\n\n\n\n3.2.3.2 Format axes\nNow we need to make the axes look neater. There are several functions you can use to change the axis labels, but the most powerful ones are the scale_ functions. You need to use a scale function that matches the data you’re plotting on that axis and this is where it becomes particularly important to know what type of data you’re working with. Both of the axes here are continuous, so we’ll use scale_x_continuous() and scale_y_continuous().\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  # customise axis labels and breaks\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = seq(from = 0, to = 600, by = 60))\n\n\n\n\n\n\nFigure 3.7: Formatting plot axes with scale_ functions.\n\n\n\n\nThe name argument changes the axis label. The breaks argument sets the major units and needs a vector of possible values, which can extend beyond the range of the data (e.g., wait time only goes up to 350, but we can specify breaks up to 600 to make the maths easier or anticipate updates to the data). The seq() function creates a sequence of numbers from one to another by specified steps.\n\n\n\nExample of seq()\n\nseq(from = 0, to = 600, by = 60)\n\n\n [1]   0  60 120 180 240 300 360 420 480 540 600\n\n\n\nSet the y-axis name to “Call Time (seconds) and the breaks to every 60 seconds with minor breaks every 15 seconds. Check the help for ?scale_y_continuous to see how you would set the minor units.\n\n\n\nSolution\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  # customise axis labels and breaks\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = seq(from = 0, to = 600, by = 60)) +\n  scale_y_continuous(name = \"Call Time (seconds)\",\n                     breaks = seq(from = 0, to = 600, by = 60),    \n                     minor_breaks = seq(from = 0, to = 600, by = 15))\n\n\n\n\n\n\n\n\n\n\n3.2.3.3 Axis limits\nIf you want to change the minimum and maximum values on an axis, use the coord_cartesian() function. Many plots make more sense if the minimum and maximum values represent the range of possible values, even if those values aren’t present in the data. Here, wait and call times can’t be less than 0 seconds, so we’ll set the minimum values to 0 and the maximum values to the first break above the highest value.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = seq(from = 0, to = 600, by = 60)) +\n  scale_y_continuous(name = \"Call Time (seconds)\",\n                     breaks = seq(from = 0, to = 600, by = 60),    \n                     minor_breaks = seq(from = 0, to = 600, by = 15)) +\n  # set axis limits\n  coord_cartesian(xlim = c(0, 360), \n                  ylim = c(0, 180))\n\n\n\n\n\n\nFigure 3.8: Changing the axis limits.\n\n\n\n\n\n\n\n\n\n\nSetting limits with the scale_ function\n\n\n\nYou can also set the limits argument inside the scale_ functions, but this actually removes any data that falls outside these limits, rather than cropping your plot, and this can change the appearance of certain types of plots like violin plots and density plots.\n\n\n\n3.2.3.4 Themes\nggplot2 comes with several built-in themes, such as theme_minimal() and theme_bw(), but the ggthemes package provides even more themes to match different software, such as GoogleDocs or Stata, or publications, such as the Economist or the Wall Street Journal. Let’s add the GoogleDocs theme, but change the font size to 20 with the base_size argument.\nIt’s also worth highlighting that this code is starting to look quite complicated because of the number of layers, but because we’ve built it up slowly it should (hopefully!) make sense. If you see examples of ggplot2 code online that you’d like to adapt, build the plot up layer by layer and it will make it easier to understand what each layer adds.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = seq(from = 0, to = 600, by = 60)) +\n  scale_y_continuous(name = \"Call time (seconds)\",\n                     breaks = seq(from = 0, to = 600, by = 30)) +\n  coord_cartesian(xlim = c(0, 360), \n                  ylim = c(0, 180)) +\n  # change the theme\n  ggthemes::theme_gdocs(base_size = 20)\n\n\n\n\n\n\nFigure 3.9: Changing the theme to the Google Docs style.\n\n\n\n\n\n3.2.3.5 Theme tweaks\nIf you’re still not quite happy with a theme, you can customise it even further with the themes() function. Check the help for this function to see all of the possible options. The most common thing you’ll want to do is to remove an element entirely. You do this by setting the relevant argument to element_blank(). Below, we’re getting rid of the x-axis line and the plot background, which removes the line around the plot.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = seq(from = 0, to = 600, by = 60)) +\n  scale_y_continuous(name = \"Call time (seconds)\",\n                     breaks = seq(from = 0, to = 600, by = 30)) +\n  coord_cartesian(xlim = c(0, 360), \n                  ylim = c(0, 180)) +\n  theme_gdocs(base_size = 11) +\n  # customise theme elements\n  theme(axis.line.x = element_blank(),\n        plot.background = element_blank())\n\n\n\n\n\n\nFigure 3.10: Customising the theme to remove the x-axis line and background outline.\n\n\n\n\n\n3.2.4 Figure captions\nYou can add a caption directly to the image using the labs() function, which also allows you to add or edit the title, subtitle, and axis labels.\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(colour = \"dodgerblue\", \n             alpha = 0.2) + \n  geom_smooth(method = lm, \n              formula = y~x, \n              colour = rgb(0, .5, .8)) +\n  scale_x_continuous(name = \"Wait Time (seconds)\", \n                     breaks = seq(from = 0, to = 600, by = 60)) +\n  scale_y_continuous(name = \"Call time (seconds)\",\n                     breaks = seq(from = 0, to = 600, by = 30)) +\n  coord_cartesian(xlim = c(0, 360), \n                  ylim = c(0, 180)) +\n  theme_gdocs(base_size = 11) +\n  theme(axis.line.x = element_blank(),\n        plot.background = element_blank()) +\n  labs(title = \"The relationship between wait time and call time\",\n       subtitle = \"2020 Call Data\",\n       caption = \"Figure 1. As wait time increases, call time increases.\")\n\n\n\n\n\n\nFigure 3.11: Adding a title, subtitle, and caption.\n\n\n\n\nHowever, it is more accessible to include this sort of information in plain text for screen readers. You can add a text caption in the chunk header. You can also add alt-text descriptions for screen readers that describe the image.\n\n```{r}\n#| label: fig-wait-vs-call\n#| fig-cap: \"As wait time increases, call time increases.\"\n#| fig-alt: \"A scatterplot showing wait time on the x-axis (range 0-360 seconds) and call time on the y-axis (range 0-180 seconds) with a trend line showing that as wait time increases, call time increases from about 60 wait/30 call to about 300 wait/65 call.\"\n\n# figure code here\n```\n\n\n3.2.5 Saving Plots\nYou can save a ggplot using ggsave(). It saves the last ggplot you made, by default, but you can specify which plot you want to save if you assigned that plot to a variable.\nYou can set the width and height of your plot. The default units are inches, but you can change the units argument to “in”, “cm”, or “mm”.\n\nbox &lt;- ggplot(pets, aes(pet, score, fill=pet)) +\n  geom_boxplot(alpha = 0.5)\n\nviolin &lt;- ggplot(pets, aes(pet, score, fill=pet)) +\n  geom_violin(alpha = 0.5)\n\nggsave(\"demog_violin_plot.png\", width = 5, height = 7)\n\nggsave(\"demog_box_plot.jpg\", plot = box, width = 5, height = 7)\n\n\n\n\n\n\n\nNote\n\n\n\nThe file type is set from the filename suffix, or by specifying the argument device, which can take the following values: “eps”, “ps”, “tex”, “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg” or “wmf”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#sec-appropriate-plots",
    "href": "03-viz.html#sec-appropriate-plots",
    "title": "3  Data Visualisation",
    "section": "\n3.3 Appropriate plots",
    "text": "3.3 Appropriate plots\nNow that you know how to build up a plot by layers and customise its appearance, you’re ready to learn about some more plot types. Different types of data require different types of plots, so this section is organised by data type.\nThe ggplot2 cheat sheet is a great resource to help you find plots appropriate to your data, based on how many variables you’re plotting and what type they are. The examples below all use the same customer satisfaction data, but each plot communicates something different.\nWe don’t expect you to memorise all of the plot types or the methods for customising them, but it will be helpful to try out the code in the examples below for yourself, changing values to test your understanding.\n\n3.3.1 Counting categories\n\n3.3.1.1 Bar plot\nIf you want to count the number of things per category, you can use geom_bar(). You only need to provide a x mapping to geom_bar() because by default geom_bar() uses the number of observations in each group of x as the value for y, so you don’t need to tell it what to put on the y-axis.\n\nggplot(survey_data, aes(x = issue_category)) +\n  geom_bar()\n\n\n\n\n\n\nFigure 3.12: A basic bar plot.\n\n\n\n\n\nYou probably want to customise some things, like the colours, order of the columns, and their labels. Inspect the code below and try running it layer by layer to figure out where these things change. The functions scale_fill_manual() and scale_x_discrete() are new, but work in the same way as the other scale_ functions. You’ll learn more about this in Section 10.1.\n\n\n\nCode\n\nggplot(survey_data, aes(x = issue_category, \n                        fill = issue_category)) +\n  geom_bar() +\n  scale_x_discrete(\n    # change axis title\n    name = \"Issue Category\", \n    # change order\n    limits = c(\"tech\", \"returns\", \"sales\", \"other\"), \n    # change labels\n    labels = c(\"Technical\", \"Returns\", \"Sales\", \"Other\") \n  ) +\n  scale_fill_manual(\n    # change colours\n    values = c(tech = \"goldenrod\", \n                returns = \"darkgreen\", \n                sales = \"dodgerblue3\", \n                other = \"purple3\"),\n    # remove the legend\n    guide = \"none\" \n  ) +\n  scale_y_continuous(\n    name = \"\", # remove axis title\n    # remove the space above and below the y-axis\n    expand = expansion(add = 0)\n  ) +\n  # minimum = 0, maximum = 350\n  coord_cartesian(ylim = c(0, 350)) + \n  ggtitle(\"Number of issues per category\") # add a title\n\n\n\n\n\n\n\n\n\n\n3.3.1.2 Column plot\nIf your data already have a column with the number you want to plot, you can use geom_col() to plot it. We can use the count() function to make a table with a row for each issue_category and a column called n with the number of observations in that category.\n\ncount_data &lt;- count(survey_data, issue_category)\n\n\n\n\nissue_category\nn\n\n\n\ntech\n311\n\n\nsales\n88\n\n\nreturns\n232\n\n\nother\n76\n\n\n\n\nThe mapping for geom_col() requires you to set both the x and y aesthetics. Set y = n because we want to plot the number of issues in each category, and that information is in the column called n.\n\nggplot(count_data, aes(x = issue_category, y = n)) +\n  geom_col()\n\n\n\n\n\n\nFigure 3.13: A basic column plot.\n\n\n\n\n\n3.3.1.3 Pie chart\nPie charts are a misleading form of data visualisation, so we won’t cover them. We’ll cover options for visualising proportions, like waffle, lollipop and treemap plots, in Section 10.1.4.\n\nTest your understanding; here is a small data table.\n\n\ncountry\npopulation\nisland\n\n\n\nNorthern Ireland\n1,895,510\nIreland\n\n\nWales\n3,169,586\nGreat Britain\n\n\nRepublic of Ireland\n4,937,786\nIreland\n\n\nScotland\n5,466,000\nGreat Britain\n\n\nEngland\n56,550,138\nGreat Britain\n\n\n\n\nWhat geom would you use to plot the population for each of the 5 countries? \ngeom_bar\ngeom_col\n\nWhat mapping would you use?\n\naes(x = country, y = population)aes(x = population, y = country)aes(x = country)aes(x = island)aes(y = population)\n\n\nWhat geom would you use to plot the number of countries on each island? \ngeom_bar\ngeom_col\n\nWhat mapping would you use?\n\naes(x = country, y = population)aes(x = population, y = country)aes(x = country)aes(x = island)aes(y = population)\n\n\n\n\n\n3.3.2 One continuous variable\nIf you have a continuous variable, like the number of seconds callers have to wait, you can use geom_histogram() to show the distribution. Just like geom_bar() you are only required to specify the x variable.\nA histogram splits the data into “bins” along the x-axis and shows the count of how many observations are in each bin along the y-axis.\n\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram()\n\n\n\n\n\n\nFigure 3.14: Histogram of wait times.\n\n\n\n\nYou should always set the binwidth or number of bins to something meaningful for your data (otherwise you get the annoying message above). You might need to try a few options before you find something that looks good and conveys the meaning of your plot – try changing the values of binwidth and bins below to see what works best.\n\n# adjust width of each bar\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(binwidth = 30)\n\n# adjust number of bars\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(bins = 5)\n\nBy default, the bars start centered on 0, so if binwidth is set to 30, the first bar would include -15 to 15 seconds, which doesn’t make much sense. We can set boundary = 0 so that each bar represents increments of 30 seconds starting from 0.\n\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(binwidth = 30, boundary = 0)\n\n\n\n\n\n\nFigure 3.15: A histogram with the boundary set to 0.\n\n\n\n\nFinally, the default style of grey bars is ugly, so you can change that by setting the fill and colour, as well as using scale_x_continuous() to update the axis labels.\n\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(binwidth = 15, \n                 boundary = 0, \n                 fill = \"white\", \n                 color = \"black\") +\n  scale_x_continuous(name = \"Wait time (seconds)\",\n                     breaks = seq(0, 600, 60))\n\n\n\n\n\n\nFigure 3.16: Histogram with custom styles.\n\n\n\n\n\nTest your understanding\nImagine you have a table of the population for each country in the world with the columns country and population. We’ll just look at the 76 countries with populations of less than a million.\n\n\n\n\n\n\n\n\n\nHow would you set the mapping for this plot?\n\naes(x = country, y = population)aes(x = population, y = country)aes(x = population)aes(x = population, y = count)\n\n\nWhat is the binwidth of the histogram? \n1\n100\n100K\n1M\n\n\n\n\n\n\n\n\n\nAxis label customisation\n\n\n\n\n\nIf you’re curious how we got the x-axis labels to read “100K” instead of “100000”, you just need to add a vector of labels the same length as breaks.\n\n  scale_x_continuous(breaks = seq(0, 1e6, 1e5),\n                     labels = c(paste0(0:9*100, \"K\"), \"1M\"))\n\n\n\n\n\n3.3.3 Grouped continuous variables\nThere are several ways to compare continuous data across groups. Which you choose depends on what point you are trying to make with the plot.\n\n3.3.3.1 Stacked histogram\nIn previous plots, we have used fill purely for visual reasons, e.g., we changed the colour of the histogram bars to make them look nicer. However, you can also use fill to represent another variable so that the colours become meaningful.\nSetting the fill aesthetic in the mapping will produce different coloured bars for each category of the fill variable, in this case issue_category.\n\nggplot(survey_data, aes(x = wait_time, fill = issue_category)) +\n  geom_histogram(boundary = 0, \n                 binwidth = 15,\n                 color = \"black\")\n\n\n\nHistogram with categories represented by fill.\n\n\n\n\n\n\n\n\n\nArguments inside aes()\n\n\n\nWhen you set an aspect to represent the data, you do this inside the aes() function for the mapping, not as an argument to the geom. If you try to set this in a geom, you’ll get the following error (unless you coincidentally have an object named issue_category that is a colour word).\n\nggplot(survey_data, aes(x = wait_time)) +\n  geom_histogram(boundary = 0, \n                 binwidth = 15, \n                 color = \"black\",\n                 fill = issue_category)\n\nError: object 'issue_category' not found\n\n\n\n\n\n\n\n\n\n\nArea plot alternative\n\n\n\n\n\nThe function geom_area() gives a similar effect when stat = \"bin\".\n\n# area plot\nggplot(survey_data, mapping = aes(x = wait_time, fill = issue_category)) +\n  geom_area(stat = \"bin\", \n            boundary = 0, \n            binwidth = 15, \n            color = \"black\")\n\n\n\nStacked area plot.\n\n\n\n\n\n\n\n3.3.3.2 Dodged histogram\nBy default, the categories are positioned stacked on top of each other. If you want to compare more than one distribution, you can set the position argument of geom_histogram() to “dodge” to put the bars for each group next to each other instead of stacking them. However, this can look confusing with several categories.\n\n# dodged histogram\nggplot(survey_data, aes(x = wait_time, \n                        fill = issue_category,\n                        colour = issue_category))+\n  geom_histogram(boundary = 0, \n                 binwidth = 15, \n                 position = \"dodge\") +\n  scale_x_continuous(name = \"Wait time (seconds)\",\n                     breaks = seq(0, 600, 60))\n\n\n\n\n\n\nFigure 3.17: A histogram with multiple groups.\n\n\n\n\n\n\n\n\n\n\nFrequency plot alternative\n\n\n\n\n\nAlternatively, you can use geom_freqpoly() to plot a line connecting the top of each bin.\n\n# frequency plot\nggplot(survey_data, aes(x = wait_time,\n                        colour = issue_category)) +\n  geom_freqpoly(binwidth = 15, \n                boundary = 0,\n                size = 1) +\n  scale_x_continuous(name = \"Wait time (seconds)\",\n                     breaks = seq(0, 600, 60))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nFigure 3.18: A frequency plot with multiple groups.\n\n\n\n\n\n\n\n\n3.3.3.3 Violin plot\nAnother way to compare groups of continuous variables is the violin plot. This is like a density plot, but rotated 90 degrees and mirrored - the fatter the violin, the larger proportion of data points there are at that value.\n\nviolin_area &lt;- \n  ggplot(survey_data, aes(x = issue_category, y = wait_time)) +\n  geom_violin() +\n  ggtitle('scale = \"area\"')\n\nviolin_count &lt;- \n  ggplot(survey_data, aes(x = issue_category, y = wait_time)) +\n  geom_violin(scale = \"count\") +\n  ggtitle('scale = \"count\"')\n\nviolin_area + violin_count\n\n\n\n\n\n\nFigure 3.19: The default violin plot gives each shape the same area. Set scale=‘count’ to make the size proportional to the number of observations.”\n\n\n\n\n\n3.3.3.4 Boxplot\nBoxplots serve a similar purpose to violin plots (without the giggles from the back row). They don’t show you the shape of the distribution, but rather some statistics about it. The middle line represents the median; half the data are above this line and half below it. The box encloses the 25th to 75th percentiles of the data, so 50% of the data falls inside the box. The “whiskers” extending above and below the box extend 1.5 times the height of the box, although you can change this with the coef argument. The points show outliers – individual data points that fall outside of this range.\nBoxplots can be horizontal if you swap to x and y columns, and there are many other customisations you can apply.\n\nboxplot &lt;- ggplot(survey_data, aes(x = issue_category, y = wait_time)) +\n geom_boxplot() +\n  ggtitle(\"Default vertical boxplot\")\n\ncustom &lt;- ggplot(survey_data, aes(y = issue_category,x = wait_time)) +\n geom_boxplot(fill = \"grey80\", \n              outlier.colour = \"red\",\n              outlier.shape = 8,\n              coef = 1,   # length of whiskers relative to box\n              varwidth = TRUE, # set width proportional to sample size\n              notch = TRUE) +\n  ggtitle(\"Customised horizontal boxplot\")\n\nboxplot + custom\n\n\n\n\n\n\nFigure 3.20: Boxplots.\n\n\n\n\n\n3.3.3.5 Combo plots\nViolin plots are frequently layered with other geoms that represent the mean or median values in the data. This is a lot of code; to help your understanding, run it layer by layer to see how it builds up and change the values throughout the code.\n\n# add fill and colour to the mapping\n\nggplot(survey_data,  aes(x = issue_category, \n                         y = wait_time,\n                         fill = issue_category,\n                         colour = issue_category)) +\n  scale_x_discrete(name = \"Issue Category\") +\n  scale_y_continuous(name = \"Wait Time (seconds)\",\n                     breaks = seq(0, 600, 60)) +\n  coord_cartesian(ylim = c(0, 360)) +\n  guides(fill = \"none\", colour = \"none\") + \n  # add a violin plot\n  geom_violin(draw_quantiles = 0.5, # adds a line at median (50%) score\n              alpha = 0.4) + \n  # add a boxplot\n  geom_boxplot(width = 0.25, \n               fill = \"white\", \n               alpha = 0.75, \n               fatten = 0, # removes the median line\n               outlier.alpha = 0) + \n  # add a point that represents the mean\n  stat_summary(fun = mean, \n               geom = \"point\", \n               size = 2) + \n  ggtitle(\"ViolinBox\")\n\n\n\n\n\n\nFigure 3.21: Violin plots combined with different methods to represent means and medians.\n\n\n\n\n\n\n\n\n\n\nMisleading Bar Charts\n\n\n\n\n\nA very common type of plot is to produce a bar chart of means, however, the example below demonstrates just how misleading this is. It communicates the mean value for each category, but the bars hide the distribution of the actual data. You can’t tell if most wait times are close to 3 minutes, or spread from 0 to 6 minutes, or if the vast majority are less than 2 minutes, but the mean is pulled up by some very high outliers.\nColumn plots can also be very misleading. The plot on the left starts the y-axis at 0, which makes the bar heights proportional, showing almost no difference in average wait times. Since the differences are hard to see, you may be tempted to start the y-axis higher, but that makes it look like the average wait time for returns is double that for tech.\n\n\n\n\n\n\n\nFigure 3.22: Don’t plot continuous data with column plots. They are only appropriate for count data.\n\n\n\n\n\n\n\n\nTest your understanding\n\n\n\n\n\n\n\n\n\nHow would you create plot A? \ngeom_box()\ngeom_boxplot()\ngeom_violin()\ngeom_violinplot()\n\nHow would you create plot B? \ngeom_box()\ngeom_boxplot()\ngeom_violin()\ngeom_violinplot()\n\nWhat does the mapping look like for both plots?\n\naes(x = employee_id, y = call_time, colour = employee_id)aes(x = employee_id, y = call_time, colour = call_time)aes(x = employee_id, y = call_time, fill = call_time)aes(x = employee_id, y = call_time, fill = employee_id)\n\n\nWhich employee has the longest median call time? \ne01\ne02\ne03\ne04\ne05\ne06\ne07\ne08\ne09\ne10\n\nWhich employee has the record longest call? \ne01\ne02\ne03\ne04\ne05\ne06\ne07\ne08\ne09\ne10\n\n\n\n\n3.3.4 Two continuous variables\nWhen you want to see how two continuous variables are related, set one as the x-axis and the other as the y-axis. Usually, if one variable causes the other, you plot the cause on the x-axis and the effect on the y-axis. Here, we want to see if longer wait times cause the calls to be longer.\n\n3.3.4.1 Scatterplot\nThe function to create a scatterplot is called geom_point().\n\nggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point()\n\n\n\n\n\n\nFigure 3.23: Scatterplot with geom_point().\n\n\n\n\n\n3.3.4.2 Trendlines\nIn Figure 3.2, we emphasised the relationship between wait time and call time with a trendline created by geom_smooth() using the argument method = lm (“lm” stands for “linear model” or a straight line relationship). You can also set method = loess to visualise a non-linear relationship.\n\nlm_plot &lt;- \n  ggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = lm, formula = y~x) +\n  ggtitle(\"method = lm\")\n\nloess_plot &lt;- \n  ggplot(survey_data, aes(x = wait_time, y = call_time)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = loess, formula = y~x) +\n  ggtitle(\"method = loess\")\n\nlm_plot + loess_plot\n\n\n\n\n\n\nFigure 3.24: Different ways to show the relationship between two continuous variables.\n\n\n\n\n\n\n\n\n\n\nError shading\n\n\n\nIf there isn’t much data at the extremes of the x-axis, the curve can be very uncertain. This is represented by the wider shaded area, which means that the true relationship might be anywhere within that area. Add the argument se = FALSE to geom_smooth() to remove this “standard error” shading.\n\n\n\n3.3.4.3 Dates\nThe call_start column contains both a date and a time, so use the date() function from lubridate to convert it to just a date. We’ll need it in this format to be able to transform the x-axis below.\n\nggplot(survey_data, aes(x = lubridate::date(call_start), \n                        y = satisfaction)) + \n  geom_smooth(method = lm, formula = y~x)\n\n\n\n\n\n\nFigure 3.25: Plotting dates.\n\n\n\n\nWe can use scale_x_date() to set the date_breaks to be “1 month” apart. The date_labels argument uses a code for different date formats; you can see the full list of possibilities in the help for ?strptime. For example, %b means “Abbreviated month name”, whilst if you wanted to use a format like “2020/01/31” you could try \"%Y/%m/%d\".\n\nggplot(survey_data, aes(x = lubridate::date(call_start), \n                        y = satisfaction)) +\n  geom_smooth(method = lm, formula = y~x) +\n  scale_x_date(name = \"\",\n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  scale_y_continuous(name = \"Caller Satisfaction\") +\n  ggtitle(\"2020 Caller Satisfaction\")\n\n\n\n\n\n\nFigure 3.26: Plotting dates with breaks one month apart.\n\n\n\n\n\nIt looks like customer satisfaction declined across the year, but is this change meaningful? See what the plot looks like when the y-axis spans the full range of possible satisfaction values from 1 to 5. You can also plot the individual data points to emphasise the range of values.\n\n\n\nSolution\n\nggplot(survey_data, aes(x = lubridate::date(call_start), \n                        y = satisfaction)) +\n  # show individual data, jitter the height to avoid overlap\n  geom_jitter(width = 0, height = .1, alpha = 0.2) + \n  geom_smooth(method = lm,  formula = y~x) +\n  scale_x_date(name = \"\",\n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  scale_y_continuous(name = \"Caller Satisfaction\",\n                     breaks = 1:5) +\n  coord_cartesian(ylim = c(1, 5)) + # changes limits\n  ggtitle(\"2020 Caller Satisfaction\")\n\n\n\n\n\n\n\n\n\n\n3.3.5 Overplotting\nWhen you have a limited range of numeric values, such as an ordinal rating scale, sometimes overlapping data makes it difficult to see what is going on in a point plot. For example, the plot below shows satisfaction ratings by call time, but because all the ratings are 1, 2, 3, 4 or 5, it makes it hard to see exactly how many data points there are at each point.\nIn this section, we’ll explore a few options for dealing with this.\n\nggplot(survey_data, aes(x = call_time, y = satisfaction)) + \n  geom_point()\n\n\n\n\n\n\nFigure 3.27: Overlapping data makes plots hard to understand.\n\n\n\n\n\n3.3.5.1 Jitter plot\nYou can use geom_jitter() to move the points around a bit to make them easier to see. You can also set alpha transparency. Here, the x-axis is continuous, so there is no need to jitter the width, but the y-axis is ordinal categories, so the height is jittered between -0.2 and +0.2 away from the true y-value.\n\nChange these values to understand what jitter is doing\n\n\nggplot(survey_data, aes(x = call_time, y = satisfaction)) +\n  geom_jitter(width = 0, height = .2, alpha = 0.5)\n\n\n\n\n\n\nFigure 3.28: Jitter plot.\n\n\n\n\n\n3.3.5.2 Facets\nAlternatively, you can use facet_wrap() to create a separate plot for each level of satisfaction. facet_wrap() uses the tilde (~) symbol, which you can roughly translate as “by”, e.g., facet the plot by satisfaction rating. The labeller function controls the labels above each plot. label_both specifies that we want both the variable name (satisfaction) and the value (e.g., 1) printed on the plot to make it easier to read.\n\nggplot(survey_data, aes(x = call_time)) +\n  geom_histogram(binwidth = 10, \n                 boundary = 0, \n                 fill = \"dodgerblue\", \n                 color = \"black\") +\n  facet_wrap(~satisfaction, \n             ncol = 1, # try changing this to 2 \n             labeller = label_both) +\n  scale_x_continuous(name = \"Call Time (seconds)\",\n                     breaks = seq(0, 600, 30))\n\n\n\n\n\n\nFigure 3.29: A histogram with facets.\n\n\n\n\n\n\n\n\n\n\nMore plots styles\n\n\n\nThese are not, by any means, all the plot types that you can make in R. This chapter just gave you a basic overview, and we will go into more detail in Section 10.1). The further resources section at the end of this chapter lists many resources, but the R Graph Gallery is especially useful to get inspiration for the kinds of beautiful plots you can make in R.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#exercises",
    "href": "03-viz.html#exercises",
    "title": "3  Data Visualisation",
    "section": "\n3.4 Exercises",
    "text": "3.4 Exercises\nFor the final step in this chapter, we will create a report of data visualisations. You may need to refer back to Chapter 2) to help you complete these exercises and you may also want to take a break before you work through this section. We’d also recommend you render at every step so that you can see how your output changes.\n\n3.4.1 New Document\n\nCreate and save a new quarto document named plots_report.qmd\n\nGive it the title “Personality Data”.\nRemove the default template text\nAdd the code below in the set-up code chunk:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\n\nlibrary(tidyverse) \nlibrary(patchwork) \nlibrary(ggthemes)  \nlibrary(lubridate) \n\npersonality &lt;- read_csv(\"https://psyteachr.github.io/reprores/data/personality_scores.csv\")\n```\n\n\n3.4.2 Summary\nCreate a level 2 heading titled “Overview”. Underneath this heading, write a short summary of what the data set contains (scores on a 5-factor personality questionnaire) and what each of the variables means (you can use the information from the personality_scores.json codebook).\n\n3.4.3 Appropriate plots\nCreate a visualisation with an appropriate plot style for each question below:\n\nWhat does the distribution of scores look like for Agreeableness (Ag)?\nAre Extraversion (Ex) and Openness (Op) correlated?\nHow many people completed the questionnaire each year? (hint: year(date))\n\n3.4.4 Combining plots\nDuplicate the code for your first plot and create the same for each of the five personality factors. Combine these 5 plots into a single figure.\n\n3.4.5 Polishing plots\nFor each plot:\n\nCreate a level 2 heading in your quarto document and give it an informative title.\nWrite a short summary that interprets the data shown in the plots - it’s not enough just to present visualisations, effective reports will also help the reader understand the conclusions they should draw from the plots you’ve presented.\nMake sure each plot has a figure caption.\nOrganise your report so that the plots are shown after the text summary in each section.\n\n3.4.6 Customising your report\n\nLook through the different themes available with ggtheme and choose one to apply to all your plots.\nEdit the YAML header of your quarto document so that your rendered report does not show any code, messages, or warnings.\nSet the default figure size to 8 x 5.\nCustomise the figure size for your combined plot so it is readable\nAdd a table of contents\n\n3.4.7 Inappropriate plots\nPick one plot above and make a new version:\n\nAdjust the visual aesthetics to make it look as bad and as difficult to read as possible.\nSave the plot as a PNG image\nPost this plot to the week 3 thread on Teams and explain why it is so bad. Include the code in your post (see Appendix D for advice on posting code on Teams)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#sec-glossary-viz",
    "href": "03-viz.html#sec-glossary-viz",
    "title": "3  Data Visualisation",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\nterm\ndefinition\n\n\n\nargument\nA variable that provides input to a function.\n\n\ncategorical\nData that can only take certain values, such as types of pet.\n\n\ncategorical\nData that can only take certain values, such as types of pet.\n\n\ncharacter\nA data type representing strings of text.\n\n\ncontinuous\nData that can take on any values between other existing values.\n\n\ncontinuous\nData that can take on any values between other existing values.\n\n\ndata-type\nThe kind of data represented by an object.\n\n\ndefault-value\nA value that a function uses for an argument if it is skipped.\n\n\ndouble\nA data type representing a real decimal number\n\n\nfactor\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\ngeom\nThe geometric style in which data are displayed, such as boxplot, density, or histogram.\n\n\ninteger\nA data type representing whole numbers.\n\n\nlikert\nA rating scale with a small number of discrete points in order\n\n\nlogical\nA data type representing TRUE or FALSE values.\n\n\nmedian\nThe middle number in a distribution where half of the values are larger and half are smaller.\n\n\nnominal\nCategorical variables that don't have an inherent order, such as types of animal.\n\n\nnumeric\nA data type representing a real decimal number or integer.\n\n\nobservation\nAll of the data about a single trial or question.\n\n\nordinal\nDiscrete variables that have an inherent order, such as level of education or dislike/like.\n\n\noutlier\nA data point that is extremely distant from most of the other data points\n\n\nrender\nTo create a file (usually an image or PDF) or widget from source code\n\n\nstring\nA piece of text inside of quotes.\n\n\ntidy-data\nA format for data that maps the meaning onto the structure.\n\n\nvalue\nA single number or piece of data.\n\n\nvariable\n(coding): A word that identifies and stores the value of some data for later use; (stats): An attribute or characteristic of an observation that you can measure, count, or describe\n\n\nvector\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#sec-resources-viz",
    "href": "03-viz.html#sec-resources-viz",
    "title": "3  Data Visualisation",
    "section": "Further Resources",
    "text": "Further Resources\n\nggplot2 cheat sheet\n\nData visualisation using R, for researchers who don’t use R (Nordmann et al., 2021)\n\n\nChapter 1: Data Visualisation of R for Data Science\n\nggplot2 FAQs\nggplot2 documentation\n\nHack Your Data Beautiful workshop by University of Glasgow postgraduate students\n\nChapter 28: Graphics for communication of R for Data Science\n\n\ngganimate: A package for making animated plots",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "03-viz.html#references",
    "href": "03-viz.html#references",
    "title": "3  Data Visualisation",
    "section": "References",
    "text": "References\n\n\n\n\nNordmann, E., McAleer, P., Toivo, W., Paterson, H., & DeBruine, L. M. (2021). Data visualisation using R, for researchers who don’t use R. PsyArXiv. https://doi.org/10.31234/osf.io/4huvw",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualisation</span>"
    ]
  },
  {
    "objectID": "04-summary.html",
    "href": "04-summary.html",
    "title": "4  Data Summaries",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#ilo-data",
    "href": "04-summary.html#ilo-data",
    "title": "4  Data Summaries",
    "section": "",
    "text": "Be able to structure data for scripting\nImport data from CSV and Excel files\nBe able to summarise data by groups\nBe able to produce well-formatted tables\nUse pipes to chain together functions",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#sec-func-summary",
    "href": "04-summary.html#sec-func-summary",
    "title": "4  Data Summaries",
    "section": "Functions used",
    "text": "Functions used\n\nbuilt-in (you can always use these without loading any packages)\n\nbase:: c(), dput(), library(), max(), mean(), min(), sum()\n\nstats:: quantile()\n\nutils:: head()\n\n\n\ntidyverse (you can use all these with library(tidyverse))\n\nreadr:: cols(), col_factor(), col_skip(), read_csv(), spec()\n\ndplyr:: arrange(), count(), desc(), filter(), glimpse(), group_by(), median(), mutate(), n(), summarise(), ungroup()\n\n\n\nother (you need to load each package to use these)\n\njanitor:: clean_names()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#sec-setup-summary",
    "href": "04-summary.html#sec-setup-summary",
    "title": "4  Data Summaries",
    "section": "Setup",
    "text": "Setup\n\nOpen your reprores project\nCreate a new quarto file called 04-data.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\n\n# packages needed for this chapter\nlibrary(tidyverse)     # loads readr for importing data\n                       #   and tibble for creating tables\nlibrary(readxl)        # importing excel files\nlibrary(jsonlite)      # importing JSON files\nlibrary(rio)           # importing and exporting many types of files\nlibrary(skimr)         # summarising datasets\nlibrary(janitor)       # clean up data\n```\n\nDownload the Data import cheatsheet.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#sec-import-data",
    "href": "04-summary.html#sec-import-data",
    "title": "4  Data Summaries",
    "section": "\n4.1 Import Data",
    "text": "4.1 Import Data\nThe data we’ll be working with is on method of delivery for singleton births from Public Health Scotland. You can see the most recent version at Method of Delivery, but we’ll be working from a saved version.\nThe data are in a CSV file (download 12.1_delivery.csv), so we can read this with the function read_csv() , and assign it to a new object that we’ll call births. You can learn more about importing data from other file types in Appendix G.\n\nbirths &lt;- read_csv(\"data/12.1_delivery.csv\")\n\nRows: 97077 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): FinancialYear, CA, SIMDQuintileQF, SIMDVersion, AgeGroup, Delivery,...\ndbl (2): SIMDQuintile, Livebirths\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhen you load data with read_csv(), you get a quick summary that you should always check to see if the data were loaded correctly. One common problem is that data load with the the wrong type (see Appendix H), usually because a numeric column contains some unexpected text values. So the first thing to check is that each column has the expected data type. The abbreviation “chr” means this is a character column, which can contain any text. The abbreviation “dbl” is a double, which is a number that can have decimal places.\nThere are too many columns to show all of them in the summary, so it tells you to use the spec() function to check all the columns.\n\nspec(births)\n\ncols(\n  FinancialYear = col_character(),\n  CA = col_character(),\n  SIMDQuintile = col_double(),\n  SIMDQuintileQF = col_character(),\n  SIMDVersion = col_character(),\n  AgeGroup = col_character(),\n  Delivery = col_character(),\n  Induced = col_character(),\n  Livebirths = col_double()\n)\n\n\nThis gives you the info formatted inside the cols() function to make it easy for you to copy and edit this if any of the columns imported incorrectly.\nYou can also use the glimpse() function to check the type of each column, and see a few examples of the cell values.\n\nglimpse(births)\n\nRows: 97,077\nColumns: 9\n$ FinancialYear  &lt;chr&gt; \"1997/98\", \"1997/98\", \"1997/98\", \"1997/98\", \"1997/98\", …\n$ CA             &lt;chr&gt; \"RA2704\", \"RA2704\", \"RA2704\", \"RA2704\", \"RA2704\", \"RA27…\n$ SIMDQuintile   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ SIMDQuintileQF &lt;chr&gt; \":\", \":\", \":\", \":\", \":\", \":\", \":\", \":\", \":\", \":\", \":\", …\n$ SIMDVersion    &lt;chr&gt; \"SIMD2004\", \"SIMD2004\", \"SIMD2004\", \"SIMD2004\", \"SIMD20…\n$ AgeGroup       &lt;chr&gt; \"25-34\", \"25-34\", \"25-34\", \"25-34\", \"25-34\", \"25-34\", \"…\n$ Delivery       &lt;chr&gt; \"Caesarean - Elective\", \"Caesarean - Emergency\", \"Caesa…\n$ Induced        &lt;chr&gt; \"Not Induced\", \"Induced\", \"Not Induced\", \"Induced\", \"No…\n$ Livebirths     &lt;dbl&gt; 9, 5, 8, 1, 6, 21, 60, 4, 4, 3, 1, 1, 1, 14, 2, 6, 1, 7…\n\n\nThe column SIMDQuintileQF looks a little odd. We can quickly check what all the values are in a data table with the count() function:\n\ncount(births, SIMDQuintileQF)\n\n\n\n\nSIMDQuintileQF\nn\n\n\n\n:\n1137\n\n\nNA\n95940\n\n\n\n\n\n\nIt looks like this column doesn’t contain any useful info, so we can just ignore it, or skip it. To do this at the point of loading in the data, we create an object (birth_cols) that contains our column specification using the col_types argument and two helper functions, cols() and col_skip(). We then pass birth_cols to the col_types argument of read_csv() which uses this info to amend the file that is loaded in.\n\n# edit the output of spec(births)\nbirth_cols &lt;- cols(\n  SIMDQuintileQF = col_skip()\n)\n\nbirths &lt;- read_csv(\"data/12.1_delivery.csv\", col_types = birth_cols)\n\nThere’s also additional edits you can make to the initial file, for example, you can also set the order of levels for categorical data when you first import it.\nTo check which groups our data has, we can use the count() function to check the level labels for AgeGroup, Delivery and Induced; and set sort = TRUE to sort by frequency.\n\ncount(births, AgeGroup)\ncount(births, Delivery, sort = TRUE)\ncount(births, Induced, sort = TRUE)\n\n\n\n\nAgeGroup\nn\n\n\n\n25-34\n37557\n\n\n35 and over\n29592\n\n\nUnder 25\n29918\n\n\nUnknown\n10\n\n\n\n\n\n\n\nDelivery\nn\n\n\n\nSpontaneous\n25607\n\n\nCaesarean - Emergency\n22526\n\n\nForceps\n19041\n\n\nVacuum\n14573\n\n\nCaesarean - Elective\n12417\n\n\nBreech\n2483\n\n\nNot Known\n430\n\n\n\n\n\n\n\nInduced\nn\n\n\n\nNot Induced\n54217\n\n\nInduced\n39097\n\n\nUnknown\n3763\n\n\n\n\n\n\nWe’ll now add an extra code to birth_cols to set the order of our factors. What order you choose will depend on what makes most sense for the data. For AgeGroup we’ll list them in chronological order, whilst for Delivery and Induced, we’ll sort them according to the highest value - it can be helpful to think of what order you’ll like the bars to be in if you were making a graph.\n\n\n\n\n\n\nTip\n\n\n\nThe levels for Delivery are numerous and complex, which increases the risk of a typo and is just tedious to type. Here is a quick trick to generate the text you can copy and paste into your code. The function dput() gives you the code you can use to recreate an object.\n\n\n\nType in the console\n\ndelivery_table &lt;- count(births, Delivery, sort = TRUE)\ndput(delivery_table$Delivery)\n\n\nc(\"Spontaneous\", \"Caesarean - Emergency\", \"Forceps\", \"Vacuum\", \n\"Caesarean - Elective\", \"Breech\", \"Not Known\")\n\n\n\n\n\n# edit the output of spec(births)\nbirth_cols &lt;- cols(\n  SIMDQuintileQF = col_skip(),\n  AgeGroup = col_factor(levels = c(\"Under 25\", \"25-34\", \"35 and over\",\"Unknown\")),\n  Delivery = col_factor(levels = c(\"Spontaneous\", \n                                   \"Caesarean - Emergency\", \n                                   \"Forceps\", \n                                   \"Vacuum\", \n                                   \"Caesarean - Elective\", \n                                   \"Breech\", \n                                   \"Not Known\")),\n  Induced = col_factor(levels = c(\"Not Induced\", \"Induced\", \"Unknown\"))\n)\n\nbirths &lt;- read_csv(\"data/12.1_delivery.csv\", col_types = birth_cols)\n\n\n4.1.1 Clean up names\nData sets often import with column names that are a little difficult to use in code. You can manually rename them, but the janitor package makes this incredibly easy. This dataset has pretty good names, with no spaces or special characters, but it does use uppercase letters to separate words, which can be hard to remember. the clean_names() function changes everything to lowercase and uses underscores to separate word parts. Using clean_names every time you import a dataset is a good habit to get into.\n\nbirths &lt;- clean_names(births)\n\ncolnames(births)\n\n[1] \"financial_year\" \"ca\"             \"simd_quintile\"  \"simd_version\"  \n[5] \"age_group\"      \"delivery\"       \"induced\"        \"livebirths\"    \n\n\n\n4.1.2 Exploring a Dataset\nThere are 97077 rows and 8 columns of data. The data dictionary on the NHS website includes the following, but doesn’t clearly state what the possible values are for each column. We’re going to practice figuring out more about how data is structured using data summaries in this exercise, since it’s a common task to get a mysteriously formatted data file and need to figure it out yourself. At the very least, this exercise should remind you to never do this to other people – always provide a clear codebook with all values!\n\nData Codebook\n\n\n\n\n\n\nColumn\nType\nLabel\n\n\n\nFinancialYear\ntext\nData is recorded for financial years (1st April to 31st March) based on the date of the mother’s discharge\n\n\nCA\ntext\n9-digit code for Council areas based on boundaries as at 1st April 2019\n\n\nSIMDQuintile\ntext\nScottish Index of Multiple Deprivation (SIMD) quintile; 1(Most Deprived) - 5(Least Deprived)\n\n\nSIMDQuintileQF\ntext\nQualifier for SIMDQuintile indicating unknown quintiles\n\n\nSIMDVersion\ntext\nMost appropriate SIMD release used for each year\n\n\nAgeGroup\ntext\nAge Group of the mother at time of admission\n\n\nDelivery\ntext\nThe method by which the baby was delivered\n\n\nInduced\ntext\nWas the delivery induced, that is, was it started artificially\n\n\nLivebirths\nnumeric\nNumber of live births\n\n\n\n4.1.3 The $ operator\nWe need to take a couple of brief detours to introduce some additional coding conventions. First, let’s introduce the $ notation. The dollar sign allows you to select items from some objects, like lists or data frames. The left-hand side is the object, and the right-hand side is the item. Here, we will select columns from a table. When you call a column like this, R will return all the observations in that column.\n\nyears &lt;- births$financial_year\n\nIf your item has multiple observations, you can specify which ones to return using square brackets [] and the row number or a vector of row numbers.\n\nbirths$financial_year[1] # select the first observation\nbirths$livebirths[c(20,30,40)] # select multiple with c()\n\n[1] \"1997/98\"\n[1]  2 42  3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#sec-pipes-first",
    "href": "04-summary.html#sec-pipes-first",
    "title": "4  Data Summaries",
    "section": "\n4.2 Pipes",
    "text": "4.2 Pipes\n\n\n\nLet’s say you want to filter the dataset down to just the emergency C-sections, and then sum up the total number of live births per year, and then arrange the data from largest to smallest (don’t worry, we’ll introduce these functions soon). You could do it by creating intermediate objects for each step:\n\n# filter the table to just emergency c-sections\nc_sections &lt;- filter(births, \n                     delivery == \"Caesarean - Emergency\")\n\n# calculate the total number of births per year\nc_per_year &lt;- summarise(c_sections, \n                        n = sum(livebirths), \n                        .by = financial_year)\n\n# sort by n, descending\nc_sorted &lt;- arrange(c_per_year, desc(n))\n\nhead(c_sorted)\n\n\n\n\nfinancial_year\nn\n\n\n\n2014/15\n8964\n\n\n2015/16\n8958\n\n\n2021/22\n8920\n\n\n2011/12\n8916\n\n\n2013/14\n8810\n\n\n2022/23\n8739\n\n\n\n\n\n\nWhilst the above code is functional, it adds three unnecessary objects to the environment, increasing cognitive load and the risk of mistakes. Enter… the pipe, that weird |&gt; you may have seen.\nPipes allow you to send the output from one function straight into another function. Specifically, they send the result of the function before |&gt; to be an argument in the function after |&gt;. By default, this is the first argument, but we’ll show you ways to change that later.\nIt can be useful to translate the pipe as “and then”. It’s easier to show than tell, so let’s look at an example.\n\nc_per_year &lt;- births |&gt; # and then\n  \n  filter(delivery == \"Caesarean - Emergency\") |&gt; # and then\n  \n  summarise(n = sum(livebirths), \n            .by = financial_year) |&gt; # and then\n  \n  arrange(desc(n))\n\nhead(c_per_year)\n\n\n\n\nfinancial_year\nn\n\n\n\n2014/15\n8964\n\n\n2015/16\n8958\n\n\n2021/22\n8920\n\n\n2011/12\n8916\n\n\n2013/14\n8810\n\n\n2022/23\n8739\n\n\n\n\n\n\nNotice that filter(), summarise() and arrange() no longer need the first argument to be the data table; it is pulled in from the pipe above. The power of the pipe may not be obvious now, but it will soon prove its worth.\n\n\n\n\n\n\nNote\n\n\n\nBase R recently added a “native pipe” that looks like this: |&gt;, while the tidyverse has traditionally used the “magrittr pipe” that looks like this %&gt;%. They have a few small differences that you don’t need to learn about yet. We’ll be using the base R pipe, but you might see the magrittr pipe in other sources.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#counting",
    "href": "04-summary.html#counting",
    "title": "4  Data Summaries",
    "section": "\n4.3 Counting",
    "text": "4.3 Counting\nYou can count categorical data with the count() function. This will give you a new table with each combination of the counted columns and a column called n containing the number of rows from that group.\nLet’s figure out how many entries there were per delivery type. The first argument is the name of the data table object, and the second argument is the name of the column we want to count.\n\ncount(births, delivery)\n\n\n\n\ndelivery\nn\n\n\n\nSpontaneous\n25607\n\n\nCaesarean - Emergency\n22526\n\n\nForceps\n19041\n\n\nVacuum\n14573\n\n\nCaesarean - Elective\n12417\n\n\nBreech\n2483\n\n\nNot Known\n430\n\n\n\n\n\n\nThere are 7 types of deliveries, and the new column n tells you how many rows of the data table there are per type.\nYou can add on a column with the numbers expressed in percent using the function mutate(). We’ll go into more detail on how to use mutate() in Chapter 7, but for now, it can be used to add new columns or overwrite existing columns.\nThe code below divides the value in the n column by the total sum of the numbers in that column, and adds it to a new column called percent. The next step modifies the percent column by multiplying it by 100 and rounding the value. You could do this all in one step, like round(100 * n / sum(n)), but often it’s clearer to break it into a few steps to avoid too many nested parentheses.\n\ncount(births, delivery) |&gt;\n  mutate(percent = n / sum(n),\n         percent = round(100 * percent))\n\n\n\n\ndelivery\nn\npercent\n\n\n\nSpontaneous\n25607\n26\n\n\nCaesarean - Emergency\n22526\n23\n\n\nForceps\n19041\n20\n\n\nVacuum\n14573\n15\n\n\nCaesarean - Elective\n12417\n13\n\n\nBreech\n2483\n3\n\n\nNot Known\n430\n0\n\n\n\n\n\n\nWe can also count combinations of columns by adding more arguments. The table below shows the number of rows per age group and induction status, sorted by the number of rows. We won’t add on percent just yet as the additional variable requires another function that we’ll come back to later.\n\ncount(births, age_group, induced, sort = TRUE)\n\n\n\n\nage_group\ninduced\nn\n\n\n\n25-34\nNot Induced\n20441\n\n\n35 and over\nNot Induced\n16965\n\n\nUnder 25\nNot Induced\n16805\n\n\n25-34\nInduced\n15176\n\n\nUnder 25\nInduced\n12206\n\n\n35 and over\nInduced\n11711\n\n\n25-34\nUnknown\n1940\n\n\n35 and over\nUnknown\n916\n\n\nUnder 25\nUnknown\n907\n\n\nUnknown\nNot Induced\n6\n\n\nUnknown\nInduced\n4\n\n\n\n\n\n\n\nHow would you create the table of counts below?\n\n\n\n\n\ninduced\nn\n\n\n\nNot Induced\n54217\n\n\nInduced\n39097\n\n\nUnknown\n3763\n\n\n\n\n\n\n\ncount(births, induced, sort = TRUE)count(induced, births)count(induced, births, sort = TRUE)count(births, induced)\n\n\nHowever, the numbers above are not the number of births, but rather the number of rows in the data set. The column live_births contains the number per each category, so we will need to add those numbers together to see the total number of births.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#sec-summary-summarise",
    "href": "04-summary.html#sec-summary-summarise",
    "title": "4  Data Summaries",
    "section": "\n4.4 Summarise",
    "text": "4.4 Summarise\nThe summarise() function from the dplyr package is loaded as part of the tidyverse and creates summary statistics. It creates a new table with columns that summarise the data from a larger table using summary functions. Check the Data Transformation Cheat Sheet for various summary functions. Some common ones are: n(), min(), max(), sum(), mean(), and quantile().\n\n\n\n\n\n\nWarning\n\n\n\nIf you get the answer NA from a summary function, that usually means that there are missing values in the columns you were summarising. This may seem counter-intuitive but it is actually very logical if you consider that NA means “I don’t know the value of this cell” because the average of 1 + 2 + I don’t know isn’t 1.5, it’s “I don’t know”. We’ll discuss this more in Section 7.2.2, but you can ignore missing values for many functions by adding the argument na.rm = TRUE.\n\nvalues &lt;- c(1, 2, 4, 3, NA, 2)\nmean(values) # is NA\nmean(values, na.rm = TRUE) # removes NAs first\n\n[1] NA\n[1] 2.4\n\n\n\n\nThis function can be used to answer questions like: How many total live births were there? What are the mean and median number of births per year? Let’s start with a very simple example to calculate the total number of births:\n\nThe first argument that summarise() takes is the data table you wish to summarise, in this case the object delivery.\n\nsummarise() will create a new table. The column names of this new table will be the left hand-side arguments (e.g., total_births)\nThe values of these columns are the result of the summary operation on the right hand-side.\n\n\nsummarise(births,\n          total_births = sum(livebirths))\n\n\n\n\ntotal_births\n\n\n1351669\n\n\n\n\n\nIf you want to summarise by category, you can use the .by argument, as long as you have a version of dplyr that is 1.1.0 or above (if not, you can use the method in the next section).\n\nsummarise(births,\n          total_births = sum(livebirths),\n          .by = delivery)\n\n\n\n\ndelivery\ntotal_births\n\n\n\nCaesarean - Elective\n156405\n\n\nCaesarean - Emergency\n209473\n\n\nForceps\n115698\n\n\nSpontaneous\n815827\n\n\nVacuum\n50470\n\n\nBreech\n3282\n\n\nNot Known\n514\n\n\n\n\n\n\nIf you want to group by more than one column, use the c() function to group the column names.\n\nsummarise(births,\n          total_births = sum(livebirths),\n          .by = c(delivery, induced))\n\n\n\n\ndelivery\ninduced\ntotal_births\n\n\n\nCaesarean - Elective\nNot Induced\n154531\n\n\nCaesarean - Emergency\nInduced\n81812\n\n\nCaesarean - Emergency\nNot Induced\n125973\n\n\nForceps\nInduced\n42440\n\n\nForceps\nNot Induced\n72528\n\n\nSpontaneous\nInduced\n225033\n\n\nSpontaneous\nNot Induced\n586595\n\n\nVacuum\nNot Induced\n32492\n\n\nVacuum\nInduced\n17714\n\n\nBreech\nInduced\n300\n\n\nBreech\nNot Induced\n2966\n\n\nSpontaneous\nUnknown\n4199\n\n\nVacuum\nUnknown\n264\n\n\nCaesarean - Emergency\nUnknown\n1688\n\n\nNot Known\nNot Induced\n331\n\n\nCaesarean - Elective\nInduced\n1549\n\n\nForceps\nUnknown\n730\n\n\nNot Known\nInduced\n146\n\n\nBreech\nUnknown\n16\n\n\nCaesarean - Elective\nUnknown\n325\n\n\nNot Known\nUnknown\n37",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#sec-grouping",
    "href": "04-summary.html#sec-grouping",
    "title": "4  Data Summaries",
    "section": "\n4.5 Grouping",
    "text": "4.5 Grouping\nYou can also create summary values by group using a combination of group_by() and summarise(). The function group_by() takes an existing data table and converts it into a grouped table, where any operations that are subsequently performed on it are done “by group”.\nIt differs from the .by argument to summarise() in that it is persistent, so the table stays grouped until you explicitly remove the groups using the ungroup() functions, while the .by argument only applies to the function it is inside. Most of the code examples you’ll see use this style, since the .by argument is fairly new.\n\nbirths |&gt;\n  group_by(delivery) |&gt;\n  summarise(births = sum(livebirths)) |&gt;\n  ungroup()\n\n\n\n\ndelivery\nbirths\n\n\n\nSpontaneous\n815827\n\n\nCaesarean - Emergency\n209473\n\n\nForceps\n115698\n\n\nVacuum\n50470\n\n\nCaesarean - Elective\n156405\n\n\nBreech\n3282\n\n\nNot Known\n514\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure you call the ungroup() function when you are done with grouped functions. Failing to do this can cause all sorts of mysterious problems if you use that data table later assuming it isn’t grouped.\n\n\nYou might have noticed that the order of the table above is different from the order when using the .by argument of summarise(). This is because group_by() always sorts the values in the grouping columns in ascending order, while .by leaves them in the order they are first encountered in your data frame. Use the function arrange() to manually change order (see Section 7.1.3).\n\nbirths |&gt;\n  group_by(delivery) |&gt;\n  summarise(births = sum(livebirths)) |&gt;\n  ungroup() |&gt;\n  arrange(desc(births)) # sort by number of births in descending order\n\n\n\n\ndelivery\nbirths\n\n\n\nSpontaneous\n815827\n\n\nCaesarean - Emergency\n209473\n\n\nCaesarean - Elective\n156405\n\n\nForceps\n115698\n\n\nVacuum\n50470\n\n\nBreech\n3282\n\n\nNot Known\n514\n\n\n\n\n\n\n\n4.5.1 Multiple groupings\nYou can add multiple variables to group_by() to further break down your data. For example, the below gives us the number of births broken down by delivery type and year.\n\nReverse the order of delivery and financial_yearingroup_by()` to see how it changes the output.\n\n\nbirths |&gt;\n  group_by(delivery, financial_year) |&gt;\n  summarise(n = sum(livebirths)) |&gt;\n  ungroup()\n\n\n\n\n\n\n\nWarning\n\n\n\nYou may get the following message when using summarise() after group_by().\n\nsummarise() has grouped output by ‘delivery’. You can override using the .groups argument.\n\nTidyverse recently added a message to remind you whether the summarise() function automatically ungroups grouped data or not (it may do different things depending on how it’s used). You can set the argument .groups to “drop”, “drop_last”, “keep”, or “rowwise” (see the help for ?summarise), but it’s good practice to explicitly use ungroup() when you’re done working by groups, regardless.\n\n\n\n4.5.2 Percent by groups\nCalculating percent by groups is a great example of the flexibility of group_by() but also why you have to be very careful and always check the output of your code.\nWhen we just had one variable to count, adding percent was nice and easy:\n\ncount(births, delivery) |&gt;\n  mutate(percent = n/sum(n)*100)\n\n\n\n\ndelivery\nn\npercent\n\n\n\nSpontaneous\n25607\n26.3780298\n\n\nCaesarean - Emergency\n22526\n23.2042605\n\n\nForceps\n19041\n19.6143268\n\n\nVacuum\n14573\n15.0117948\n\n\nCaesarean - Elective\n12417\n12.7908773\n\n\nBreech\n2483\n2.5577634\n\n\nNot Known\n430\n0.4429474\n\n\n\n\n\n\nWith multiple variables, problems can arise if we use the exact same approach because by default, it will calculate the percent that each row contributes to the dataset as a whole. This might be what you want:\n\ncount(births, age_group, induced, sort = TRUE)|&gt;\n  mutate(percent = n/sum(n)*100)\n\n\n\n\nage_group\ninduced\nn\npercent\n\n\n\n25-34\nNot Induced\n20441\n21.0564809\n\n\n35 and over\nNot Induced\n16965\n17.4758182\n\n\nUnder 25\nNot Induced\n16805\n17.3110005\n\n\n25-34\nInduced\n15176\n15.6329512\n\n\nUnder 25\nInduced\n12206\n12.5735241\n\n\n35 and over\nInduced\n11711\n12.0636196\n\n\n25-34\nUnknown\n1940\n1.9984136\n\n\n35 and over\nUnknown\n916\n0.9435809\n\n\nUnder 25\nUnknown\n907\n0.9343099\n\n\nUnknown\nNot Induced\n6\n0.0061807\n\n\nUnknown\nInduced\n4\n0.0041204\n\n\n\n\n\n\nHowever, it’s more likely that you would want to calculate percent by groups. For example, what percent of people in the Induced category were in which age group? Or what percent of 25-34 year olds were induced? To calculate these numbers, we add in a call to group_by():\n\n# group by age, percentages within each age group will sum to 100\ncount(births, age_group, induced)|&gt; #age then induced\n  group_by(age_group) |&gt;\n  mutate(percent = n/sum(n)*100)\n\n# group by induced, percentages within each induction group will sum to 100\ncount(births, induced, age_group)|&gt; #induced then age\n  group_by(induced) |&gt;\n  mutate(percent = n/sum(n)*100)\n\n\n\n\nage_group\ninduced\nn\npercent\n\n\n\nUnder 25\nNot Induced\n16805\n56.170198\n\n\nUnder 25\nInduced\n12206\n40.798182\n\n\nUnder 25\nUnknown\n907\n3.031620\n\n\n25-34\nNot Induced\n20441\n54.426605\n\n\n25-34\nInduced\n15176\n40.407913\n\n\n25-34\nUnknown\n1940\n5.165482\n\n\n35 and over\nNot Induced\n16965\n57.329684\n\n\n35 and over\nInduced\n11711\n39.574885\n\n\n35 and over\nUnknown\n916\n3.095431\n\n\nUnknown\nNot Induced\n6\n60.000000\n\n\nUnknown\nInduced\n4\n40.000000\n\n\n\n\n\n\n\ninduced\nage_group\nn\npercent\n\n\n\nNot Induced\nUnder 25\n16805\n30.9958131\n\n\nNot Induced\n25-34\n20441\n37.7021967\n\n\nNot Induced\n35 and over\n16965\n31.2909235\n\n\nNot Induced\nUnknown\n6\n0.0110666\n\n\nInduced\nUnder 25\n12206\n31.2197867\n\n\nInduced\n25-34\n15176\n38.8162775\n\n\nInduced\n35 and over\n11711\n29.9537049\n\n\nInduced\nUnknown\n4\n0.0102310\n\n\nUnknown\nUnder 25\n907\n24.1031092\n\n\nUnknown\n25-34\n1940\n51.5546107\n\n\nUnknown\n35 and over\n916\n24.3422801\n\n\n\n\n\n\nIf you have updated dyplr then you can also use the .by argument in mutate():\n\n# group by age, percentages within each age group will sum to 100\ncount(births, age_group, induced)|&gt; #age then induced\n  mutate(percent = n/sum(n)*100,\n         .by = age_group)\n\n# group by induced, percentages within each induction group will sum to 100\ncount(births, induced, age_group)|&gt; #induced then age\n  mutate(percent = n/sum(n)*100,\n         .by = induced)\n\n\n\n\nage_group\ninduced\nn\npercent\n\n\n\nUnder 25\nNot Induced\n16805\n56.170198\n\n\nUnder 25\nInduced\n12206\n40.798182\n\n\nUnder 25\nUnknown\n907\n3.031620\n\n\n25-34\nNot Induced\n20441\n54.426605\n\n\n25-34\nInduced\n15176\n40.407913\n\n\n25-34\nUnknown\n1940\n5.165482\n\n\n35 and over\nNot Induced\n16965\n57.329684\n\n\n35 and over\nInduced\n11711\n39.574885\n\n\n35 and over\nUnknown\n916\n3.095431\n\n\nUnknown\nNot Induced\n6\n60.000000\n\n\nUnknown\nInduced\n4\n40.000000\n\n\n\n\n\n\n\ninduced\nage_group\nn\npercent\n\n\n\nNot Induced\nUnder 25\n16805\n30.9958131\n\n\nNot Induced\n25-34\n20441\n37.7021967\n\n\nNot Induced\n35 and over\n16965\n31.2909235\n\n\nNot Induced\nUnknown\n6\n0.0110666\n\n\nInduced\nUnder 25\n12206\n31.2197867\n\n\nInduced\n25-34\n15176\n38.8162775\n\n\nInduced\n35 and over\n11711\n29.9537049\n\n\nInduced\nUnknown\n4\n0.0102310\n\n\nUnknown\nUnder 25\n907\n24.1031092\n\n\nUnknown\n25-34\n1940\n51.5546107\n\n\nUnknown\n35 and over\n916\n24.3422801",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#additional-functions",
    "href": "04-summary.html#additional-functions",
    "title": "4  Data Summaries",
    "section": "\n4.6 Additional Functions",
    "text": "4.6 Additional Functions\nYou can also use additional functions like filter() after group_by or with the .by argument. You’ll learn more about these in Chapter 7 but briefly:\n\n\nfilter() keeps observations (rows) according to specified criteria, e.g., all values above 5, or all induced births\n\narrange() sorts the rows by value\n\nYou can combine functions like this to get detailed insights into your data. For example, you can\n\nrecode the financial year into just the first year, and make it numeric\nfilter your data to remove unknown ages and delivery types\n\n\nbirths_per_year_type_age &lt;- births |&gt;\n  mutate(year = str_extract(financial_year, \".{4}\"), # takes first four digits\n         year = as.integer(year)) |&gt; # transform to numeric\n  filter(age_group != \"Unknown\", # remove unknown\n         delivery != \"Not Known\") |&gt; # remove not known\n  summarise(n = sum(livebirths),\n            .by = c(year, delivery, age_group)) |&gt;\n  mutate(pcnt = n / sum(n) * 100, \n         .by = c(year, age_group))\n  \n\n# show just the first 6 rows\nhead(births_per_year_type_age)\n\n\n\n\nyear\ndelivery\nage_group\nn\npcnt\n\n\n\n1997\nCaesarean - Elective\n25-34\n2454\n7.114899\n\n\n1997\nCaesarean - Emergency\n25-34\n3875\n11.234815\n\n\n1997\nForceps\n25-34\n2570\n7.451219\n\n\n1997\nSpontaneous\n25-34\n23933\n69.389116\n\n\n1997\nVacuum\n25-34\n1534\n4.447537\n\n\n1997\nCaesarean - Elective\n35 and over\n801\n10.831643\n\n\n\n\n\n\n\nRe-write the code above using group_by() instead of the .by argument?\n\nNow you can use your skills from Chapter 3 to plot the data! The code below has a few elements you haven’t seen before. For example, it adds a transparent horizontal line at 0, which is a trick to force all the y-axes to start at 0, but allows different scales per facet. It also angles the text in the x-axis.\n\nbirths_per_year_type_age  |&gt;\n  ggplot(aes(x = year, y = pcnt, color = age_group)) +\n  geom_line() +\n  facet_wrap(~delivery, scales = \"free_y\") +\n  geom_hline(yintercept = 0, color = \"transparent\") + \n  labs(x = NULL, \n       y = \"Percent of Live Births per Year\",\n       color = \"Age Group\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\nFigure 4.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#exercises",
    "href": "04-summary.html#exercises",
    "title": "4  Data Summaries",
    "section": "\n4.7 Exercises",
    "text": "4.7 Exercises\nTake a break and then try one (or more) of the following:\n\nCreate a quarto report that presents the above code with nice formatting. For example, hide the code from the output and format the tables.\nExplore the data and add at least one unique insights of your own to the report.\nDownload a different data set from Scottish Health and Social Care Open Data and create summary tables and plots.\nIf you have data of your own, practice summarising this in R.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#sec-glossary-summary",
    "href": "04-summary.html#sec-glossary-summary",
    "title": "4  Data Summaries",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\nterm\ndefinition\n\n\n\ncategorical\nData that can only take certain values, such as types of pet.\n\n\ncharacter\nA data type representing strings of text.\n\n\ncsv\nComma-separated variable: a file type for representing data where each variable is separated from the next by a comma.\n\n\ndouble\nA data type representing a real decimal number\n\n\nmean\nA descriptive statistic that measures the average value of a set of numbers.\n\n\nmedian\nThe middle number in a distribution where half of the values are larger and half are smaller.\n\n\npipe\nA way to order your code in a more readable format using the symbol %&gt;%\n\n\nvector\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "04-summary.html#sec-resources-summary",
    "href": "04-summary.html#sec-resources-summary",
    "title": "4  Data Summaries",
    "section": "Further resources",
    "text": "Further resources\n\nData transformation cheat sheet\n\nChapter 3: Data Transformation in R for Data Science",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Summaries</span>"
    ]
  },
  {
    "objectID": "05-join.html",
    "href": "05-join.html",
    "title": "5  Data Relations",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#sec-ilo-joins",
    "href": "05-join.html#sec-ilo-joins",
    "title": "5  Data Relations",
    "section": "",
    "text": "Be able to match related data across multiple tables\nBe able to combine data from multiple files",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#functions-joins",
    "href": "05-join.html#functions-joins",
    "title": "5  Data Relations",
    "section": "\n5.1 Functions used",
    "text": "5.1 Functions used\n\nbuilt-in (you can always use these without loading any packages)\n\nbase:: library(), dir.create(), list.files(), as.numeric(), as.factor(), as.character(), as.data.frame()\n\n\n\ntidyverse (you can use all these with library(tidyverse))\n\nreadr:: readr::write_csv(), readr::read_csv()\n\ndplyr:: dplyr::left_join(), dplyr::right_join(), dplyr::inner_join(), dplyr::full_join(), dplyr::semi_join(), dplyr::anti_join(), dplyr::bind_rows(), dplyr::bind_cols(), dplyr::intersect(), dplyr::union(), dplyr::setdiff(), dplyr::mutate()\n\ntibble:: tibble::tibble(), tibble::as_tibble()\n\npurrr:: purrr::map_df()\n\nstringr:: stringr::str_replace_all()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#sec-setup-joins",
    "href": "05-join.html#sec-setup-joins",
    "title": "5  Data Relations",
    "section": "\n5.2 Set-up",
    "text": "5.2 Set-up\n\nOpen your reprores project\nCreate a new quarto file called 05-relations.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\nlibrary(tidyverse)     # includes readr & tibble\n```\n\nDownload the Data transformation cheatsheet.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#sec-joins-data",
    "href": "05-join.html#sec-joins-data",
    "title": "5  Data Relations",
    "section": "\n5.3 Loading data",
    "text": "5.3 Loading data\nThe data you want to report on or visualise are often in more than one file (or more than one tab of an excel file or googlesheet). You might need to join up a table of customer information with a table of orders, or combine the monthly social media reports across several months.\n\nFor this demo, rather than loading in data, create two small data tables from scratch using the tibble() function.\n\ncustomers &lt;- tibble(\n  id = 1:5,\n  city = c(\"Port Ellen\", \"Dufftown\", NA, \"Aberlour\", \"Tobermory\"),\n  postcode = c(\"PA42 7DU\", \"AB55 4DH\", NA, \"AB38 7RY\", \"PA75 6NR\")\n)\n\norders &lt;- tibble(\n  id = c(2, 3, 4, 4, 5, 5, 6, 6, 7),\n  items = c(10, 18, 21, 23, 9, 11, 11, 12, 3)\n)\n\n\ncustomers has id, city and postcode for five customers 1-5.\n\n\n1:5 will fill the variable id with all integers between 1 and 5.\n\ncity and code both use the c() function to enter multiple strings. Note that each entry is contained within its own quotation marks, apart from missing data, which is recorded as NA.\nWhen entering data like this, it’s important that the order of each variable matches up. So number 1 will correspond to “Port Ellen” and “PA42 7DU”.\n\n\n\n\n\n\nid\ncity\npostcode\n\n\n\n1\nPort Ellen\nPA42 7DU\n\n\n2\nDufftown\nAB55 4DH\n\n\n3\nNA\nNA\n\n\n4\nAberlour\nAB38 7RY\n\n\n5\nTobermory\nPA75 6NR\n\n\n\n\nDemo customers table\n\n\norders has customer id and the number of items ordered. Some customers from the previous table have no orders, some have more than one order, and some are not in the customer table.\n\n\n\n\n\nid\nitems\n\n\n\n2\n10\n\n\n3\n18\n\n\n4\n21\n\n\n4\n23\n\n\n5\n9\n\n\n5\n11\n\n\n6\n11\n\n\n6\n12\n\n\n7\n3\n\n\n\n\nDemo orders table.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#sec-mutating-joins",
    "href": "05-join.html#sec-mutating-joins",
    "title": "5  Data Relations",
    "section": "\n5.4 Mutating Joins",
    "text": "5.4 Mutating Joins\nMutating joins act like the dplyr::mutate() function in that they add new columns to one table based on values in another table. (We’ll learn more about the mutate() function in Chapter 6).)\nAll the mutating joins have this basic syntax:\n****_join(x, y, by = NULL, suffix = c(\".x\", \".y\"))\n\n\nx = the first (left) table\n\ny = the second (right) table\n\nby = what columns to match on. If you leave this blank, it will match on all columns with the same names in the two tables.\n\nsuffix = if columns have the same name in the two tables, but you aren’t joining by them, they get a suffix to make them unambiguous. This defaults to “.x” and “.y”, but you can change it to something more meaningful.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can leave out the by argument if you’re matching on all of the columns with the same name, but it’s good practice to always specify it so your code is robust to changes in the loaded data.\n\n\n\n5.4.1 left_join()\n\n\n\nA left_join keeps all the data from the first (left) table and adds anything that matches from the second (right) table. If the right table has more than one match for a row in the left table, there will be more than one row in the joined table (see ids 4 and 5).\n\nLeft join the order data to the customer data, matching by the id column.\n\nleft_data &lt;- left_join(customers, orders, by = \"id\")\nleft_data\n\n\n\n\nid\ncity\npostcode\nitems\n\n\n\n1\nPort Ellen\nPA42 7DU\nNA\n\n\n2\nDufftown\nAB55 4DH\n10\n\n\n3\nNA\nNA\n18\n\n\n4\nAberlour\nAB38 7RY\n21\n\n\n4\nAberlour\nAB38 7RY\n23\n\n\n5\nTobermory\nPA75 6NR\n9\n\n\n5\nTobermory\nPA75 6NR\n11\n\n\n\n\n\n\n\n\n\n\nThe order you specify the tables matters, in the below code we have reversed the order and so the result is all rows from the orders table joined to any matching rows from the customers table.\n\nleft2_data &lt;- left_join(orders, customers, by = \"id\")\nleft2_data\n\n\n\n\nid\nitems\ncity\npostcode\n\n\n\n2\n10\nDufftown\nAB55 4DH\n\n\n3\n18\nNA\nNA\n\n\n4\n21\nAberlour\nAB38 7RY\n\n\n4\n23\nAberlour\nAB38 7RY\n\n\n5\n9\nTobermory\nPA75 6NR\n\n\n5\n11\nTobermory\nPA75 6NR\n\n\n6\n11\nNA\nNA\n\n\n6\n12\nNA\nNA\n\n\n7\n3\nNA\nNA\n\n\n\n\n\n\n\n5.4.2 right_join()\n\n\n\nA right_join keeps all the data from the second (right) table and joins anything that matches from the first (left) table.\n\nRight join the order data to the customer data, matching by the id column.\n\nright_data &lt;- right_join(customers, orders, by = \"id\")\nright_data\n\n\n\n\nid\ncity\npostcode\nitems\n\n\n\n2\nDufftown\nAB55 4DH\n10\n\n\n3\nNA\nNA\n18\n\n\n4\nAberlour\nAB38 7RY\n21\n\n\n4\nAberlour\nAB38 7RY\n23\n\n\n5\nTobermory\nPA75 6NR\n9\n\n\n5\nTobermory\nPA75 6NR\n11\n\n\n6\nNA\nNA\n11\n\n\n6\nNA\nNA\n12\n\n\n7\nNA\nNA\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis table has the same information as left_join(orders, customers, by = \"id\"), but the columns are in a different order (left table, then right table).\n\n\n\n5.4.3 inner_join()\n\n\n\nAn inner_join returns all the rows that have a match in both tables. Changing the order of the tables will change the order of the columns, but not which rows are kept.\n\nInner join the order data to the customer data, matching by the id column.\n\ninner_data &lt;- inner_join(customers, orders, by = \"id\")\ninner_data\n\n\n\n\nid\ncity\npostcode\nitems\n\n\n\n2\nDufftown\nAB55 4DH\n10\n\n\n3\nNA\nNA\n18\n\n\n4\nAberlour\nAB38 7RY\n21\n\n\n4\nAberlour\nAB38 7RY\n23\n\n\n5\nTobermory\nPA75 6NR\n9\n\n\n5\nTobermory\nPA75 6NR\n11\n\n\n\n\n\n\n\n\n5.4.4 full_join()\n\n\n\nA full_join lets you join up rows in two tables while keeping all of the information from both tables. If a row doesn’t have a match in the other table, the other table’s column values are set to NA.\n\nFull join the order data to the customer data, matching by the id column.\n\nfull_data &lt;- full_join(customers, orders, by = \"id\")\nfull_data\n\n\n\n\nid\ncity\npostcode\nitems\n\n\n\n1\nPort Ellen\nPA42 7DU\nNA\n\n\n2\nDufftown\nAB55 4DH\n10\n\n\n3\nNA\nNA\n18\n\n\n4\nAberlour\nAB38 7RY\n21\n\n\n4\nAberlour\nAB38 7RY\n23\n\n\n5\nTobermory\nPA75 6NR\n9\n\n\n5\nTobermory\nPA75 6NR\n11\n\n\n6\nNA\nNA\n11\n\n\n6\nNA\nNA\n12\n\n\n7\nNA\nNA\n3",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#filtering-joins",
    "href": "05-join.html#filtering-joins",
    "title": "5  Data Relations",
    "section": "\n5.5 Filtering Joins",
    "text": "5.5 Filtering Joins\nFiltering joins act like the dplyr::filter() function in that they keep and remove rows from the data in one table based on the values in another table. The result of a filtering join will only contain rows from the left table and have the same number or fewer rows than the left table. We’ll learn more about the filter() function in Chapter 7.\n\n5.5.1 semi_join()\n\n\n\nA semi_join returns all rows from the left table where there are matching values in the right table, keeping just columns from the left table.\n\nSemi join the order data to the customer data, matching by the id column.\n\nsemi_data &lt;- semi_join(customers, orders, by = \"id\")\nsemi_data\n\n\n\n\nid\ncity\npostcode\n\n\n\n2\nDufftown\nAB55 4DH\n\n\n3\nNA\nNA\n\n\n4\nAberlour\nAB38 7RY\n\n\n5\nTobermory\nPA75 6NR\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike an inner join, a semi join will never duplicate the rows in the left table if there is more than one matching row in the right table.\n\n\n\n\n\nOrder matters in a semi join.\n\nsemi2_data &lt;- semi_join(orders, customers, by = \"id\")\nsemi2_data\n\n\n\n\nid\nitems\n\n\n\n2\n10\n\n\n3\n18\n\n\n4\n21\n\n\n4\n23\n\n\n5\n9\n\n\n5\n11\n\n\n\n\n\n\n\n5.5.2 anti_join()\n\n\n\nAn anti_join return all rows from the left table where there are not matching values in the right table, keeping just columns from the left table.\n\nAnti join the order data to the customer data, matching by the id column.\n\nanti_data &lt;- anti_join(customers, orders, by = \"id\")\nanti_data\n\n\n\n\nid\ncity\npostcode\n\n\n1\nPort Ellen\nPA42 7DU\n\n\n\n\n\n\n\n\n\nOrder matters in an anti join.\n\nanti2_data &lt;- anti_join(orders, customers, by = \"id\")\nanti2_data\n\n\n\n\nid\nitems\n\n\n\n6\n11\n\n\n6\n12\n\n\n7\n3",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#multiple-joins",
    "href": "05-join.html#multiple-joins",
    "title": "5  Data Relations",
    "section": "\n5.6 Multiple joins",
    "text": "5.6 Multiple joins\nThe ****_join() functions are all two-table verbs, that is, you can only join together two tables at a time. However, you may often need to join together multiple tables. To do so, you simply need to add on additional joins. You can do this by creating an intermediate object or more efficiently by using a pipe.\n\n# create a table of overall customer satisfaction scores\nsatisfaction &lt;- tibble(\n  id = 1:5,\n  satisfaction = c(4, 3, 2, 3, 1)\n)\n\n# perform the initial join\njoin_1 &lt;- left_join(customers, orders, by = \"id\")\n\n# perform the second join on the new object\njoin_2 &lt;- left_join(join_1, satisfaction, \n                    by = \"id\")\n\n\n# more efficient method using the pipe\npipe_join &lt;- customers |&gt;\n  left_join(orders, by = \"id\") |&gt;\n  left_join(satisfaction, by = \"id\")\n\n\n\n\n\n\n\nWarning\n\n\n\nAt every stage of any analysis you should check your output to ensure that what you created is what you intended to create, but this is particularly true of joins. You should be familiar enough with your data through routine checks using functions like glimpse(), str(), and summary() to have a rough idea of what the join should result in. At the very least, you should know whether the joined object should result in more or fewer variables and observations.\nIf you have a multi-line join like in the above piped example, build up the code and check the output at each stage.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#binding-joins",
    "href": "05-join.html#binding-joins",
    "title": "5  Data Relations",
    "section": "\n5.7 Binding Joins",
    "text": "5.7 Binding Joins\nBinding joins bind one table to another by adding their rows or columns together.\n\n5.7.1 bind_rows()\nYou can combine the rows of two tables with bind_rows.\nHere we’ll add customer data for customers 6-9 and bind that to the original customer table.\n\nnew_customers &lt;- tibble(\n  id = 6:9,\n  city = c(\"Falkirk\", \"Ardbeg\", \"Doogal\", \"Kirkwall\"),\n  postcode = c(\"FK1 4RS\", \"PA42 7EA\", \"G81 4SJ\", \"KW15 1SE\")\n)\n\nbindr_data &lt;- bind_rows(customers, new_customers)\nbindr_data\n\n\n\n\nid\ncity\npostcode\n\n\n\n1\nPort Ellen\nPA42 7DU\n\n\n2\nDufftown\nAB55 4DH\n\n\n3\nNA\nNA\n\n\n4\nAberlour\nAB38 7RY\n\n\n5\nTobermory\nPA75 6NR\n\n\n6\nFalkirk\nFK1 4RS\n\n\n7\nArdbeg\nPA42 7EA\n\n\n8\nDoogal\nG81 4SJ\n\n\n9\nKirkwall\nKW15 1SE\n\n\n\n\n\n\nThe columns just have to have the same names, they don’t have to be in the same order. Any columns that differ between the two tables will just have NA values for entries from the other table.\nIf a row is duplicated between the two tables (like id 5 below), the row will also be duplicated in the resulting table. If your tables have the exact same columns, you can use union() (see Section 5.8.2) to avoid duplicates.\n\nnew_customers &lt;- tibble(\n  id = 5:9,\n  postcode = c(\"PA75 6NR\", \"FK1 4RS\", \"PA42 7EA\", \"G81 4SJ\", \"KW15 1SE\"),\n  city = c(\"Tobermory\", \"Falkirk\", \"Ardbeg\", \"Doogal\", \"Kirkwall\"),\n  new = c(1,2,3,4,5)\n)\n\nbindr2_data &lt;- bind_rows(customers, new_customers)\nbindr2_data\n\n\n\n\nid\ncity\npostcode\nnew\n\n\n\n1\nPort Ellen\nPA42 7DU\nNA\n\n\n2\nDufftown\nAB55 4DH\nNA\n\n\n3\nNA\nNA\nNA\n\n\n4\nAberlour\nAB38 7RY\nNA\n\n\n5\nTobermory\nPA75 6NR\nNA\n\n\n5\nTobermory\nPA75 6NR\n1\n\n\n6\nFalkirk\nFK1 4RS\n2\n\n\n7\nArdbeg\nPA42 7EA\n3\n\n\n8\nDoogal\nG81 4SJ\n4\n\n\n9\nKirkwall\nKW15 1SE\n5\n\n\n\n\n\n\n\n5.7.2 bind_cols()\nYou can merge two tables with the same number of rows using bind_cols. This is only useful if the two tables have the same number of rows in the exact same order.\n\nnew_info &lt;- tibble(\n  colour = c(\"red\", \"orange\", \"yellow\", \"green\", \"blue\")\n)\n\nbindc_data &lt;- bind_cols(customers, new_info)\nbindc_data \n\n\n\n\nid\ncity\npostcode\ncolour\n\n\n\n1\nPort Ellen\nPA42 7DU\nred\n\n\n2\nDufftown\nAB55 4DH\norange\n\n\n3\nNA\nNA\nyellow\n\n\n4\nAberlour\nAB38 7RY\ngreen\n\n\n5\nTobermory\nPA75 6NR\nblue\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe only advantage of bind_cols() over a mutating join is when the tables don’t have any IDs to join by and you have to rely solely on their order. Otherwise, you should use a mutating join (all four mutating joins result in the same output when all rows in each table have exactly one match in the other table).\n\n\n\n5.7.3 Importing multiple files\nIf you need to import and bind a whole folder full of files that have the same structure, get a list of all the files you want to combine. It’s easiest if they’re all in the same directory, although you can use a pattern to select the files you want if they have a systematic naming structure.\nFirst, save the two customer tables to CSV files. The dir.create() function makes a folder called “data”. The showWarnings = FALSE argument means that you won’t get a warning if the folder already exists, it just won’t do anything.\n\n# write our data to a new folder for the demo\ndir.create(\"data\", showWarnings = FALSE)\nwrite_csv(x = customers, file = \"data/customers1.csv\")\nwrite_csv(x = new_customers, file = \"data/customers2.csv\")\n\nNext, retrieve a list of all file names in the data folder that contain the string “customers”\n\nfiles &lt;- list.files(\n  path = \"data\", \n  pattern = \"customers\", \n  full.names = TRUE\n)\n\nfiles\n\n[1] \"data/customers1.csv\" \"data/customers2.csv\"\n\n\nNext, we’ll iterate over this list to read in the data from each file. Whilst this won’t be something we cover in detail in the core resources of this course, iteration is an important concept to know about. Iteration is where you perform the same task on multiple different inputs. As a general rule of thumb, if you find yourself copying and pasting the same thing more than twice, there’s a more efficient and less error-prone way to do it, although these functions do typically require a stronger grasp of programming.\nThe purrr package contains functions to help with iteration. purrr::map_df() maps a function to a list and returns a data frame (table) of the results.\n\n\n.x is the list of file paths\n\n.f specifies the function to map to each of those file paths.\nThe resulting object all_files will be a data frame that combines all the files together, similar to if you had imported them separately and then used bind_rows().\n\n\nall_files &lt;- purrr::map_df(.x = files, .f = read_csv)\n\n\n\n\n\n\n\nNote\n\n\n\nIf all of your data files have the exact same structure (i.e., the same number of columns with the same names), you can just use read_csv() and set the file argument to a vector of the file names. However, the pattern above is safer if you might have a file with an extra or missing column.\n\nfiles &lt;- c(\"data/customers1.csv\", \"data/customers1.csv\")\nall_files &lt;- read_csv(files)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#set-operations",
    "href": "05-join.html#set-operations",
    "title": "5  Data Relations",
    "section": "\n5.8 Set Operations",
    "text": "5.8 Set Operations\nSet operations compare two tables and return rows that match (intersect), are in either table (union), or are in one table but not the other (setdiff).\n\n5.8.1 intersect()\ndplyr::intersect() returns all rows in two tables that match exactly. The columns don’t have to be in the same order, but they have to have the same names.\n\nnew_customers &lt;- tibble(\n  id = 5:9,\n  postcode = c(\"PA75 6NR\", \"FK1 4RS\", \"PA42 7EA\", \"G81 4SJ\", \"KW15 1SE\"),\n  city = c(\"Tobermory\", \"Falkirk\", \"Ardbeg\", \"Doogal\", \"Kirkwall\")\n)\n\nintersect_data &lt;- intersect(customers, new_customers)\nintersect_data\n\n\n\n\nid\ncity\npostcode\n\n\n5\nTobermory\nPA75 6NR\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you’ve forgotten to load dplyr or the tidyverse, base R also has a base::intersect() function that doesn’t work like dplyr::intersect() and will give you unexpected output or an error message.\n\nbase::intersect(customers, new_customers)\n\nlist()\n\n\n\n\n\n5.8.2 union()\ndplyr::union() returns all the rows from both tables, removing duplicate rows, unlike bind_rows().\n\nunion_data &lt;- union(customers, new_customers)\nunion_data\n\n\n\n\nid\ncity\npostcode\n\n\n\n1\nPort Ellen\nPA42 7DU\n\n\n2\nDufftown\nAB55 4DH\n\n\n3\nNA\nNA\n\n\n4\nAberlour\nAB38 7RY\n\n\n5\nTobermory\nPA75 6NR\n\n\n6\nFalkirk\nFK1 4RS\n\n\n7\nArdbeg\nPA42 7EA\n\n\n8\nDoogal\nG81 4SJ\n\n\n9\nKirkwall\nKW15 1SE\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you’ve forgotten to load dplyr or the tidyverse, base R also has a base::union() function. You usually won’t get an error message, but the output won’t be what you expect.\n\nbase::union(customers, new_customers)\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n[1] \"Port Ellen\" \"Dufftown\"   NA           \"Aberlour\"   \"Tobermory\" \n\n[[3]]\n[1] \"PA42 7DU\" \"AB55 4DH\" NA         \"AB38 7RY\" \"PA75 6NR\"\n\n[[4]]\n[1] 5 6 7 8 9\n\n[[5]]\n[1] \"PA75 6NR\" \"FK1 4RS\"  \"PA42 7EA\" \"G81 4SJ\"  \"KW15 1SE\"\n\n[[6]]\n[1] \"Tobermory\" \"Falkirk\"   \"Ardbeg\"    \"Doogal\"    \"Kirkwall\" \n\n\n\n\n\n5.8.3 setdiff()\ndplyr::setdiff returns rows that are in the first table, but not in the second table.\n\nsetdiff_data &lt;- setdiff(customers, new_customers)\nsetdiff_data\n\n\n\n\nid\ncity\npostcode\n\n\n\n1\nPort Ellen\nPA42 7DU\n\n\n2\nDufftown\nAB55 4DH\n\n\n3\nNA\nNA\n\n\n4\nAberlour\nAB38 7RY\n\n\n\n\n\n\nOrder matters for setdiff.\n\nsetdiff2_data &lt;- setdiff(new_customers, customers)\nsetdiff2_data\n\n\n\n\nid\npostcode\ncity\n\n\n\n6\nFK1 4RS\nFalkirk\n\n\n7\nPA42 7EA\nArdbeg\n\n\n8\nG81 4SJ\nDoogal\n\n\n9\nKW15 1SE\nKirkwall\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you’ve forgotten to load dplyr or the tidyverse, base R also has a base::setdiff() function. You usually won’t get an error message, but the output might not be what you expect because base::setdiff() expects columns to be in the same order, so id 5 here registers as different between the two tables.\n\nbase::setdiff(customers, new_customers)\n\n$id\n[1] 1 2 3 4 5\n\n$city\n[1] \"Port Ellen\" \"Dufftown\"   NA           \"Aberlour\"   \"Tobermory\" \n\n$postcode\n[1] \"PA42 7DU\" \"AB55 4DH\" NA         \"AB38 7RY\" \"PA75 6NR\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#conflicting-variable-types",
    "href": "05-join.html#conflicting-variable-types",
    "title": "5  Data Relations",
    "section": "\n5.9 Conflicting variable types",
    "text": "5.9 Conflicting variable types\nAs covered in Appendix G, when you import or create data, R will do its best to set each column to an appropriate data type. However, sometimes it gets it wrong or sometimes there’s something in the way the data has been encoded in the original spreadsheet that causes the data type to be different than expected. When joining datasets by common columns, it’s important that not only are the variable names identical, but the data type of those variables is identical.\nLet’s recreate our new_customers dataset but this time, we’ll specify that id is a character variable.\n\nnew_customers2 &lt;- tibble(\n  id = as.character(5:9),\n  postcode = c(\"PA75 6NR\", \"FK1 4RS\", \"PA42 7EA\", \"G81 4SJ\", \"KW15 1SE\"),\n  city = c(\"Tobermory\", \"Falkirk\", \"Ardbeg\", \"Doogal\", \"Kirkwall\")\n)\nstr(new_customers2)\n\ntibble [5 × 3] (S3: tbl_df/tbl/data.frame)\n $ id      : chr [1:5] \"5\" \"6\" \"7\" \"8\" ...\n $ postcode: chr [1:5] \"PA75 6NR\" \"FK1 4RS\" \"PA42 7EA\" \"G81 4SJ\" ...\n $ city    : chr [1:5] \"Tobermory\" \"Falkirk\" \"Ardbeg\" \"Doogal\" ...\n\n\nIf you try to join this dataset to any of the other datasets where id is stored as a numeric variable, it will produce an error.\n\ninner_join(customers, new_customers2)\n\nError in `inner_join()`:\n! Can't join `x$id` with `y$id` due to incompatible types.\nℹ `x$id` is a &lt;integer&gt;.\nℹ `y$id` is a &lt;character&gt;.\n\n\nThe same goes for bind_rows():\n\nbind_rows(customers, new_customers2)\n\nError in `bind_rows()`:\n! Can't combine `..1$id` &lt;integer&gt; and `..2$id` &lt;character&gt;.\n\n\nOne method to change variable types is to use the as.*** functions. If you type as. into a code chunk and hit tab, you will see that there are a huge number of these functions for transforming variables and datasets to different types. Exactly which one you need will depend on the data you have, but a few commonly used ones are:\n\n\nas.numeric() - convert a variable to numeric. Useful for when you have a variable of real numbers that have been encoded as character. Any values that can’t be turned into numbers (e.g., if you have the word “missing” in cells that you have no data for), will be returned as NA.\n\nas.factor() - convert a variable to a factor. You can set the factor levels and labels manually, or use the default order (alphabetical).\n\nas.character() - convert a variable to character data.\n\nas_tibble() and as.data.frame() - convert a list object (not a variable) to a tibble or a data frame (two different table formats). This isn’t actually relevant to what we’re discussing here, but it’s a useful one to be aware of because sometimes you’ll run into issues where you get an error that specifically requests your data is a tibble or data frame type and you can use this function to overwrite your object.\n\nTo use these functions on a variable we can use dplyr::mutate() to overwrite the variable with that variable as the new data type:\n\nnew_customers2 &lt;- new_customers2 |&gt;\n  mutate(id = as.numeric(id))\n\nOnce you’ve done this, the joins will now work:\n\ninner_join(orders, new_customers2)\n\n\n\n\nid\nitems\npostcode\ncity\n\n\n\n5\n9\nPA75 6NR\nTobermory\n\n\n5\n11\nPA75 6NR\nTobermory\n\n\n6\n11\nFK1 4RS\nFalkirk\n\n\n6\n12\nFK1 4RS\nFalkirk\n\n\n7\n3\nPA42 7EA\nArdbeg",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#exercises",
    "href": "05-join.html#exercises",
    "title": "5  Data Relations",
    "section": "\n5.10 Exercises",
    "text": "5.10 Exercises\nThere’s lots of different use cases for the ****_join() functions. These exercises will allow you to practice different joins. If you have any examples of where joins might be helpful in your own work, please post them on Teams, as having many concrete examples can help distinguish between the different joins.\n\n5.10.1 Grade data\nThe University of Glasgow’s Schedule A grading scheme uses a 22-point alphanumeric scale. Each alphanumeric grade (e.g., B2) has an underlying numeric Grade Point (e.g., 16).\nOften when we’re working with student grades they are provided to us in only one of these forms, but we need to be able to go between the two. For example, we need the numeric form in order to be able to calculate descriptive statistics about the mean grade, but we need the alphanumeric form to release to student records.\n\nDownload grade_data.csv, grade_data2.csv and scheduleA.csv into your data folder.\nRead in scheduleA.csv and save it to an object named schedule.\nRead in grade_data1.csv and save it to an object named grades1.\nRead in grade_data2.csv and save it to an object named grades2.\n\n\n\n\nSolution\n\nschedule &lt;- read_csv(\"data/scheduleA.csv\")\ngrades1 &lt;- read_csv(\"data/grade_data1.csv\") \ngrades2 &lt;- read_csv(\"data/grade_data2.csv\")\n\n\n\n5.10.2 Matching the variable types\nAt UofG, all students are given a GUID, a numeric ID number. However, that ID number is also then combined with the first letter from your surname to create your username that is used with your email address. For example, if your ID is 1234567 and your surname is Nordmann, your username would be 1234567n. From a data wrangling perspective this is very annoying because the numeric ID will be stored as numeric data, but the username will be stored as character because of the letter at the end. grades1 has a numeric id whilst grades2 has the additional letter. In order to join these datasets, we need to standardise the variables.\nFirst, remove the letter character from id using the function stringr::str_replace_all(), which replaces text that matches a pattern. Here, we’re using the pattern \"[a-z]\", which matches all lowercase letters a through z, and replacing them with \"\". See the help for ?about_search_regex for more info about how to set patterns (these can get really complex).\n\ngrades1 &lt;- grades1 |&gt;\n  mutate(id = str_replace_all(\n    id, # the variable you want to search\n    pattern = \"[a-z]\", # find all letters a-z\n    replacement = \"\" # replace with nothing\n  ))  \n\nNow, transform the data type of id so that it matches the data type in grades2.\n\n\n\nSolution\n\n# check variable types\nglimpse(grades1)\nglimpse(grades2) \n\ngrades1 &lt;- grades1 |&gt;\n  mutate(id = as.numeric(id))\n\n\n\n5.10.3 Complete records\nIn this example, we want to join the grade data to schedule A so that each student with a grade has both the grade and the grade point. But we also want a complete record of all students on the course, so students with missing grades should still be included in the data.\n\nJoin grades1 and scheduleA and store this table in an object named exam_all.\nDo the same for grades2 and save it in essay_all.\nBoth exam_all and essay_all should have 100 observations of 4 variables.\n\n\n\nHint\n\nYou want to keep all of the data from grade_data1 and grade_data2, but you only want the alphanumeric grades from schedule for the Grade Point values that exist in grades. E.g., if no-one was awarded an F1, your final dataset shouldn’t have that in it.\n\n\n\n\nSolution\n\nexam_all &lt;- left_join(grades1, schedule, by = \"Points\")\nessay_all &lt;- left_join(grades2, schedule, by = \"Points\")\n\n\n\n5.10.4 Missing data\nAlternatively, you may wish to have a dataset that only contains data for students who submitted each assessment and have a grade. First, run summary() on both exam_all and essay_all.\n\nHow many exam grades are missing? \n\nHow many essay grades are missing? \n\n\nNow, create an object exam_grades that joins together grades1 and schedule, but this time the resulting object should only contain data from students who have a grade. Do the same but for grades2 and store it in essay_grades.\nBefore you do this, given what you know about how many data points are missing in each data set:\n\nHow many observations should exam_grades have? \n\nHow many observations should essay_grades have? \n\n\n\n\n\nSolution\n\nexam_grades &lt;- inner_join(grades1, schedule, by = \"Points\")\nessay_grades &lt;- inner_join(grades2, schedule, by = \"Points\")\n\n\n\n\nAlternative solution\n\nIt’s worth noting that in reality you wouldn’t actually go back to the raw data and do another join to get this dataset, you could just remove all the missing response by adding |&gt; drop_na() to exam_all and essay_all. However, for the purposes of teaching joins, we’ll do it this slightly artificial way.\n\nNow, create a dataset completes that joins the grades for students who have a grade for both the essay and the exam.\n\nBecause both exam_grades and essay_grades have the variables Assessment, Points and Grades that are named the same, but have different data, you should amend the suffix so that the resulting variables are named Points_exam and Points_essay etc. You may need to consult the help documentation to see an example to figure this out.\nClean up the file with select() and only keep the variables id, Grade_exam, and Grade_essay\n\n\n\n\n\nSolution\n\ncompletes &lt;- inner_join(exam_grades, essay_grades, \n                        by = \"id\", \n                        suffix = c(\"_exam\", \"_essay\")) |&gt;\n  select(id, Grade_exam, Grade_essay)\n\n\n\nHow many students have a grade for both the exam and the essay? \n\n\nNow create a dataset no_essay that contains students that have a grade for the exam, but not the essay.\n\n\n\nSolution\n\nno_essay &lt;- anti_join(exam_grades, essay_grades, by = \"id\")\n\n\n\nHow many students have a grade for the exam but not the essay? \n\n\nFinally, now make a dataset no_exam that contains students have have a grade for the essay but not the exam\n\n\n\nSolution\n\nno_exam &lt;- anti_join(essay_grades, exam_grades, by = \"id\")\n\n\n\nHow many students have a grade for the exam but not the essay?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#sec-glossary-joines",
    "href": "05-join.html#sec-glossary-joines",
    "title": "5  Data Relations",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\nterm\ndefinition\n\n\n\nbase-r\nThe set of R functions that come with a basic installation of R, before you add external packages.\n\n\nbinding-joins\nJoins that bind one table to another by adding their rows or columns together.\n\n\ncharacter\nA data type representing strings of text.\n\n\nfactor\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\nfiltering-joins\nJoins that act like the dplyr::filter() function in that they remove rows from the data in one table based on the values in another table.\n\n\niteration\nRepeating a process or function\n\n\nmutating-joins\nJoins that act like the dplyr::mutate() function in that they add new columns to one table based on values in another table.\n\n\nnumeric\nA data type representing a real decimal number or integer.\n\n\nset-operations\nFunctions that compare two tables and return rows that match (intersect), are in either table (union), or are in one table but not the other (setdiff).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "05-join.html#sec-resources-joins",
    "href": "05-join.html#sec-resources-joins",
    "title": "5  Data Relations",
    "section": "Further resources",
    "text": "Further resources\n\nData transformation cheatsheet\n\nChapter 19: Joins in R for Data Science\n\n\nChapter 26: Iteration in R for Data Science.\npurrr cheatsheet",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Relations</span>"
    ]
  },
  {
    "objectID": "06-tidy.html",
    "href": "06-tidy.html",
    "title": "6  Data Tidying",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#sec-ilo-tidy",
    "href": "06-tidy.html#sec-ilo-tidy",
    "title": "6  Data Tidying",
    "section": "",
    "text": "Be able to reshape data between long and wide formats\nSeparate, change, reorder, and rename columns\nUse pipes to chain together functions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#functions-tidy",
    "href": "06-tidy.html#functions-tidy",
    "title": "6  Data Tidying",
    "section": "\n6.1 Functions used",
    "text": "6.1 Functions used\n\nbuilt-in (you can always use these without loading any packages)\n\nbase:: library(), c(), list()\n\n\n\ntidyverse (you can use all these with library(tidyverse))\n\nreadr:: read_csv(), type_convert()\n\ndplyr:: filter(), group_by(), summarise(), select(), mutate(), glimpse()\n\ntidyr:: pivot_longer(), pivot_wider(), separate()\n\nggplot2:: ggplot(), aes(), geom_histogram()\n\nstringr:: str_replace_all()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#sec-setup-tidy",
    "href": "06-tidy.html#sec-setup-tidy",
    "title": "6  Data Tidying",
    "section": "Set-up",
    "text": "Set-up\n\nOpen your reprores project\nCreate a new quarto file called 06-tidy.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\n\nlibrary(tidyverse) # for data wrangling\n```\n\nYou’ll also need to make a folder called “data” and download two data files into it: tidy_data.csv and untidy_data.csv.\nDownload the Data tidying cheat sheet.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#data-structures",
    "href": "06-tidy.html#data-structures",
    "title": "6  Data Tidying",
    "section": "\n6.2 Data Structures",
    "text": "6.2 Data Structures\nThe data you work with will likely come in many different formats and structures. Some of these structures may be driven by how the software you use outputs the data, but data structures may also differ because of human intervention or attempts at organisation, some of which may not be particularly helpful.\nData cleaning and tidying will likely be the most time consuming and difficult task you perform. Whilst you can create code recipes for analyses and visualisations, as Hadley Whickham puts it, “every messy dataset is messy in its own way”, which means that you will often have to solve new problems that are specific to the dataset. Additionally, moving between data structures that are intuitive to read by humans and those that are useful for a computer requires a conceptual shift that only comes with practice.\nThis is all a verbose way of saying that what lies ahead in this chapter is unlikely to sink in on the first attempt and you will need to practice with different examples (preferably with data you know well) before you truly feel comfortable with it.\nFirst, some terminology.\nAn observation is all the information about a single “thing” in a single condition, such as at one point in time. These things can be customers, sales, orders, feedback questionnaires, tweets, or really anything. Observations should have a way to identify them, such as a unique ID or a unique combination of values like country and year.\nA variable is one type of information about the observation. For example, if the observation is a sale, the variables you might have about the sale are the sale ID, the customer’s ID, the date of the sale, the price paid, and method of payment.\nA value is the data for one variable for one observation. For example, the value of the date variable from the observation of a sale might be 2021-08-20.\n\nThe following table is data that shows the number of items each customer bought each year.\n\n\n\n\n\ncustomer_id\nyear\nitems\n\n\n\n1\n2018\n2\n\n\n1\n2019\n8\n\n\n1\n2020\n10\n\n\n2\n2018\n1\n\n\n2\n2019\n6\n\n\n2\n2020\n1\n\n\n\n\n\n\n\nWhat is items? \nObservation\nVariable\nValue\n\nHow many observations are there in this dataset? \n\nWhat is 8? \nObservation\nVariable\nValue\n\n\n\n\nExplain these answers\n\n\nThere are three variables, customer_id, year, and items.\nThere are six observations, one for each of two customers for each of three years.\n\n8 is a value because it is a single data point for one variable for one observation.\n\n\n\n\n6.2.1 Untidy data\nFirst, let’s have a look at an example of a messy, or untidy, dataset. Each row has all of the data relating to one customer.\n\nuntidy_data &lt;- read_csv(\"data/untidy_data.csv\", show_col_types = FALSE)\n\n\n\n\nTable 6.1: Untidy table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nitemsprice_2018\nitemsprice_2019\nitemsprice_2020\ntotalprice_2018\ntotalprice_2019\ntotalprice_2020\n\n\n\n1\n2 (3.91)\n8 (4.72)\n10 (5.59)\n7.82\n37.76\n55.90\n\n\n2\n1 (3.91)\n6 (4.72)\n1 (5.59)\n3.91\n28.32\n5.59\n\n\n3\n4 (3.91)\n5 (4.72)\n5 (5.59)\n15.64\n23.60\n27.95\n\n\n4\n10 (3.91)\n1 (4.72)\n3 (5.59)\n39.10\n4.72\n16.77\n\n\n5\n3 (3.91)\n9 (4.72)\n8 (5.59)\n11.73\n42.48\n44.72\n\n\n\n\n\n\n\n\n\n\nThe itemsprice_{year} columns contain two values (number of items and price per item)\nThe totalprice_{year} columns contain the total amount spent by that customer that year, i.e., items * price.\nThere is data for three different years in the dataset.\n\nLet’s say you wanted to calculate the total price per customer over the three years and the total number of items bought per customer. You can’t perform mathematical operations on the itemsprice_{year} columns because they are character data types.\nYou would probably normally use Excel to\n\nsplit itemsprice_2018 column into item_2018 and price_2018 columns\nsplit itemsprice_2019 column into item_2019 and price_2019 columns\nsplit itemsprice_2020 column into item_2018 and price_2020 columns\nadd item_2018 + item_2019 + item_2020 to get the total number of items bought per customer\nadd totalprice_2018 + totalprice_2019 + totalprice_2020 to get the total price per customer\n\n\nThink about how many steps in Excel this would be if there were 10 years in the table, or a different number of years each time you encountered data like this.\n\n\n6.2.2 Tidy data\nThere are three rules for “tidy data, which is data in a format that makes it easier to combine data from different tables, create summary tables, and visualise your data.\n\nEach observation must have its own row\nEach variable must have its own column\nEach value must have its own cell\n\nThis is the tidy version:\n\ntidy_data &lt;- read_csv(\"data/tidy_data.csv\", show_col_types = FALSE)\n\n\n\n\nTable 6.2: Tidy table\n\n\n\n\n\ncustomer_id\nyear\nitems\nprice_per_item\ntotalprice\n\n\n\n1\n2018\n2\n3.91\n7.82\n\n\n1\n2019\n8\n4.72\n37.76\n\n\n1\n2020\n10\n5.59\n55.90\n\n\n2\n2018\n1\n3.91\n3.91\n\n\n2\n2019\n6\n4.72\n28.32\n\n\n2\n2020\n1\n5.59\n5.59\n\n\n3\n2018\n4\n3.91\n15.64\n\n\n3\n2019\n5\n4.72\n23.60\n\n\n3\n2020\n5\n5.59\n27.95\n\n\n4\n2018\n10\n3.91\n39.10\n\n\n4\n2019\n1\n4.72\n4.72\n\n\n4\n2020\n3\n5.59\n16.77\n\n\n5\n2018\n3\n3.91\n11.73\n\n\n5\n2019\n9\n4.72\n42.48\n\n\n5\n2020\n8\n5.59\n44.72\n\n\n\n\n\n\n\n\n\n\nThere are now five variables (columns) because there are five different types of information we have for each observation: the customer id, the year, number of items bought, price per item, and total price.\nEach row is a customer’s orders in a particular year.\nThe number of items (items) and price per item (price_per_item) are in separate columns, so now you can perform mathematical operations on them.\n\nTo calculate the total price per customer over the three years and the total number of items bought per customer in R, you could then:\n\ngroup the table by customer_id\nsum the items column to get the total number of items bought per customer\nsum the totalprice column to get the total price per customer\n\n\ntidy_data |&gt;\n  group_by(customer_id) |&gt;\n  summarise(\n    total_items = sum(items),\n    total_price = sum(totalprice)\n  )\n\n\n\n\ncustomer_id\ntotal_items\ntotal_price\n\n\n\n1\n20\n101.48\n\n\n2\n8\n37.82\n\n\n3\n14\n67.19\n\n\n4\n14\n60.59\n\n\n5\n20\n98.93\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf there were 10 years in the table, or a different number of years each time you encountered data like this, the code for producing the tables and plots above never changes.\n\n\nIf you have control over how the data are recorded, it will make your life easier to record it in a tidy format from the start. However, we don’t always have control, so this class will also teach you how to convert untidy tables into tidy tables.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#sec-reshaping-data",
    "href": "06-tidy.html#sec-reshaping-data",
    "title": "6  Data Tidying",
    "section": "\n6.3 Reshaping Data",
    "text": "6.3 Reshaping Data\nData tables can be in wide format or long format (or a mix of the two). Wide data are where all of the observations about one thing are in the same row, while long data are where each observation is on a separate row. You often need to convert between these formats to do different types of summaries or visualisation. You may have done something similar using pivot tables in Excel.\n\n\n\n\nConverting between wide and long formats using pivot tables in Excel.\n\n\n\nIt can be easier to just consider one type of measurement at a time. untidy_data has two types of measurements, total price and price per item. Let’s look at just the totalprice data first.\nWe can select just the columns we want using the dplyr::select() function. This function’s first argument is the data table you want to select from, then each argument after that is either the name of a column in that table, or new_name = old_name. This is a useful function for changing the column names and order of columns, as well as selecting a subset of columns.\n\n\n\n\n\n\nWarning\n\n\n\nNote that because the names of the columns are numbers, they need to be wrapped in backticks otherwise you’ll get an error like:\nError: unexpected '=' in:\n\"  customer_id, \n  2018 =\"\n\n\n\n# select just the customer ID and 3 total price columns\nwide_totalprice &lt;- select(\n  .data = untidy_data,\n  customer_id, \n  `2018` = totalprice_2018,\n  `2019` = totalprice_2019,\n  `2020` = totalprice_2020\n)\n\n\n\n\nTable 6.3: Wide table\n\n\n\n\n\ncustomer_id\n2018\n2019\n2020\n\n\n\n1\n7.82\n37.76\n55.90\n\n\n2\n3.91\n28.32\n5.59\n\n\n3\n15.64\n23.60\n27.95\n\n\n4\n39.10\n4.72\n16.77\n\n\n5\n11.73\n42.48\n44.72\n\n\n\n\n\n\n\n\n\nThis is in wide format, where each row is a customer, and represents the data from several years. This is a really intuitive way for humans to read a table, but it’s not as easy to process with code.\nThe same data can be represented in a long format by creating a new column that specifies what year the observation is from and a new column that specifies the totalprice of that observation. This is easier to use to make summaries and plots.\n\nlong_totalprice &lt;- pivot_longer(\n  data = wide_totalprice,\n  cols = `2018`:`2020`,\n  names_to = \"year\",\n  values_to = \"totalprice\")\n\n\n\n\nTable 6.4: Long table\n\n\n\n\n\ncustomer_id\nyear\ntotalprice\n\n\n\n1\n2018\n7.82\n\n\n1\n2019\n37.76\n\n\n1\n2020\n55.90\n\n\n2\n2018\n3.91\n\n\n2\n2019\n28.32\n\n\n2\n2020\n5.59\n\n\n3\n2018\n15.64\n\n\n3\n2019\n23.60\n\n\n3\n2020\n27.95\n\n\n4\n2018\n39.10\n\n\n4\n2019\n4.72\n\n\n4\n2020\n16.77\n\n\n5\n2018\n11.73\n\n\n5\n2019\n42.48\n\n\n5\n2020\n44.72\n\n\n\n\n\n\n\n\n\nIt also makes it very easy to use with ggplot(). Run the following plot, and consider how you’d make it with the wide version (it is likely impossible).\n\nggplot(long_totalprice, aes(x = totalprice, fill = year)) +\n  geom_histogram(binwidth = 10, color = \"black\")\n\n\n\nMost plots are easier to make with data in a long format.\n\n\n\n\nCreate a long version of the following table of how many million followers each band has on different social media platforms. You don’t need to use code, just sketch it in a notebook or make a table in a spreadsheet.\n\n\nband\ntwitter\ninstagram\n\n\n\nThe Beatles\n3.8\n3.8\n\n\nThe Rolling Stones\n3.4\n3.1\n\n\nOne Direction\n31.3\n22.8\n\n\n\n\n\nAnswer\n\nYour answer doesn’t need to have the same column headers or be in the same order.\n\n\naccount\nsocial_media\nfollowers\n\n\n\nThe Beatles\ntwitter\n3.8\n\n\nThe Beatles\ninstagram\n3.8\n\n\nThe Rolling Stones\ntwitter\n3.4\n\n\nThe Rolling Stones\ninstagram\n3.1\n\n\nOne Direction\ntwitter\n31.3\n\n\nOne Direction\ninstagram\n322.8\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re a researcher and you’re used to thinking about IVs and DVs, you may find it easier to remember that each IV and DV should have its own column, rather than a column for each level of the IV.\n\n\nThe pivot functions allow you to transform a data table from wide to long or long to wide.\n\n6.3.1 Wide to long\nThe function pivot_longer() converts a wide data table to a longer format by converting the headers from specified columns into the values of new columns, and combining the values of those columns into a new condensed column.\nThis function has several arguments:\n\n\ncols: the columns you want to make long; you can refer to them by their names, like c(`2018`, `2019`, `2020`) or `2018`:`2020` or by their numbers, like c(2, 3, 4) or 2:4\n\n\nnames_to: what you want to call the new columns that the cols column header names will go into\n\nvalues_to: what you want to call the new column that contains the values in the cols\n\n\nWith the pivot functions, it can be easier to show than tell - run the below code and then compare wide_totalprice with long_totalprice and the pivot code and try to map each argument to what has changed.\n\nlong_totalprice &lt;- pivot_longer(\n  data = wide_totalprice, \n  cols = `2018`:`2020`, # columns to make long \n  names_to = \"year\", # new column name for headers\n  values_to = \"totalprice\" # new column name for values\n)\n\n\n6.3.2 Long to wide\nWe can also go from long to wide format using the pivot_wider() function. Instead of returning to the original table with a row for each customer and a column for each year, this new wide table will have a row for each year and a column for each customer. It can be awkward to have numbers for column names, so we use names_prefix to add “C_” before each new column name.\n\n\nid_cols: the column(s) that uniquely identify each new row\n\nnames_from: the column(s) that contain your new column headers\n\nnames_prefix: A prefix to add to the values in the names column\n\nvalues_from: the column that contains the values for the new columns\n\n\nwide_by_customer &lt;- pivot_wider(\n  data = long_totalprice,\n  id_cols = year, # identifying column(s)\n  names_from = customer_id, # the new column names\n  names_prefix = \"C_\", # prefix for new column names\n  values_from = totalprice # the new column values\n)\n\n\n\n\nTable 6.5: Data made wider with pivot_wider()\n\n\n\n\n\nyear\nC_1\nC_2\nC_3\nC_4\nC_5\n\n\n\n2018\n7.82\n3.91\n15.64\n39.10\n11.73\n\n\n2019\n37.76\n28.32\n23.60\n4.72\n42.48\n\n\n2020\n55.90\n5.59\n27.95\n16.77\n44.72",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#sec-multistep",
    "href": "06-tidy.html#sec-multistep",
    "title": "6  Data Tidying",
    "section": "\n6.4 Multi-step tidying",
    "text": "6.4 Multi-step tidying\nYou often need to go from wide, to long, to an intermediate shape in order to get your data into a format that is useful for plotting, where there is a column for each variable that you want to represent with an aesthetic.\nOur full untidy_data table has seven columns: a customer ID, three columns for itemsprice and 3 columns for totalprice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nitemsprice_2018\nitemsprice_2019\nitemsprice_2020\ntotalprice_2018\ntotalprice_2019\ntotalprice_2020\n\n\n\n1\n2 (3.91)\n8 (4.72)\n10 (5.59)\n7.82\n37.76\n55.90\n\n\n2\n1 (3.91)\n6 (4.72)\n1 (5.59)\n3.91\n28.32\n5.59\n\n\n3\n4 (3.91)\n5 (4.72)\n5 (5.59)\n15.64\n23.60\n27.95\n\n\n4\n10 (3.91)\n1 (4.72)\n3 (5.59)\n39.10\n4.72\n16.77\n\n\n5\n3 (3.91)\n9 (4.72)\n8 (5.59)\n11.73\n42.48\n44.72\n\n\n\n\n\n\nWe want to get it into the tidy format below where each row is an observation of one customer per year, with the columns of customer_id, year, item, price_per_item and totalprice. Before trying to reshape any dataset, you should be able to visualise what it will look like. Sketching out your tables on a piece of paper can really help make these transformations make sense.\n\n\n\n\n\ncustomer_id\nyear\nitems\nprice_per_item\ntotalprice\n\n\n\n1\n2018\n2\n3.91\n7.82\n\n\n1\n2019\n8\n4.72\n37.76\n\n\n1\n2020\n10\n5.59\n55.90\n\n\n2\n2018\n1\n3.91\n3.91\n\n\n2\n2019\n6\n4.72\n28.32\n\n\n2\n2020\n1\n5.59\n5.59\n\n\n3\n2018\n4\n3.91\n15.64\n\n\n3\n2019\n5\n4.72\n23.60\n\n\n3\n2020\n5\n5.59\n27.95\n\n\n4\n2018\n10\n3.91\n39.10\n\n\n4\n2019\n1\n4.72\n4.72\n\n\n4\n2020\n3\n5.59\n16.77\n\n\n5\n2018\n3\n3.91\n11.73\n\n\n5\n2019\n9\n4.72\n42.48\n\n\n5\n2020\n8\n5.59\n44.72\n\n\n\n\n\n\n\n6.4.1 One observation per row\nThe original table has observations from each customer over three years. This is too many observations per row, so first we’ll start by making the table long. We need to make 6 rows for each customer, one for each category (item price/total price) and year combination, with columns for the customer ID, year, category, and value.\nBecause we’ll be combining columns with numeric (totalprice) and character (itemsprice) data, we need to make the new value column a character data type using values_transform, since numbers can be represented as characters (like \"3.5\"), but character strings can’t be represented as numbers.\nThe argument names_sep is set to the character string used to join names if names_from is more than one column. Alternatively, you can use the argument names_pattern, which can be more powerful but also a little harder to understand how to set up.\n\nlonger_data &lt;- pivot_longer(\n  data = untidy_data, \n  cols = itemsprice_2018:totalprice_2020, # columns to make long \n  names_to = c(\"category\", \"year\"), # new column names for cols\n  names_sep = \"_\", # how to split cols into new columns\n  # names_pattern = \"(.*)_(.*)\", # alternative to names_sep\n  values_to = \"value\", # new column name for values\n  \n  # make sure new columns are the right data type\n  names_transform = list(year = as.integer),\n  values_transform = list(value = as.character) \n)\n\n\n\n\nTable 6.6: Untidy data converted from wide to long.\n\n\n\n\n\ncustomer_id\ncategory\nyear\nvalue\n\n\n\n1\nitemsprice\n2018\n2 (3.91)\n\n\n1\nitemsprice\n2019\n8 (4.72)\n\n\n1\nitemsprice\n2020\n10 (5.59)\n\n\n1\ntotalprice\n2018\n7.82\n\n\n1\ntotalprice\n2019\n37.76\n\n\n1\ntotalprice\n2020\n55.9\n\n\n2\nitemsprice\n2018\n1 (3.91)\n\n\n2\nitemsprice\n2019\n6 (4.72)\n\n\n2\nitemsprice\n2020\n1 (5.59)\n\n\n2\ntotalprice\n2018\n3.91\n\n\n2\ntotalprice\n2019\n28.32\n\n\n2\ntotalprice\n2020\n5.59\n\n\n3\nitemsprice\n2018\n4 (3.91)\n\n\n3\nitemsprice\n2019\n5 (4.72)\n\n\n3\nitemsprice\n2020\n5 (5.59)\n\n\n3\ntotalprice\n2018\n15.64\n\n\n3\ntotalprice\n2019\n23.6\n\n\n3\ntotalprice\n2020\n27.95\n\n\n4\nitemsprice\n2018\n10 (3.91)\n\n\n4\nitemsprice\n2019\n1 (4.72)\n\n\n4\nitemsprice\n2020\n3 (5.59)\n\n\n4\ntotalprice\n2018\n39.1\n\n\n4\ntotalprice\n2019\n4.72\n\n\n4\ntotalprice\n2020\n16.77\n\n\n5\nitemsprice\n2018\n3 (3.91)\n\n\n5\nitemsprice\n2019\n9 (4.72)\n\n\n5\nitemsprice\n2020\n8 (5.59)\n\n\n5\ntotalprice\n2018\n11.73\n\n\n5\ntotalprice\n2019\n42.48\n\n\n5\ntotalprice\n2020\n44.72\n\n\n\n\n\n\n\n\n\n\n6.4.2 One variable per column\nNow this table is long, but not tidy. The value column contains data from two different variables. We need to make the table wider, but not as wide as before. We want to keep the year column and make new columns called itemsprice and totalprice with the relevant customer’s value for that variable and year.\n\nwider_data &lt;- pivot_wider(\n  data = longer_data,\n  id_cols = c(customer_id, year),\n  names_from = category,\n  values_from = value\n)\n\n\n\n\nTable 6.7: Data converted from long to an intermediate shape.\n\n\n\n\n\ncustomer_id\nyear\nitemsprice\ntotalprice\n\n\n\n1\n2018\n2 (3.91)\n7.82\n\n\n1\n2019\n8 (4.72)\n37.76\n\n\n1\n2020\n10 (5.59)\n55.9\n\n\n2\n2018\n1 (3.91)\n3.91\n\n\n2\n2019\n6 (4.72)\n28.32\n\n\n2\n2020\n1 (5.59)\n5.59\n\n\n3\n2018\n4 (3.91)\n15.64\n\n\n3\n2019\n5 (4.72)\n23.6\n\n\n3\n2020\n5 (5.59)\n27.95\n\n\n4\n2018\n10 (3.91)\n39.1\n\n\n4\n2019\n1 (4.72)\n4.72\n\n\n4\n2020\n3 (5.59)\n16.77\n\n\n5\n2018\n3 (3.91)\n11.73\n\n\n5\n2019\n9 (4.72)\n42.48\n\n\n5\n2020\n8 (5.59)\n44.72\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTechinically, you can skip setting the id_cols argument, because all of the columns apart from the names_from column and the values_from column identify the observation (e.g., each observation is identified by the unique combination of customer_id and year). You only have to set the id_cols argument when this is not the case.\n\n\n\n6.4.3 One value per cell\nThe cells in the itemsprice column actually contain two different values. We need to split it into two columns for the variables items, and price_per_item. You can split a column into parts with the function tidyr::separate(). There is a space between the number of items and the brackets, so we can split it along this space – if you are in charge of how data is stored, ensuring data is entered consistently makes this much easier.\n\nsplit_data &lt;- separate(\n  data = wider_data, \n  col = itemsprice, # the column to split\n  into = c(\"items\", \"price_per_item\"), # the new columns to create\n  sep = \" \", # split col by space\n  remove = TRUE, # whether to remove to old col\n  convert = TRUE # whether to fix the data type of the new columns\n)\n\n\n\n\nTable 6.8: The itemsprice column split into items and price_per_item using separate().\n\n\n\n\n\ncustomer_id\nyear\nitems\nprice_per_item\ntotalprice\n\n\n\n1\n2018\n2\n(3.91)\n7.82\n\n\n1\n2019\n8\n(4.72)\n37.76\n\n\n1\n2020\n10\n(5.59)\n55.9\n\n\n2\n2018\n1\n(3.91)\n3.91\n\n\n2\n2019\n6\n(4.72)\n28.32\n\n\n2\n2020\n1\n(5.59)\n5.59\n\n\n3\n2018\n4\n(3.91)\n15.64\n\n\n3\n2019\n5\n(4.72)\n23.6\n\n\n3\n2020\n5\n(5.59)\n27.95\n\n\n4\n2018\n10\n(3.91)\n39.1\n\n\n4\n2019\n1\n(4.72)\n4.72\n\n\n4\n2020\n3\n(5.59)\n16.77\n\n\n5\n2018\n3\n(3.91)\n11.73\n\n\n5\n2019\n9\n(4.72)\n42.48\n\n\n5\n2020\n8\n(5.59)\n44.72\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the new columns should have a different data type from the old column, set convert = TRUE to automatically fix them. This is common when you have columns that contain multiple numbers, separated by commas or semicolons. These are character types before they are separated, but should be numeric types after so that you can do mathematical operations like sum them.\n\n\n\n6.4.4 Altering data\nThe column price_per_item is still a character column because it has parentheses. There are a few ways to fix this. You can use the dplyr::mutate() function to change a column or add a new one.\nHere, we’ll use stringr::str_replace_all() to replace all of the “(” and “)” with ““.\n\nmutated_data &lt;- mutate(\n  .data = split_data,\n  price_per_item = stringr::str_replace_all(\n    string = price_per_item, \n    pattern = \"[()]\", \n    replacement = \"\"\n  )\n)\n\n\n\n\nTable 6.9: Mutating data to remove the parentheses from price_per_item.\n\n\n\n\n\ncustomer_id\nyear\nitems\nprice_per_item\ntotalprice\n\n\n\n1\n2018\n2\n3.91\n7.82\n\n\n1\n2019\n8\n4.72\n37.76\n\n\n1\n2020\n10\n5.59\n55.9\n\n\n2\n2018\n1\n3.91\n3.91\n\n\n2\n2019\n6\n4.72\n28.32\n\n\n2\n2020\n1\n5.59\n5.59\n\n\n3\n2018\n4\n3.91\n15.64\n\n\n3\n2019\n5\n4.72\n23.6\n\n\n3\n2020\n5\n5.59\n27.95\n\n\n4\n2018\n10\n3.91\n39.1\n\n\n4\n2019\n1\n4.72\n4.72\n\n\n4\n2020\n3\n5.59\n16.77\n\n\n5\n2018\n3\n3.91\n11.73\n\n\n5\n2019\n9\n4.72\n42.48\n\n\n5\n2020\n8\n5.59\n44.72\n\n\n\n\n\n\n\n\n\n\n6.4.5 Fixing data types\nThe price_per_item and totalprice columns are still characters, so you can’t do things like calculate the sum of totalprice.\n\n# check the data types\nglimpse(mutated_data)\n\nRows: 15\nColumns: 5\n$ customer_id    &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5\n$ year           &lt;int&gt; 2018, 2019, 2020, 2018, 2019, 2020, 2018, 2019, 2020, 2…\n$ items          &lt;int&gt; 2, 8, 10, 1, 6, 1, 4, 5, 5, 10, 1, 3, 3, 9, 8\n$ price_per_item &lt;chr&gt; \"3.91\", \"4.72\", \"5.59\", \"3.91\", \"4.72\", \"5.59\", \"3.91\",…\n$ totalprice     &lt;chr&gt; \"7.82\", \"37.76\", \"55.9\", \"3.91\", \"28.32\", \"5.59\", \"15.6…\n\n\nOnce the data are clean and tidy, you can fix all of your column data types in one step using readr::type_convert(). This is good practice when you’ve finished cleaning a data set. If the automatic type detection doesn’t work as expected, this usually means that you still have non-numeric characters in a column where there were only supposed to be numbers. You can also manually set the column types in the same way as for readr::read_csv() (see Appendix G)).\n\ntidy_data &lt;- type_convert(\n  df = mutated_data,\n  trim_ws = TRUE # removes spaces before and after values\n)\n\n# check the data types\nglimpse(tidy_data)\n\nRows: 15\nColumns: 5\n$ customer_id    &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5\n$ year           &lt;int&gt; 2018, 2019, 2020, 2018, 2019, 2020, 2018, 2019, 2020, 2…\n$ items          &lt;int&gt; 2, 8, 10, 1, 6, 1, 4, 5, 5, 10, 1, 3, 3, 9, 8\n$ price_per_item &lt;dbl&gt; 3.91, 4.72, 5.59, 3.91, 4.72, 5.59, 3.91, 4.72, 5.59, 3…\n$ totalprice     &lt;dbl&gt; 7.82, 37.76, 55.90, 3.91, 28.32, 5.59, 15.64, 23.60, 27…",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#sec-pipes",
    "href": "06-tidy.html#sec-pipes",
    "title": "6  Data Tidying",
    "section": "\n6.5 Pipes",
    "text": "6.5 Pipes\n\n\n\nWe’ve already introduced pipes in Section 4.2 but this type of data processing is where they really start to shine, as they can significantly reduce the amount of code you write.\nAs a recap, a pipe takes the result of the previous function and sends it to the next function as its first argument, which means that you do not need to create intermediate objects. Below is all the code we’ve used in this chapter, and in the process we created five objects. This can get very confusing in longer scripts.\n\nuntidy_data &lt;- read_csv(\"data/untidy_data.csv\", \n                        show_col_types = FALSE)\n\nlonger_data &lt;- pivot_longer(\n  data = untidy_data,\n  cols = itemsprice_2018:totalprice_2020,\n  names_to = c(\"category\", \"year\"),\n  names_sep = \"_\", \n  values_to = \"value\", \n  names_transform = list(year = as.integer),\n  values_transform = list(value = as.character) \n) \n\nwider_data &lt;- pivot_wider(\n  data = longer_data,\n  id_cols = c(customer_id, year),\n  names_from = category,\n  values_from = value\n)\n\nsplit_data &lt;- separate(\n  data = wider_data,\n  col = itemsprice,\n  into = c(\"items\", \"price_per_item\"),\n  sep = \" \", \n  remove = TRUE, \n  convert = TRUE\n) \n\nmutated_data &lt;- mutate(\n  .data = split_data,\n  price_per_item = stringr::str_replace_all(\n    string = price_per_item, \n    pattern = \"[()]\", \n    replacement = \"\"\n  )\n) \n\ntidy_data &lt;- type_convert(\n  df = mutated_data,\n  trim_ws = TRUE\n)\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can give each object the same name and keep replacing the old data object with the new one at each step. This will keep your environment clean, but it makes debugging code much harder.\n\n\nFor longer series of steps like the one above, using pipes can eliminate many intermediate objects. This also makes it easier to add an intermediate step to your process without having to think of a new table name and edit the table input to the next step (which is really easy to accidentally miss).\n\ntidy_data &lt;- read_csv(file = \"data/untidy_data.csv\",\n                      show_col_types = FALSE) |&gt;\n  pivot_longer(\n    cols = itemsprice_2018:totalprice_2020,\n    names_to = c(\"category\", \"year\"),\n    names_sep = \"_\", \n    values_to = \"value\", \n    names_transform = list(year = as.integer),\n    values_transform = list(value = as.character) \n  ) |&gt;\n  pivot_wider(\n    id_cols = c(customer_id, year),\n    names_from = category,\n    values_from = value\n  ) |&gt;\n  separate(\n    col = itemsprice,\n    into = c(\"items\", \"price_per_item\"),\n    sep = \" \", \n    remove = TRUE, \n    convert = TRUE\n  ) |&gt;\n  mutate(\n    price_per_item = stringr::str_replace_all(\n      string = price_per_item, \n      pattern = \"[()]\", \n      replacement = \"\"\n    )\n  ) |&gt;\n  type_convert(\n    trim_ws = TRUE\n  )\n\nYou can read the code above like this:\n\n\nRead the data with read_csv()\n\n\nfile: from the file at r path(“data/untidy_data.csv”)`,\n\nshow_col_types: do not show the colukmn types message; and then\n\n\n\n\nReshape the data longer with pivot_longer()\n\n\ncols: take the columns from itemsprice_2018 to totalprice_2020,\n\nnames_to: create new columns called “category” and “year” from the cols header names,\n\nnames_sep: separate the column names using “_”\n\nvalues_to: create a new column called “value” from the cols values,\n\nnames_transform: transform the year column to integers,\n\nvalues_transform: transform the value column to characters; and then\n\n\n\n\nReshape the data wider with pivot_wider()\n\n\nid_cols: each row should be an observation of a unique customer_id and year,\n\nnames_from: get the new column names from the values in the category column,\n\nvalues_from: get the new column values from the values in the value column; and then\n\n\n\n\nSplit multiple values in the same column with separate()\n\n\ncol: separate the column itemsprice,\n\ninto: into new columns called “items” and “price_per_item”,\n\nsep: separate the values at each ” “,\n\nremove: do remove the old column,\n\nconvert: do convert the new columns into the right data types; and then\n\n\n\n\nChange a column with mutate()\n\n\nprice_per_item: replace the existing column price_per_item with the result of a search and replace with str_replace_all():\n\n\nstring: the strings to modify come from the price_per_item columns,\n\npattern: search for left or right parentheses,\n\nreplacement: replace them with ““; and then,\n\n\n\n\n\nFix data types with type_convert()\n\n\ntrim_ws: remove spaces, tabs, and line breaks from the start and end of each value\n\n\n\nDon’t feel like you always need to get all of your data wrangling code into a single pipeline. You should make intermediate objects whenever you need to break up your code because it’s getting too complicated or if you need to debug something.\n\n\n\n\n\n\nNote\n\n\n\nYou can debug a pipe by highlighting from the beginning to just before the pipe you want to stop at. Try this by highlighting from data &lt;- to the end of the separate function and typing command-enter (mac) or control-enter (PC). What does data look like now?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#exercises",
    "href": "06-tidy.html#exercises",
    "title": "6  Data Tidying",
    "section": "\n6.6 Exercises",
    "text": "6.6 Exercises\nLet’s try a couple of examples.\n\nSave your current script, close it, and open a new quarto script named “patient-survey.qmd”.\nDownload a copy of wide_exercise-1.csv and wide_exercise-2.csv into your data folder.\nIn the set-up code chunk, load the tidyverse then load the two data files in using read_csv() and name the objects wide1 and wide2\n\n\n\n\n\nSolution\n\nlibrary(tidyverse)\nwide1 &lt;- read_csv(\"data/wide_exercise-1.csv\")\nwide2 &lt;- read_csv(\"data/wide_exercise-2.csv\")\n\n\nThe two datasets represent simulated data from a patient satisfaction survey. We’ll do them one at a time, as they differ in complexity.\n\n6.6.1 Survey 1\nwide1 has data from 50 patients who were asked five questions about their most recent experience at a health centre. The results from this questionnaire are typically reported as a single overall satisfaction score, which is calculated by taking the mean of the five responses. Additionally, the survey also records whether the patient was attending the clinic for the first time, or as a repeat patient.\n\nUse your method of choice to look at the dataset and familiarise yourself with its structure and data.\n\nAs noted, it’s important to think through what your tidied data should look like. Often, the problem with data wrangling in R isn’t actually the code, it’s a lack of understanding of the data that’s being worked on.\n\nHow many variables should the long-form version of wide have? \n\nHow many observations should the long-form version of wide1 have? \n\n\n\n\nExplain these answers\n\n\nThere should be four variables, as there are 4 types of data: patient id, whether they are a repeat patient, the question they were asked, and their response.\nThere will be 250 observations or rows of data because each patient will have 5 rows of data (one per question) and there are 50 patients (50 * 5 = 250).\n\n\n\n6.6.2 Tidy 1\nTransform wide1 to long-form using pivot_longer() and store it in an object named tidy1\n\n\n\nSolution\n\ntidy1 &lt;- wide1 |&gt;\n  pivot_longer(cols = q1:q5,\n               names_to = \"question\", \n               values_to = \"response\")\n\n\n\n6.6.3 Survey 2\nwide2 also has data from 50 patients, however, there are now two measures included in the questionnaire. There are still five questions that relate to satisfaction, but there are also five questions that relate to whether the patient would recommend the medical practice to a friend. Both measures are typically reported by calculating an overall mean for each of the five items.\n\nUse your method of choice to look at the dataset and familiarise yourself with its structure and data.\n\nThis is not as simple as the first exercise because there’s actually two potential ways you might tidy this data, depending on what you want to do with it and how you conceptualise the two different measurements. It’s important to recognise that many of your coding problems will not have just one solution.\n\n6.6.3.1 Tidy 2a\nFor the first option, we’re going to treat the “satisfaction” and “recommendation” measurements as two categories of the same variable. This will be a fully long-form data set with five variables id, repeat_patient, question (the question number), category (whether it’s sat or rec), and response (the numerical rating).\n\nHow many observations should the fully long-form version of wide2 have? \n\n\n\n\nExplain this answer\n\nThere will be 500 rows of data because each participant will have 10 rows: 5 for the satisfaction questions and five for the recommendation questions.\n\nTransform wide2 to full long-form using pivot_longer() and store it in an object named tidy2a.\nThis exercise requires multiple steps and you may need to look at the help documentation.\n\n\nHint 1\n\ndata |&gt; pivot_longer() |&gt; separate()\n\n\n\nHint 2\n\ninto  = c(\"col1\", \"col2\")\n\n\n\n\nSolution\n\ntidy2a &lt;- wide2 |&gt;\n  pivot_longer(cols = q1_sat:q5_rec,\n               names_to = \"question\", \n               values_to = \"response\") |&gt;\n  separate(col = \"question\", into = c(\"question\", \"category\"))\n\n\n\n\n\nAlternative solution\n\n# combine pivot_longer and separate by setting two values for names_to\n# must include names_sep to determine how to separate the column names\ntidy2a &lt;- wide2 |&gt;\n  pivot_longer(cols = q1_sat:q5_rec,\n               names_to = c(\"question\", \"category\"), \n               names_sep = \"_\",\n               values_to = \"response\")\n\n\n\n6.6.3.2 Tidy 2b\nThe second option is to treat the satisfaction and recommendation scores as two distinct variables. This only makes sense if the satisfaction and recommendation scores for each question number are related to each other (e.g., q1 is about the same thing for both questions), making them part of the same observation.\nThis version should also have five variables, but it won’t be fully long-form, it’ll be a slight mix of the two that we’re going to call “semi-long”. The variables in the semi-long version will be id, repeat, question (the question number), sat (the response for the satisfaction question), and rec (the response for the recommendation question).\n\nHow many observations should the semi-long version of wide2 have? \n\n\n\n\nExplain this answer\n\nThere will be 250 rows of data because, just like tidy1, each participant will have 5 rows, one for each of the five questions. The different responses to the satisfaction and recommendation questions are in different variables.\n\nThis also takes multiple steps.\n\n\nHint 1\n\nYou can reuse the code from tidy2a, you just need to add on an extra line that makes the data slightly wider.\n\n\n\nHint 2\n\ndata |&gt; pivot_longer() |&gt; separate() |&gt; pivot_wider()\n\n\n\n\nSolution\n\ntidy2b &lt;- wide2 |&gt;\n  pivot_longer(cols = q1_sat:q5_rec,\n               names_to = \"question\", \n               values_to = \"response\") |&gt;\n  separate(col = \"question\", into = c(\"question\", \"category\")) |&gt;\n  pivot_wider(names_from = \"category\", values_from = \"response\")\n\n\n\n6.6.4 Analysis and visualisation\nUsing group_by() and summarise(), calculate the mean score for each participant for both satisfaction and recommendation. Do this for both versions of the dataset so that you can see how the structure of the dataset changes the approach you need to take.\n\n\n\nSolution\n\ntidy2a |&gt;\n  group_by(id, category) |&gt;\n  summarise(mean = mean(response),\n            .groups = \"drop\")\n\ntidy2b |&gt;\n  group_by(id) |&gt;\n  summarise(mean_satisfaction = mean(sat),\n            mean_rec = mean(rec))\n\n\nReplicate the following:\n\n6.6.4.1 Plot 1\nScatterplot showing the relationship between satisfaction and recommendation scores, by whether the patient is a repeat patient.\n\n\nHint\n\ngeom_jitter()\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nggplot(tidy2b, aes(x = sat, y = rec, colour = repeat_patient)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Satisfaction score\", y = \"Recommendation score\", title = \"Satisfaction and recommendation scores\") +\n  theme_classic()\n\n\n\n6.6.4.2 Plot 2\nBoxplots showing satisfaction and recommends scores for new and repeat patients separately.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nggplot(tidy2a, aes(x = repeat_patient, y = response, fill = repeat_patient)) +\n  geom_boxplot(show.legend = FALSE) +\n  facet_wrap(~category)+\n  theme_bw() +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n6.6.4.3 Plot 3\nHistogram showing the distribution of all responses, across questions and categories.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\nggplot(tidy2a, aes(x = response)) +\n  geom_histogram(binwidth = 1, colour = \"black\", fill = \"Grey\") +\n  labs(x = \"Responses across all questions and categories\") +\n  theme_bw()\n\n\nIf your head hurts a bit at this point, rest assured it’s absolutely normal. As we said at the start, reshaping and tidying data is a conceptual leap and there’s no shortcut to the fact it just takes a bit of time and practice with different datasets - you will get there eventually!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#sec-glossary-tidy",
    "href": "06-tidy.html#sec-glossary-tidy",
    "title": "6  Data Tidying",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\nterm\ndefinition\n\n\n\ncharacter\nA data type representing strings of text.\n\n\ndata-type\nThe kind of data represented by an object.\n\n\nlong\nA data format where each observation is on a separate row\n\n\nobservation\nAll of the data about a single trial or question.\n\n\ntidy-data\nA format for data that maps the meaning onto the structure.\n\n\nvalue\nA single number or piece of data.\n\n\nvariable\n(coding): A word that identifies and stores the value of some data for later use; (stats): An attribute or characteristic of an observation that you can measure, count, or describe\n\n\nwide\nA data format where all of the observations about one subject are in the same row",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "06-tidy.html#sec-resources-tidy",
    "href": "06-tidy.html#sec-resources-tidy",
    "title": "6  Data Tidying",
    "section": "Further resources",
    "text": "Further resources\n\nData tidying cheat sheet\nTidy Data\n\nChapter 5: Tidy Data) in R for Data Science\n\n\nChapter 4.3: Pipes in R for Data Science",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Tidying</span>"
    ]
  },
  {
    "objectID": "07-wrangle.html",
    "href": "07-wrangle.html",
    "title": "7  Data Wrangling",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07-wrangle.html#sec-ilo-wrangle",
    "href": "07-wrangle.html#sec-ilo-wrangle",
    "title": "7  Data Wrangling",
    "section": "",
    "text": "Be able to select and filter data for relevance\nBe able to create new columns and edit existing ones\nBe able to handle missing data",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07-wrangle.html#sec-functions-wrangle",
    "href": "07-wrangle.html#sec-functions-wrangle",
    "title": "7  Data Wrangling",
    "section": "Functions used",
    "text": "Functions used\n\nbuilt-in (you can always use these without loading any packages)\n\nbase:: round(), mean(), min(), max(), `sum(), as.character(), as.numeric()\n\nutils:: head(), packageVersion()\n\n\n\ntidyverse (you can use all these with library(tidyverse))\n\nreadr:: readr::read_csv()\n\ndplyr:: dplyr::filter(), dplyr::arrange(), dplyr::mutate(), dplyr::summarise(), dplyr::group_by(), dplyr::ungroup(), dplyr::case_when(), dplyr::na_if()\n\ntidyr:: tidyr::pivot_longer(), tidyr::pivot_wider()\n\ntibble:: tibble::tibble()\n\n\n\nother (you need to load each package to use these)\n\njanitor:: janitor::round_half_up()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07-wrangle.html#sec-setup-wrangle",
    "href": "07-wrangle.html#sec-setup-wrangle",
    "title": "7  Data Wrangling",
    "section": "Set-up",
    "text": "Set-up\n\nOpen your reprores project\nCreate a new quarto file called 03-dataviz.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\nlibrary(tidyverse)   # data wrangling functions\nlibrary(janitor)     # clean data table names\n```\n\n\n\n\n\n\n\nWarning\n\n\n\nAt this point it’s very likely you will have the most recent version of the tidyverse packages but just in case, be aware that the function case_when() that we will use in this chapter was updated in to introduce new arguments and ways of handling NA. The code we provide in this chapter will only work if you have v1.1.0 or above of dplyr installed. To check run packageVersion(\"dplyr\") and if it’s below 1.1.0, install it again to update.\n\n\nYou’ll need to make a folder called “data” and download a data file into it: budget.csv.\nDownload the Data transformation cheat sheet.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07-wrangle.html#sec-wrangling-functions",
    "href": "07-wrangle.html#sec-wrangling-functions",
    "title": "7  Data Wrangling",
    "section": "\n7.1 Wrangling functions",
    "text": "7.1 Wrangling functions\nData wrangling refers to the process of cleaning, transforming, and restructuring your data to get it into the format you need for analysis and it’s something you will spend an awful lot of time doing. Most data wrangling involves the reshaping functions you learned in Chapter 6 and six functions from the dplyr package that is loaded as part of the tidyverse: select, filter, arrange, mutate, summarise, and group_by. You’ll remember the last two from Chapter 4, so we’ll only cover them briefly.\nIt’s worth highlighting that in this chapter we’re going to cover these common functions and common uses of said functions. However, dplyr has a huge number of additional wrangling functions and each function has many different arguments. Essentially, if you think you should be able to wrangle your data in a particular way that we haven’t explicitly shown you, you almost certainly can, it might just take a bit of Googling to find out how.\nWe’ll use a small example table with the sales, expenses, and satisfaction for two years from four regions over two products. After you load the data, use glimpse(budget) or View(budget) to get familiar with the data.\n\nbudget &lt;- read_csv(\"data/budget.csv\", show_col_types = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\nproduct\nsales_2019\nsales_2020\nexpenses_2019\nexpenses_2020\nsatisfaction_2019\nsatisfaction_2020\n\n\n\nNorth\nwidgets\n2129\n-517\n822\n-897\nhigh\nvery high\n\n\nNorth\ngadgets\n723\n77\n1037\n1115\nvery high\nvery high\n\n\nSouth\nwidgets\n1123\n-1450\n1004\n672\nhigh\nneutral\n\n\nSouth\ngadgets\n2022\n-945\n-610\n200\nlow\nlow\n\n\nEast\nwidgets\n-728\n-51\n-801\n-342\nvery low\nvery low\n\n\nEast\ngadgets\n-423\n-354\n94\n2036\nneutral\nhigh\n\n\nWest\nwidgets\n633\n790\n783\n-315\nneutral\nneutral\n\n\nWest\ngadgets\n1204\n426\n433\n-136\nlow\nlow\n\n\n\n\n\n\n\n7.1.1 Select\nYou can select a subset of the columns (variables) in a table to make it easier to view or to prepare a table for display. You can also select columns in a new order.\n\n7.1.1.1 By name or index\nYou can select columns by name or number (which is sometimes referred to as the column index). Selecting by number can be useful when the column names are long or complicated.\n\n# select single column by name\nproduct_dat &lt;- budget |&gt; select(product) \n\n# select single column by number\nproduct_dat &lt;- budget |&gt; select(2) \n\nYou can select each column individually, separated by commas (e.g., region, sales_2019) but you can also select all columns from one to another by separating them with a colon (e.g., sales_2019:expenses_2020).\nThe colon notation can be much faster because you don’t need to type out each individual variable name, but requires you know what order your columns are in and don’t expect them to change. Always check the output to make sure you have selected what you intended.\n\n# select columns individually\nsales2019 &lt;- budget |&gt; select(region, product, sales_2019)\n\n# select columns with colon\nsales2019 &lt;- budget |&gt; select(region:sales_2019)\n\nYou can rename columns at the same time as selecting them by setting new_name = old_col.\n\nregions &lt;- budget |&gt; select(`Sales Region` = 1, 3:6)\n\nhead(regions, 2)\n\n\n\n\nSales Region\nsales_2019\nsales_2020\nexpenses_2019\nexpenses_2020\n\n\n\nNorth\n2129\n-517\n822\n-897\n\n\nNorth\n723\n77\n1037\n1115\n\n\n\n\n\n\n\n7.1.1.2 Un-selecting columns\nYou can select columns either by telling R which ones you want to keep as in the previous examples, or by specifying which ones you want to exclude by using a minus symbol to un-select columns. You can also use the colon notation to de-select columns, but to do so you need to put parentheses around the span first, e.g., -(sales_2019:expenses_2020), not -sales_2019:expenses_2020.\n\n# de-select individual columns\nsales &lt;- budget |&gt; select(-expenses_2019, -expenses_2020)\n\n# de-select a range of columns\nsales &lt;- budget |&gt; select(-(expenses_2019:expenses_2020))\n\n\n7.1.1.3 Select helpers\nFinally, you can select columns based on criteria about the column names.\n\n\n\n\n\n\nfunction\ndefinition\n\n\n\nstarts_with()\nselect columns that start with a character string\n\n\nends_with()\nselect columns that end with a character string\n\n\ncontains()\nselect columns that contain a character string\n\n\nnum_range()\nselect columns with a name that matches the pattern prefix\n\n\n\n\n\nWhat are the resulting columns for these four examples?\n\n\nbudget |&gt; select(contains(\"_\"))\n\nsales_2019, sales_2020sales_2020, expenses_2020, satisfaction_2020sales_2019, sales_2020, expenses_2019, expenses_2020, satisfaction_2019, satisfaction_2020expenses_2019, expenses_2020\n\n\n\nbudget |&gt; select(num_range(\"expenses_\", 2019:2020))\n\nsales_2019, sales_2020sales_2020, expenses_2020, satisfaction_2020sales_2019, sales_2020, expenses_2019, expenses_2020, satisfaction_2019, satisfaction_2020expenses_2019, expenses_2020\n\n\n\nbudget |&gt; select(starts_with(\"sales\"))\n\nsales_2019, sales_2020sales_2020, expenses_2020, satisfaction_2020sales_2019, sales_2020, expenses_2019, expenses_2020, satisfaction_2019, satisfaction_2020expenses_2019, expenses_2020\n\n\n\nbudget |&gt; select(ends_with(\"2020\"))\n\nsales_2019, sales_2020sales_2020, expenses_2020, satisfaction_2020sales_2019, sales_2020, expenses_2019, expenses_2020, satisfaction_2019, satisfaction_2020expenses_2019, expenses_2020\n\n\n\n\n\n7.1.2 Filter\nWhilst select() chooses the columns you want to retain, filter() chooses the rows to retain by matching row or column criteria.\nYou can filter by a single criterion. This criterion can be rows where a certain column’s value matches a character value (e.g., “North”) or a number (e.g., 9003). It can also be the result of a logical equation (e.g., keep all rows with a specific column value larger than a certain value). The criterion is checked for each row, and if the result is FALSE, the row is removed. You can reverse equations by specifying != where ! means “not”.\n\n# select all rows where region equals North\nbudget |&gt; filter(region == \"North\")\n\n# select all rows where expenses_2020 were exactly equal to 200\nbudget |&gt; filter(expenses_2020 == 200)\n\n# select all rows where sales_2019 was more than 100\nbudget |&gt; filter(sales_2019 &gt; 100)\n\n# everything but the North\nbudget |&gt; filter(region != \"North\")\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember to use == and not = to check if two things are equivalent. A single = assigns the right-hand value to the left-hand variable (much like the &lt;- operator).\n\n\n\nWhich IDs are kept from the table below?\n\n\n\n\n\nid\ngrade\nscore\n\n\n\n1\nA\n95\n\n\n2\nA\n91\n\n\n3\nC\n76\n\n\n4\nB\n84\n\n\n\n\n\n\n\n\ndemo |&gt; filter(score &lt; 80) \n1, 2\n2\n3\n3, 4\n\n\ndemo |&gt; filter(grade == \"A\") \n1, 2\n2\n3\n3, 4\n\n\ndemo |&gt; filter(grade != \"A\") \n1, 2\n2\n3\n3, 4\n\n\ndemo |&gt; filter(score == 91) \n1, 2\n2\n3\n3, 4\n\n\n\nYou can also select on multiple criteria by separating them by commas (rows will be kept if they match all criteria). Additionally, you can use & (“and”) and | (“or”) to create complex criteria.\n\n# regions and products with profit in both 2019 and 2020\nprofit_both &lt;- budget |&gt; \n  filter(\n    sales_2019 &gt; expenses_2019,\n    sales_2020 &gt; expenses_2020\n  )\n\n# the same as above, using & instead of a comma\nprofit_both &lt;- budget |&gt; \n  filter(\n    sales_2019 &gt; expenses_2019 &\n    sales_2020 &gt; expenses_2020\n  )\n\n# regions and products with profit in 2019 or 2020\nprofit_either &lt;- budget |&gt; \n  filter(\n    sales_2019 &gt; expenses_2019 |\n    sales_2020 &gt; expenses_2020\n  )\n\n# 2020 profit greater than 1000\nprofit_1000 &lt;- budget |&gt;\n  filter(sales_2020 - expenses_2020 &gt; 1000)\n\nIf you want the filter to retain multiple specific values in the same variable, the “match operator” (%in%) should be used rather than | (or). The ! can also be used in combination here, but it is placed before the variable name.\n\n# retain any rows where region is north or south, and where product equals widget\nbudget |&gt;\n  filter(region %in% c(\"North\", \"South\"),\n         product == \"widgets\")\n\n# retain any rows where the region is not east or west, and where the product does not equal gadgets\nbudget |&gt;\n  filter(!region %in% c(\"East\", \"West\"),\n         product != \"gadgets\")\n\n\n\n\n\n\n\n\nOperator\nName\nis TRUE if and only if\n\n\n\nA &lt; B\nless than\nA is less than B\n\n\nA &lt;= B\nless than or equal\nA is less than or equal to B\n\n\nA &gt; B\ngreater than\nA is greater than B\n\n\nA &gt;= B\ngreater than or equal\nA is greater than or equal to B\n\n\nA == B\nequivalence\nA exactly equals B\n\n\nA != B\nnot equal\nA does not exactly equal B\n\n\nA %in% B\nin\nA is an element of vector B\n\n\n\nFinally, you can also pass many other functions to filter. For example, the package stringr that is loaded as part of the tidyverse contains many different functions for working with strings (character data). For example, you you use str_detect() to only retain rows where the customer satisfaction rating includes the word “high”\n\nbudget |&gt;\n  filter(str_detect(satisfaction_2019, \"high\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\nproduct\nsales_2019\nsales_2020\nexpenses_2019\nexpenses_2020\nsatisfaction_2019\nsatisfaction_2020\n\n\n\nNorth\nwidgets\n2129\n-517\n822\n-897\nhigh\nvery high\n\n\nNorth\ngadgets\n723\n77\n1037\n1115\nvery high\nvery high\n\n\nSouth\nwidgets\n1123\n-1450\n1004\n672\nhigh\nneutral\n\n\n\n\n\n\nNote that str_detect() is case sensitive so it would not return values of “High” or “HIGH”. You can use the function tolower() or toupper() to convert a string to lowercase or uppercase before you search for substring if you need case-insensitive matching.\n\n\n\n\n\n\nWarning\n\n\n\nfilter() is incredibly powerful and can allow you to select very specific subsets of data. But, it is also quite dangerous because when you start combining multiple criteria and operators, it’s very easy to accidentally specify something slightly different than what you intended. Always check your output. If you have a small dataset, then you can eyeball it to see if it looks right. With a larger dataset, you may wish to compute summary statistics or count the number of groups/observations in each variable to verify your filter is correct. There is no level of expertise in coding that can substitute knowing and checking your data.\n\n\n\n7.1.3 Arrange\nYou can sort your dataset using arrange(). You will find yourself needing to sort data in R much less than you do in Excel, since you don’t need to have rows next to each other in order to, for example, calculate group means. But arrange() can be useful when preparing data for display in tables. arrange() works on character data where it will sort alphabetically, as well as numeric data where the default is ascending order (smallest to largest). Reverse the order using desc().\n\n# arranging the table \n# first by product in alphabetical order\n# then by \"region\" in reverse alphabetical order\nbudget |&gt;\n  arrange(product, desc(region))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\nproduct\nsales_2019\nsales_2020\nexpenses_2019\nexpenses_2020\nsatisfaction_2019\nsatisfaction_2020\n\n\n\nWest\ngadgets\n1204\n426\n433\n-136\nlow\nlow\n\n\nSouth\ngadgets\n2022\n-945\n-610\n200\nlow\nlow\n\n\nNorth\ngadgets\n723\n77\n1037\n1115\nvery high\nvery high\n\n\nEast\ngadgets\n-423\n-354\n94\n2036\nneutral\nhigh\n\n\nWest\nwidgets\n633\n790\n783\n-315\nneutral\nneutral\n\n\nSouth\nwidgets\n1123\n-1450\n1004\n672\nhigh\nneutral\n\n\nNorth\nwidgets\n2129\n-517\n822\n-897\nhigh\nvery high\n\n\nEast\nwidgets\n-728\n-51\n-801\n-342\nvery low\nvery low\n\n\n\n\n\n\n\nIf you want to sort character data/categories in a specific order, turn the column into a factor and set the levels in the desired order.\n\nbudget |&gt;\n  mutate(region = factor(region, levels = c(\"North\", \"South\", \"East\", \"West\"))) |&gt;\n  filter(product == \"gadgets\") |&gt;\n  arrange(region)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\nproduct\nsales_2019\nsales_2020\nexpenses_2019\nexpenses_2020\nsatisfaction_2019\nsatisfaction_2020\n\n\n\nNorth\ngadgets\n723\n77\n1037\n1115\nvery high\nvery high\n\n\nSouth\ngadgets\n2022\n-945\n-610\n200\nlow\nlow\n\n\nEast\ngadgets\n-423\n-354\n94\n2036\nneutral\nhigh\n\n\nWest\ngadgets\n1204\n426\n433\n-136\nlow\nlow\n\n\n\n\n\n\n\n\n7.1.4 Mutate\nThe function mutate() allows you to add new columns or change existing ones by overwriting them by using the syntax new_column = operation. You can add more than one column in the same mutate function by separating the columns with a comma. Once you make a new column, you can use it in further column definitions. For example, the creation of profit below uses the column expenses, which is created above it.\n\nbudget2 &lt;- budget |&gt;\n  mutate(\n    sales = sales_2019 + sales_2020,\n    expenses = expenses_2019 + expenses_2020,\n    profit = sales - expenses,\n    region = paste(region, \"Office\")\n  )\n\nmutate() can also be used in conjunction with other functions and Boolean operators. For example, we can add another column to budget2 that states whether a profit was returned that year or overwrite our product variable as a factor. Just like when we used Boolean expressions with filter, it will evaluate the equation and return TRUE or FALSE depending on whether the observation meets the criteria.\n\nbudget2 &lt;- budget2 |&gt;\n  mutate(profit_category = profit &gt; 0,\n         product = as.factor(product))\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can overwrite a column by giving a new column the same name as the old column (see region or product) above. Make sure that you mean to do this and that you aren’t trying to use the old column value after you redefine it.\n\n\nYou can also use case_when() to specify what values to return, rather than defaulting to TRUE or FALSE:\n\nbudget3 &lt;- budget2 |&gt;\n  mutate(profit_category = case_when(profit &gt; 0 ~ \"PROFIT\",\n                                     profit &lt; 0 ~ \"NO PROFIT\"))\n\nUse it to recode values:\n\n# create a column where people get a bonus if customer satisfaction was overall high or very high\n\nbonus &lt;- budget3 |&gt;\n  mutate(bonus_2019 = case_when(satisfaction_2019 %in% c(\"very low\", \"low\", \"neutral\") ~ \"no bonus\",\n                                satisfaction_2019 %in% c(\"high\", \"very high\") ~ \"bonus\"))\n\nAnd combine different criteria:\n\n# new management takes over - people only get a bonus if customer satisfaction was overall high or very high AND if a profit was returned\n\nbonus2 &lt;- budget3 |&gt;\n  mutate(bonus_2020 = case_when(satisfaction_2020 == \"high\" & \n                                  profit_category == \"PROFIT\" ~ \"bonus\",\n                                satisfaction_2020 == \"very high\" & \n                                  profit_category == \"PROFIT\" ~ \"bonus\",\n                                .default = \"No bonus\")) # set all other values to \"no bonus\"\n\n\n\n\n\n\n\nTip\n\n\n\nBe mindful that .default uses = whilst the others use ~. Emily has lost quite a lot of her time and sanity to not realising this.\n\n\nJust like filter(), mutate() is incredibly powerful and the scope of what you can create is far beyond what we can cover in this book.\n\n7.1.5 Summarise\nYou were introduced to the summarise() function in Section 4.4. This applies summary functions to an entire table (or groups, as you’ll see in the next section).\nLet’s say we want to determine the mean sales and expenses, plus the minimum and maximum profit, for any region, product and year. First, we need to reshape the data like we learned in Chapter 6, so that there is a column for year and one column each for sales and expenses, instead of separate columns for each year. We’ll also drop the satisfaction data as we don’t need it for this analysis.\n\nbudget4 &lt;- budget |&gt;\n  select(-satisfaction_2019, -satisfaction_2020) |&gt;\n  pivot_longer(cols = sales_2019:expenses_2020,\n               names_to = c(\"type\", \"year\"),\n               names_sep = \"_\",\n               names_transform = list(year = as.integer),\n               values_to = \"value\") |&gt;\n  pivot_wider(names_from = type,\n              values_from = value)\n\nhead(budget4) # check the format\n\n\n\n\nregion\nproduct\nyear\nsales\nexpenses\n\n\n\nNorth\nwidgets\n2019\n2129\n822\n\n\nNorth\nwidgets\n2020\n-517\n-897\n\n\nNorth\ngadgets\n2019\n723\n1037\n\n\nNorth\ngadgets\n2020\n77\n1115\n\n\nSouth\nwidgets\n2019\n1123\n1004\n\n\nSouth\nwidgets\n2020\n-1450\n672\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe argument names_transform lets you change the data type of a new column, since the names always default to character types.\n\n\nNow we can create summary statistics for the table.\n\nbudget4 |&gt;\n  summarise(\n    mean_sales = mean(sales),\n    mean_expenses = mean(expenses),\n    min_profit = min(expenses - sales),\n    max_profit = max(expenses - sales)\n  )\n\n\n\n\nmean_sales\nmean_expenses\nmin_profit\nmax_profit\n\n\n291.1875\n318.4375\n-2632\n2390\n\n\n\n\n\n\n7.1.6 Group By\nYou were introduced to the group_by() function in Section 4.5. For example, you can break down the summary statistics above by year and product.\n\nyear_prod &lt;- budget4 |&gt;\n  group_by(year, product) |&gt;\n  summarise(\n    mean_sales = mean(sales),\n    mean_expenses = mean(expenses),\n    min_profit = min(expenses - sales),\n    max_profit = max(expenses - sales)\n  ) |&gt;\n  ungroup()\n\nyear_prod\n\n\n\n\nyear\nproduct\nmean_sales\nmean_expenses\nmin_profit\nmax_profit\n\n\n\n2019\ngadgets\n881.50\n238.50\n-2632\n517\n\n\n2019\nwidgets\n789.25\n452.00\n-1307\n150\n\n\n2020\ngadgets\n-199.00\n803.75\n-562\n2390\n\n\n2020\nwidgets\n-307.00\n-220.50\n-1105\n2122\n\n\n\n\n\n\nAlternatively, if you have a newer version of tidyverse, you can use the .by argument to summarise by groups.\n\nbudget4 |&gt;\n  summarise(\n    .by = c(year, product),\n    mean_sales = mean(sales),\n    mean_expenses = mean(expenses),\n    min_profit = min(expenses - sales),\n    max_profit = max(expenses - sales)\n  )\n\n\n\n\nyear\nproduct\nmean_sales\nmean_expenses\nmin_profit\nmax_profit\n\n\n\n2019\nwidgets\n789.25\n452.00\n-1307\n150\n\n\n2020\nwidgets\n-307.00\n-220.50\n-1105\n2122\n\n\n2019\ngadgets\n881.50\n238.50\n-2632\n517\n\n\n2020\ngadgets\n-199.00\n803.75\n-562\n2390\n\n\n\n\n\n\n\nHow would you find out the maximum sales for each region?\n\n\nbudget3 |&gt;\n  group_by(sales) |&gt;\n  summarise(max_sales = max(region))\n\nbudget3 |&gt;\n  group_by(region) |&gt;\n  summarise(max_sales = max(sales))\n\nbudget3 |&gt;\n  group_by(sales) |&gt;\n  summarise(max_sales = max(sales))\n\nbudget3 |&gt;\n  group_by(region) |&gt;\n  summarise(max_sales = max(region))\n\n\n\nYou can also use group_by() in combination with other functions. For example, slice_max() returns the top N rows, ordered by a specific variable.\n\n# return top 3 sales\nbudget4 |&gt;\n  slice_max(n = 3, order_by = sales)\n\n\n\n\nregion\nproduct\nyear\nsales\nexpenses\n\n\n\nNorth\nwidgets\n2019\n2129\n822\n\n\nSouth\ngadgets\n2019\n2022\n-610\n\n\nWest\ngadgets\n2019\n1204\n433\n\n\n\n\n\n\nBut this can be combined with group_by() to return the top sales for each region:\n\n# return top sale for each region\nbudget4 |&gt;\n  group_by(region) |&gt;\n  slice_max(n = 1, order_by = sales)\n\n\n\n\nregion\nproduct\nyear\nsales\nexpenses\n\n\n\nEast\nwidgets\n2020\n-51\n-342\n\n\nNorth\nwidgets\n2019\n2129\n822\n\n\nSouth\ngadgets\n2019\n2022\n-610\n\n\nWest\ngadgets\n2019\n1204\n433",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07-wrangle.html#complications",
    "href": "07-wrangle.html#complications",
    "title": "7  Data Wrangling",
    "section": "\n7.2 Complications",
    "text": "7.2 Complications\n\n7.2.1 Rounding\nLet’s say we want to round all the values to the nearest pound. The pattern below uses the across() function to apply the round() function to the columns from mean_sales to max_profit.\n\nyear_prod |&gt;\n  mutate(across(.cols = mean_sales:max_profit, \n                .fns = round))\n\n\n\n\nyear\nproduct\nmean_sales\nmean_expenses\nmin_profit\nmax_profit\n\n\n\n2019\ngadgets\n882\n238\n-2632\n517\n\n\n2019\nwidgets\n789\n452\n-1307\n150\n\n\n2020\ngadgets\n-199\n804\n-562\n2390\n\n\n2020\nwidgets\n-307\n-220\n-1105\n2122\n\n\n\n\n\n\nIf you compare this table to the one in Section 7.1.6, you’ll see that the 2019 gadgets mean sales rounded up from 881.5 to 882, while the mean expenses rounded down from 238.5 to 238. What’s going on!?\nThis may seem like a mistake, but R rounds .5 to the nearest even number, rather than always up, like you were probably taught in school. This prevents overestimation biases, since x.5 is exactly halfway between x and x+1, so there is no reason it should always round up.\n\nround(0.5)\nround(1.5)\n\n[1] 0\n[1] 2\n\n\nHowever, this might throw a monkey wrench into your own systems. For example, our school policy is to round up for course marks at x.5. The easiest solution is to use the round_half_up() function from the package janitor.\nwhen you run this code, a new section will appear in the environment pane labelled “Functions”. In addition to using functions from packages, you can also make your own. It’s not something we are going to go into detail on in this course, but it’s useful to know the functionality exists.\nThis should work as you’d expect.\n\nround_half_up(0.5)\nround_half_up(1.5)\n\n[1] 1\n[1] 2\n\n\n\n7.2.2 Missing values\nIf you have control over your data, it is always best to keep missing values as empty cells rather than denoting missingness with a word or implausible number. If you used “missing” rather than leaving the cell empty, the entire variable would be read as character data (unless you note this on import), which means you wouldn’t be able to perform mathematical operations like calculating the mean. If you use an implausible number (0 or 999 are common), then you risk these values being included in any calculations as real numbers.\nHowever, we often don’t have control over how the data come to us, so let’s run through how to fix this.\n\n7.2.2.1 Bad missing values\nWhat if the South region hadn’t returned their expenses (entered as 0) and the North region hadn’t returned their sales data for 2020 yet, so someone entered it as “missing”?\nFirst, we’re going to recode the data to add in the missing values\nFor the South data, we can use case_when() to set the value of expenses to 0 if the year is 2020 and region is “South”, otherwise use the value from the expenses column (i.e., don’t change).\n\nmissing_bad &lt;- budget4 |&gt;\n  mutate(expenses = case_when(\n    # set to 0 when year is 2020 and region is North\n    year == 2020 & region == \"South\" ~ 0, \n    # otherwise, set to the value in the expenses column\n    .default = expenses   \n  ))\n\n\n\nUsing case_when() for multiple criteria\n\nThe case_when() function allows allows you to set multiple criteria, although we’re only using one non-default criterion here. It can be very useful, but takes a little practice.\nThe example below creates a label for each row. Notice how the label for the first row is “x &lt; 2”, even though this row also fits the second criterion “y &lt; 4”. This is because case_when() applies the first match to each row, even if other criteria in the function also match that row.\n\ndata &lt;- tibble(\n  x = 1:5,\n  y = 1:5\n)\n\ndata |&gt;\n  mutate(label = case_when(\n    x &lt; 2           ~ \"x &lt; 2\",\n    y &lt; 4           ~ \"y &lt; 4\",\n    x == 5 & y == 5 ~ \"both 5\",\n    .default        = \"default\"\n  ))\n\n\n\n\nx\ny\nlabel\n\n\n\n1\n1\nx &lt; 2\n\n\n2\n2\ny &lt; 4\n\n\n3\n3\ny &lt; 4\n\n\n4\n4\ndefault\n\n\n5\n5\nboth 5\n\n\n\n\n\n\n\nFor the North, we will recode these values as “missing”. Since this is character data, and sales are currently numeric data, we first need to change it to a character variable.\n\n# set sales values to \"missing\" for North 2020 rows\nmissing_bad &lt;- missing_bad |&gt;\n  mutate(sales = as.character(sales),  # Convert all sales to character\n         sales = case_when(year == 2020 & region == \"North\" ~ \"missing\", \n                          TRUE ~ sales))  # Set specific condition to \"missing\"\n\n\nstr(missing_bad)\n\ntibble [16 × 5] (S3: tbl_df/tbl/data.frame)\n $ region  : chr [1:16] \"North\" \"North\" \"North\" \"North\" ...\n $ product : chr [1:16] \"widgets\" \"widgets\" \"gadgets\" \"gadgets\" ...\n $ year    : int [1:16] 2019 2020 2019 2020 2019 2020 2019 2020 2019 2020 ...\n $ sales   : chr [1:16] \"2129\" \"missing\" \"723\" \"missing\" ...\n $ expenses: num [1:16] 822 -897 1037 1115 1004 ...\n\n\nNow, if you try to compute the mean sales, you will get an error message and the result will be NA.\n\n# try to compute mean sales\nmissing_bad |&gt;\n  summarise(mean_sales = mean(sales))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean_sales = mean(sales)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n\n\n\nmean_sales\n\n\nNA\n\n\n\n\n\n\n7.2.2.2 Convert missing values to NA\nTo set the missing values to NA, we can use the handy function na_if(). We’ll also need to transform sales back to numeric.\n\nmissing_data &lt;- missing_bad |&gt;\n  mutate(\n    # if expenses = 0, set as NA\n    expenses = na_if(expenses,0),\n    # if sales = \"missing\" set as NA\n    sales = na_if(sales, \"missing\"),\n    # convert to numeric\n    sales = as.numeric(sales)\n  )\n\nNow, if we try to calculate the mean sales and profits, we get missing values for any summary value that used one of the North 2020 sales values or the South 2020 expenses.\n\nmissing_data |&gt;\n  group_by(region) |&gt;\n  summarise(\n    mean_sales = mean(sales),\n    mean_expenses = mean(expenses),\n    min_profit = min(expenses - sales),\n    max_profit = max(expenses - sales),\n    .groups = \"drop\")\n\n\n\n\nregion\nmean_sales\nmean_expenses\nmin_profit\nmax_profit\n\n\n\nEast\n-389.00\n246.75\n-291\n2390\n\n\nNorth\nNA\n519.25\nNA\nNA\n\n\nSouth\n187.50\nNA\nNA\nNA\n\n\nWest\n763.25\n191.25\n-1105\n150\n\n\n\n\n\n\n\n7.2.2.3 Ignore missing values\nThis is because NA basically means “I don’t know”, and the sum of 100 and “I don’t know” is “I don’t know”, not 100. However, when you’re calculating means, you often want to just ignore missing values. Set na.rm = TRUE in the summary function to remove missing values before calculating.\n\nmissing_data |&gt;\n  group_by(region) |&gt;\n  summarise(\n    mean_sales = mean(sales, na.rm = TRUE),\n    mean_expenses = mean(expenses, na.rm = TRUE),\n    min_profit = min(expenses - sales, na.rm = TRUE),\n    max_profit = max(expenses - sales, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n\n\n\nregion\nmean_sales\nmean_expenses\nmin_profit\nmax_profit\n\n\n\nEast\n-389.00\n246.75\n-291\n2390\n\n\nNorth\n1426.00\n519.25\n-1307\n314\n\n\nSouth\n187.50\n197.00\n-2632\n-119\n\n\nWest\n763.25\n191.25\n-1105\n150\n\n\n\n\n\n\n\n7.2.2.4 Count missing values\nIf you want to find out how many missing or non-missing values there are in a column, use the is.na() function to get a logical vector of whether or not each value is missing, and use sum() to count how many values are TRUE or mean() to calculate the proportion of TRUE values.\n\nmissing_data |&gt;\n  group_by(year, product) |&gt;\n  summarise(\n    n_valid = sum(!is.na(sales)),\n    n_missing = sum(is.na(sales)),\n    prop_missing = mean(is.na(sales)),\n    .groups = \"drop\"\n  )\n\n\n\n\nyear\nproduct\nn_valid\nn_missing\nprop_missing\n\n\n\n2019\ngadgets\n4\n0\n0.00\n\n\n2019\nwidgets\n4\n0\n0.00\n\n\n2020\ngadgets\n3\n1\n0.25\n\n\n2020\nwidgets\n3\n1\n0.25\n\n\n\n\n\n\n\n7.2.2.5 Omit missing values\nYou may also want to remove rows that have missing values and only work from complete datasets. drop_na() will remove any row that has a missing observation. You can use drop_na() on the entire dataset which will remove any row that has any missing value, or you can specify to only remove rows that are missing a specific value.\n\n# remove any rows with any missing values\ncomplete_data &lt;- missing_data |&gt;\n  drop_na()\n\n# remove any rows that are missing a value for sales\ncomplete_sales &lt;- missing_data |&gt;\n  drop_na(sales)\n\nMissing data can be quite difficult to deal with depending on how it is represented. As always, no amount of coding expertise can make up for not understanding the structure and idiosyncrasies of your data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07-wrangle.html#sec-together-wrangle",
    "href": "07-wrangle.html#sec-together-wrangle",
    "title": "7  Data Wrangling",
    "section": "\n7.3 Exercises",
    "text": "7.3 Exercises\nLet’s try some exercises using a dataset you already encountered in Chapter 3 so that you can see how much more you’re able to do with the data now.\n\nSave your current script, close it, and open a new script named “survey-data-mad-skillz.qmd”.\nIn the set-up code chunk, load the tidyverse, then load the dataset from https://psyteachr.github.io/ads-v2/data/survey_data.csv into an object named survey_data.\nUse your method of choice to review the dataset and familiarise yourself with its structure.\n\n\n\n\nSolution\n\n# from https://www.kaggle.com/kyanyoga/sample-sales-data\nlibrary(tidyverse)\nsurvey_data &lt;- read_csv(\"https://psyteachr.github.io/reprores-v4/data/survey_data.csv\")\n\n\n\n7.3.1 Creating new categories\nEmployees 1-5 were trained by Michael and employees 6-10 were trained by Dwight.\n\nCreate a new column named trainer that lists the trainer for each employee.\nThen, calculate the average satisfaction scores for employees trained by each trainer and visualise the satisfaction scores for each in whatever way you think best.\n\nr hide(\"Hint\") To add the trainer column you can use case_when() and specify multiple criteria (e.g., if the employee is 1-5, Michael, if the employee is 6-10 Dwight) r unhide()\n\n\n\nSolution\n\n# case_when() method\nsurvey_data &lt;- survey_data |&gt;\n  mutate(trainer = case_when(employee_id %in% c(\"E01\", \"E02\", \"E03\", \"E04\", \"E05\") ~ \"Michael\",\n                             employee_id %in% c(\"E06\", \"E07\", \"E08\", \"E09\", \"E10\") ~ \"Dwight\"))\n\n\n# mean satisfaction scores\nsurvey_data |&gt;\n  group_by(trainer) |&gt;\n  summarise(mean_satisfaction = mean(satisfaction))\n\n# possible visualisation \nggplot(survey_data, aes(x = satisfaction, fill = trainer)) +\n  geom_histogram(binwidth = 1, show.legend = FALSE, colour = \"black\") +\n  facet_wrap(~trainer) +\n  labs(title = \"Satisfaction scores by employee trainer\")\n\n\n\n\n\n\n\n\n\n\ntrainer\nmean_satisfaction\n\n\n\nDwight\n3.366755\n\n\nMichael\n3.088415\n\n\n\n\n\n\n\n\n7.3.2 Filter by calculated score\nFirst, calculate the average wait time and store this in an object named mean_wait. This should be a single value rather than a table.\n\n\nHint\n\nThere are multiple ways to achieve this. You could create the table and then pull out the single value, or just calculate the single value.\n\n\n\n\nSolution\n\n# method 1 - tidyverse\nmean_wait &lt;- survey_data |&gt;\n  summarise(mean_wait = mean(wait_time)) |&gt;\n  pull(mean_wait)\n\n# method 2 - base R\nmean_wait &lt;- mean(survey_data$wait_time)\n\n\nNow create a dataset named long_wait that just contains data from customers who waited more than the average wait time.\n\n\n\nSolution\n\nlong_wait &lt;- survey_data |&gt;\n  filter(wait_time &gt; mean_wait)\n\n\nCreate a visualisation that shows how many customers waited more than the average wait time for each employee.\n\n\n\nSolution\n\nlong_wait |&gt;\n  ggplot(aes(x = employee_id)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n7.3.3 Multiple critera\nNow, add a column to survey_data named follow_up that flags whether a customer should be followed up with a courtesy phone call. Your company is short-staffed so only customers that meet all three of the following criteria should be followed-up:\n\nTheir wait time should be above the average for all calls\n\nTheir call time should be above the average for their category\n\nTheir satisfaction should be less than three 3.\n\nThis is quite complicated and there are multiple ways to achieve the desired outcome. Some approaches may need other functions that were covered in previous chapters and you may need to create intermediate objects.\nCall the final object follow_data and keep only the customer ID, employee ID, trainer, and follow up columns.\n\n\n\nSolution\n\n# this is one possible solution, there are many other valid approaches \n\n# calculate mean wait time across all calls\nmean_wait &lt;- mean(survey_data$wait_time)\n\n# calculate mean call time for each category\nfollow_data &lt;- survey_data |&gt;\n  group_by(issue_category) |&gt;\n  summarise(mean_call = mean(call_time)) |&gt;\n#then join it to the survey data  \n  left_join(survey_data, by = \"issue_category\") |&gt;\n# then add on the column\n  mutate(follow_up = case_when(wait_time &gt; mean_wait & \n                               call_time &gt; mean_call & \n                               satisfaction &lt; 3 ~ \"yes\",\n                               .default = \"no\")) |&gt;\n  select(caller_id, employee_id, trainer, follow_up)\n\n\nFor all of the above, write code that stores the answer as a single value, so that you could easily use it in inline coding.\nHow many customers need to be followed up:\n\nIn total? \n\nFrom calls by employee 06? \n\nFrom calls by employees trained by Michael \n\nFrom calls by employees trained by Dwight \n\n\n\n\nHint\n\ngroup_by() |&gt; count() |&gt; filter() |&gt; pull()\n\nWhich employee needs to make the largest number of follow-up courtesy calls? \n\n\nHint\n\nAs above but add in an ungroup() and slice_max() along the way.\n\n\n\n\nSolution\n\n# in total\nfollow_data |&gt;\n  group_by(follow_up) |&gt;\n  count()|&gt;\n  filter(follow_up == \"yes\") |&gt;\n  pull(n)\n\n# by employee 6\nfollow_data |&gt;\n  group_by(follow_up, employee_id) |&gt;\n  count() |&gt;\n  filter(employee_id == \"E06\",\n         follow_up == \"yes\") |&gt;\n  pull(n)\n\n# by michael\nfollow_data |&gt;\n  group_by(follow_up, trainer) |&gt;\n  count() |&gt;\n  filter(trainer == \"Michael\",\n         follow_up == \"yes\") |&gt;\n  pull(n)\n\n# by dwight\nfollow_data |&gt;\n  group_by(follow_up, trainer) |&gt;\n  count() |&gt;\n  filter(trainer == \"Dwight\",\n         follow_up == \"yes\") |&gt;\n  pull(n)\n\n# most follow-ups needed\nfollow_data |&gt;\n  group_by(follow_up, employee_id) |&gt;\n  count() |&gt;\n  ungroup() |&gt;\n  filter(follow_up == \"yes\") |&gt;\n  slice_max(n = 1, order_by = n) |&gt;\n  pull(employee_id)\n\n[1] 120\n[1] 16\n[1] 65\n[1] 55\n[1] \"E02\"\n\n\n\n\n7.3.4 Original insight\nIn preparation for the summative assessment, explore the data to provide one original insight of your own.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07-wrangle.html#sec-glossary-wwrangle",
    "href": "07-wrangle.html#sec-glossary-wwrangle",
    "title": "7  Data Wrangling",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\nterm\ndefinition\n\n\n\nboolean-expression\nAn expression that evaluates to TRUE or FALSE.\n\n\ncharacter\nA data type representing strings of text.\n\n\ndata-type\nThe kind of data represented by an object.\n\n\ndata-wrangling\nThe process of preparing data for visualisation and statistical analysis.\n\n\nfactor\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\nlogical\nA data type representing TRUE or FALSE values.\n\n\noperator\nA symbol that performs some mathematical or comparative process.\n\n\nstring\nA piece of text inside of quotes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "07-wrangle.html#sec-resources-wrangle",
    "href": "07-wrangle.html#sec-resources-wrangle",
    "title": "7  Data Wrangling",
    "section": "Further resources",
    "text": "Further resources\n\nData transformation cheat sheet\n\nChapter 18: Missing Data in R for Data Science\n\n\nChapter 3: Data Transformation in R for Data Science\n\n\nChapter 25: Functions in R for Data Science\n\nIntroduction to stringr",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "08-func.html",
    "href": "08-func.html",
    "title": "8  Iteration & Functions",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Iteration & Functions</span>"
    ]
  },
  {
    "objectID": "08-func.html#sec-ilo-func",
    "href": "08-func.html#sec-ilo-func",
    "title": "8  Iteration & Functions",
    "section": "",
    "text": "Work with basic iteration functions: rep, seq, replicate\nUse purrr::map() and apply() functions\nWrite your own custom functions with function()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Iteration & Functions</span>"
    ]
  },
  {
    "objectID": "08-func.html#sec-setup-func",
    "href": "08-func.html#sec-setup-func",
    "title": "8  Iteration & Functions",
    "section": "Setup",
    "text": "Setup\n\nOpen your reprores project\nCreate a new quarto file called 03-dataviz.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\nlibrary(tidyverse)  # loads purrr for iteration\nlibrary(broom)      # converts test output to tidy tables\n\nset.seed(8675309) # makes sure random numbers are reproducible\n```\n\nDownload the Apply functions with purrr cheat sheet.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Iteration & Functions</span>"
    ]
  },
  {
    "objectID": "08-func.html#sec-iteration-functions",
    "href": "08-func.html#sec-iteration-functions",
    "title": "8  Iteration & Functions",
    "section": "\n8.1 Iteration functions",
    "text": "8.1 Iteration functions\nIn the next two lectures, we are going to learn more about iteration (doing the same commands over and over) and custom functions through a data simulation exercise, which will also prepare us more traditional statistical topics. We first learned about the two basic iteration functions, rep() and seq() in the Working with Data chapter.\n\n8.1.1 rep()\nThe function rep() lets you repeat the first argument a number of times.\nUse rep() to create a vector of alternating \"A\" and \"B\" values of length 24.\n\nrep(c(\"A\", \"B\"), 12)\n\n [1] \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\"\n[20] \"B\" \"A\" \"B\" \"A\" \"B\"\n\n\nIf you don’t specify what the second argument is, it defaults to times, repeating the vector in the first argument that many times. Make the same vector as above, setting the second argument explicitly.\n\nrep(c(\"A\", \"B\"), times = 12)\n\n [1] \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\"\n[20] \"B\" \"A\" \"B\" \"A\" \"B\"\n\n\nIf the second argument is a vector that is the same length as the first argument, each element in the first vector is repeated than many times. Use rep() to create a vector of 11 \"A\" values followed by 3 \"B\" values.\n\nrep(c(\"A\", \"B\"), c(11, 3))\n\n [1] \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"B\" \"B\"\n\n\nYou can repeat each element of the vector a sepcified number of times using the each argument, Use rep() to create a vector of 12 \"A\" values followed by 12 \"B\" values.\n\nrep(c(\"A\", \"B\"), each = 12)\n\n [1] \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\n[20] \"B\" \"B\" \"B\" \"B\" \"B\"\n\n\nWhat do you think will happen if you set both times to 3 and each to 2?\n\nrep(c(\"A\", \"B\"), times = 3, each = 2)\n\n [1] \"A\" \"A\" \"B\" \"B\" \"A\" \"A\" \"B\" \"B\" \"A\" \"A\" \"B\" \"B\"\n\n\n\n8.1.2 seq()\nThe function seq() is useful for generating a sequence of numbers with some pattern.\nUse seq() to create a vector of the integers 0 to 10.\n\nseq(0, 10)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\n\nYou can set the by argument to count by numbers other than 1 (the default). Use seq() to create a vector of the numbers 0 to 100 by 10s.\n\nseq(0, 100, by = 10)\n\n [1]   0  10  20  30  40  50  60  70  80  90 100\n\n\nThe argument length.out is useful if you know how many steps you want to divide something into. Use seq() to create a vector that starts with 0, ends with 100, and has 12 equally spaced steps (hint: how many numbers would be in a vector with 2 steps?).\n\nseq(0, 100, length.out = 13)\n\n [1]   0.000000   8.333333  16.666667  25.000000  33.333333  41.666667\n [7]  50.000000  58.333333  66.666667  75.000000  83.333333  91.666667\n[13] 100.000000\n\n\n\n8.1.3 replicate()\nYou can use the replicate() function to run a function n times.\nFor example, you can get 3 sets of 5 numbers from a random normal distribution by setting n to 3 and expr to rnorm(5).\n\nreplicate(n = 3, expr = rnorm(5))\n\n           [,1]       [,2]       [,3]\n[1,] 1.06541605  0.9036777 -0.9945890\n[2,] 0.98721974 -1.5495524  1.9724587\n[3,] 0.02745393  1.0226378 -0.4418016\n[4,] 0.67287232  0.1500832 -0.9006372\n[5,] 0.57206650 -0.6599640 -0.1505882\n\n\nBy default, replicate() simplifies your result into a matrix that is easy to convert into a table if your function returns vectors that are the same length. If you’d rather have a list of vectors, set simplify = FALSE.\n\nreplicate(n = 3, expr = rnorm(5), simplify = FALSE)\n\n[[1]]\n[1]  1.98582582  0.04400503 -0.40428231 -0.47299855 -0.41482324\n\n[[2]]\n[1]  0.6832342  0.6902011  0.5334919 -0.1861048  0.3829458\n\n[[3]]\n[1]  0.3761842  1.1535300  1.5749028  0.5885273 -0.6150452\n\n\n\n8.1.4 map() and apply() functions\npurrr::map() and lapply() return a list of the same length as a vector or list, each element of which is the result of applying a function to the corresponding element. They function much the same, but purrr functions have some optimisations for working with the tidyverse. We’ll be working mostly with purrr functions in this course, but apply functions are very common in code that you might see in examples on the web.\nImagine you want to calculate the power for a two-sample t-test with a mean difference of 0.2 and SD of 1, for all the sample sizes 100 to 1000 (by 100s). You could run the power.t.test() function 20 times and extract the values for “power” from the resulting list and put it in a table.\n\np100 &lt;- power.t.test(n = 100, delta = 0.2, sd = 1, type=\"two.sample\")\n# 18 more lines\np1000 &lt;- power.t.test(n = 500, delta = 0.2, sd = 1, type=\"two.sample\")\n\ntibble(\n  n = c(100, \"...\", 1000),\n  power = c(p100$power, \"...\", p1000$power)\n)\n\n\n\n\nn\npower\n\n\n\n100\n0.290266404572217\n\n\n…\n…\n\n\n1000\n0.884788352886661\n\n\n\n\n\n\nHowever, the apply() and map() functions allow you to perform a function on each item in a vector or list. First make an object n that is the vector of the sample sizes you want to test, then use lapply() or map() to run the function power.t.test() on each item. You can set other arguments to power.t.test() after the function argument.\n\nn &lt;- seq(100, 1000, 100)\npcalc &lt;- lapply(n, power.t.test, \n                delta = 0.2, sd = 1, type=\"two.sample\")\n# or\npcalc &lt;- purrr::map(n, power.t.test, \n                delta = 0.2, sd = 1, type=\"two.sample\")\n\nThese functions return a list where each item is the result of power.t.test(), which returns a list of results that includes the named item “power”. This is a special list that has a summary format if you just print it directly:\n\npcalc[[1]]\n\n\n     Two-sample t test power calculation \n\n              n = 100\n          delta = 0.2\n             sd = 1\n      sig.level = 0.05\n          power = 0.2902664\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nBut you can see the individual items using the str() function.\n\npcalc[[1]] |&gt; str()\n\nList of 8\n $ n          : num 100\n $ delta      : num 0.2\n $ sd         : num 1\n $ sig.level  : num 0.05\n $ power      : num 0.29\n $ alternative: chr \"two.sided\"\n $ note       : chr \"n is number in *each* group\"\n $ method     : chr \"Two-sample t test power calculation\"\n - attr(*, \"class\")= chr \"power.htest\"\n\n\nsapply() is a version of lapply() that returns a vector or array instead of a list, where appropriate. The corresponding purrr functions are map_dbl(), map_chr(), map_int() and map_lgl(), which return vectors with the corresponding data type.\nYou can extract a value from a list with the function [[. You usually see this written as pcalc[[1]], but if you put it inside backticks, you can use it in apply and map functions.\n\nsapply(pcalc, `[[`, \"power\")\n\n [1] 0.2902664 0.5140434 0.6863712 0.8064964 0.8847884 0.9333687 0.9623901\n [8] 0.9792066 0.9887083 0.9939638\n\n\nWe use map_dbl() here because the value for “power” is a double.\n\npurrr::map_dbl(pcalc, `[[`, \"power\")\n\n [1] 0.2902664 0.5140434 0.6863712 0.8064964 0.8847884 0.9333687 0.9623901\n [8] 0.9792066 0.9887083 0.9939638\n\n\nWe can use the map() functions inside a mutate() function to run the power.t.test() function on the value of n from each row of a table, then extract the value for “power”, and delete the column with the power calculations.\n\nmypower &lt;- tibble(\n  n = seq(100, 1000, 100)) |&gt;\n  mutate(pcalc = purrr::map(n, power.t.test, \n                            delta = 0.2, \n                            sd = 1, \n                            type=\"two.sample\"),\n         power = purrr::map_dbl(pcalc, `[[`, \"power\")) |&gt;\n  select(-pcalc)\n\n\n\n\n\nPower for a two-sample t-test with d = 0.2",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Iteration & Functions</span>"
    ]
  },
  {
    "objectID": "08-func.html#custom-functions",
    "href": "08-func.html#custom-functions",
    "title": "8  Iteration & Functions",
    "section": "\n8.2 Custom functions",
    "text": "8.2 Custom functions\nIn addition to the built-in functions and functions you can access from packages, you can also write your own functions (and eventually even packages!).\n\n8.2.1 Structuring a function\nThe general structure of a function is as follows:\n\nfunction_name &lt;- function(my_args) {\n  # process the arguments\n  # return some value\n}\n\nHere is a very simple function. Can you guess what it does?\n\nadd1 &lt;- function(my_number) {\n  my_number + 1\n}\n\nadd1(10)\n\n[1] 11\n\n\nLet’s make a function that reports p-values in APA format (with “p = [rounded value]” when p &gt;= .001 and “p &lt; .001” when p &lt; .001).\nFirst, we have to name the function. You can name it anything, but try not to duplicate existing functions or you will overwrite them. For example, if you call your function rep, then you will need to use base::rep() to access the normal rep function. Let’s call our p-value function report_p and set up the framework of the function.\n\nreport_p &lt;- function() {\n}\n\n\n8.2.2 Arguments\nWe need to add one argument, the p-value you want to report. The names you choose for the arguments are private to that argument, so it is not a problem if they conflict with other variables in your script. You put the arguments in the parentheses of function() in the order you want them to default (just like the built-in functions you’ve used before).\n\nreport_p &lt;- function(p) {\n}\n\n\n8.2.3 Argument defaults\nYou can add a default value to any argument. If that argument is skipped, then the function uses the default argument. It probably doesn’t make sense to run this function without specifying the p-value, but we can add a second argument called digits that defaults to 3, so we can round p-values to any number of digits.\n\nreport_p &lt;- function(p, digits = 3) {\n}\n\nNow we need to write some code inside the function to process the input arguments and turn them into a returned output. Put the output as the last item in function.\n\nreport_p &lt;- function(p, digits = 3) {\n  if (p &lt; .001) {\n    reported = \"p &lt; .001\"\n  } else {\n    roundp &lt;- round(p, digits)\n    reported = paste(\"p =\", roundp)\n  }\n  \n  reported\n}\n\nYou might also see the returned output inside of the return() function. This does the same thing.\n\nreport_p &lt;- function(p, digits = 3) {\n  if (p &lt; .001) {\n    reported = \"p &lt; .001\"\n  } else {\n    roundp &lt;- round(p, digits)\n    reported = paste(\"p =\", roundp)\n  }\n  \n  return(reported)\n}\n\nWhen you run the code defining your function, it doesn’t output anything, but makes a new object in the Environment tab under Functions. Now you can run the function.\n\nreport_p(0.04869)\nreport_p(0.0000023)\n\n[1] \"p = 0.049\"\n[1] \"p &lt; .001\"\n\n\n\n8.2.4 Scope\nWhat happens in a function stays in a function. You can change the value of a variable passed to a function, but that won’t change the value of the variable outside of the function, even if that variable has the same name as the one in the function.\n\nreported &lt;- \"not changed\"\n\n# inside this function, reported == \"p = 0.002\"\nreport_p(0.0023) \n\nreported # still \"not changed\"\n\n[1] \"p = 0.002\"\n[1] \"not changed\"\n\n\n\n8.2.5 Warnings and errors\n\nWhat happens when you omit the argument for p? Or if you set p to 1.5 or “a”?\n\n\nYou might want to add a more specific warning and stop running the function code if someone enters a value that isn’t a number. You can do this with the stop() function.\nIf someone enters a number that isn’t possible for a p-value (0-1), you might want to warn them that this is probably not what they intended, but still continue with the function. You can do this with warning().\n\nreport_p &lt;- function(p, digits = 3) {\n  if (!is.numeric(p)) stop(\"p must be a number\")\n  if (p &lt;= 0) warning(\"p-values are normally greater than 0\")\n  if (p &gt;= 1) warning(\"p-values are normally less than 1\")\n  \n  if (p &lt; .001) {\n    reported = \"p &lt; .001\"\n  } else {\n    roundp &lt;- round(p, digits)\n    reported = paste(\"p =\", roundp)\n  }\n  \n  reported\n}\n\n\nreport_p()\n\nError in report_p(): argument \"p\" is missing, with no default\n\nreport_p(\"a\")\n\nError in report_p(\"a\"): p must be a number\n\nreport_p(-2)\n\nWarning in report_p(-2): p-values are normally greater than 0\n\nreport_p(2)\n\nWarning in report_p(2): p-values are normally less than 1\n\n\n[1] \"p &lt; .001\"\n[1] \"p = 2\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Iteration & Functions</span>"
    ]
  },
  {
    "objectID": "08-func.html#iterating-your-own-functions",
    "href": "08-func.html#iterating-your-own-functions",
    "title": "8  Iteration & Functions",
    "section": "\n8.3 Iterating your own functions",
    "text": "8.3 Iterating your own functions\n\n8.3.1 Build code\nFirst, let’s build up the code that we want to iterate.\n\n8.3.1.1 Simulate and structure data\nCreate a vector of 20 random numbers drawn from a normal distribution with a mean of 5 and standard deviation of 1 using the rnorm() function and store them in the variable A.\n\nA &lt;- rnorm(20, mean = 5, sd = 1)\n\nA tibble is a type of table or data.frame. The function tibble::tibble() creates a tibble with a column for each argument. Each argument takes the form column_name = data_vector.\nCreate a table called dat including two vectors: A that is a vector of 20 random normally distributed numbers with a mean of 5 and SD of 1, and B that is a vector of 20 random normally distributed numbers with a mean of 5.5 and SD of 1.\n\ndat &lt;- tibble(\n  A = rnorm(20, 5, 1),\n  B = rnorm(20, 5.5, 1)\n)\n\n\n8.3.1.2 Statistical test\nYou can run a Welch two-sample t-test by including the two samples you made as the first two arguments to the function t.test. You can reference one column of a table by its names using the format table_name$column_name\n\nt.test(dat$A, dat$B)\n\n\n    Welch Two Sample t-test\n\ndata:  dat$A and dat$B\nt = -2.9374, df = 37.589, p-value = 0.005625\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.5520051 -0.2853175\nsample estimates:\nmean of x mean of y \n 4.989421  5.908082 \n\n\nYou can also convert the table to long format using the gather function and specify the t-test using the format dv_column~grouping_column.\n\nlongdat &lt;- gather(dat, group, score, A:B)\n\nt.test(score~group, data = longdat) \n\n\n    Welch Two Sample t-test\n\ndata:  score by group\nt = -2.9374, df = 37.589, p-value = 0.005625\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.5520051 -0.2853175\nsample estimates:\nmean in group A mean in group B \n       4.989421        5.908082 \n\n\n\n8.3.1.3 Tidy output\nYou can use the function broom::tidy() to extract the data from a statistical test in a table format. The example below pipes everything together.\n\ntibble(\n  A = rnorm(20, 5, 1),\n  B = rnorm(20, 5.5, 1)\n) |&gt;\n  gather(group, score, A:B) |&gt;\n  t.test(score~group, data = _) |&gt;\n  broom::tidy()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n-0.5035898\n5.093854\n5.597443\n-1.600133\n0.1180851\n36.95278\n-1.141295\n0.1341157\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\nIn the pipeline above, t.test(score~group, data = _) uses the _ notation to change the location of the piped-in data table from it’s default position as the first argument to a different position.\n\n\n8.3.1.4 Extract important values\nFinally, we can extract a single value from this results table using pull().\n\ntibble(\n  A = rnorm(20, 5, 1),\n  B = rnorm(20, 5.5, 1)\n) |&gt;\n  gather(group, score, A:B) |&gt;\n  t.test(score~group, data = _) |&gt;\n  broom::tidy() |&gt;\n  pull(p.value)\n\n[1] 0.002630565\n\n\n\n8.3.2 Custom function\nNext, we can group the code above inside a function.\nFirst, name your function t_sim and wrap the code above in a function with no arguments.\n\nt_sim &lt;- function() {\n  tibble(\n    A = rnorm(20, 5, 1),\n    B = rnorm(20, 5.5, 1)\n  ) |&gt;\n    gather(group, score, A:B) |&gt;\n    t.test(score~group, data = _) |&gt;\n    broom::tidy() |&gt;\n    pull(p.value) \n}\n\nRun it a few times to see what happens.\n\nt_sim()\n\n[1] 0.2160769\n\n\n\n8.3.2.1 Iterate\nLet’s run the t_sim function 1000 times, assign the resulting p-values to a vector called reps, and check what proportion of p-values are lower than alpha (e.g., .05). This number is the power for this analysis.\n\nreps &lt;- replicate(1000, t_sim())\nalpha &lt;- .05\npower &lt;- mean(reps &lt; alpha)\npower\n\n[1] 0.349\n\n\n\n8.3.2.2 Set seed\nYou can use the set.seed function before you run a function that uses random numbers to make sure that you get the same random data back each time. You can use any integer you like as the seed.\n\nset.seed(90201)\n\n\nMake sure you don’t ever use set.seed() inside of a simulation function, or you will just simulate the exact same data over and over again.\n\n\n\n\n\n@KellyBodwin\n\n\n\n\n8.3.2.3 Add arguments\nYou can just edit your function each time you want to calculate power for a different sample n, but it is more efficient to build this into your function as an arguments. Redefine t_sim, setting arguments for the mean and SD of group A, the mean and SD of group B, and the number of subjects per group. Give them all default values.\n\nt_sim &lt;- function(n = 10, m1=0, sd1=1, m2=0, sd2=1) {\n  tibble(\n    A = rnorm(n, m1, sd1),\n    B = rnorm(n, m2, sd2)\n  ) |&gt;\n    gather(group, score, A:B) |&gt;\n    t.test(score~group, data = _) |&gt;\n    broom::tidy() |&gt;\n    pull(p.value) \n}\n\n\n8.3.3 Test your function\nTest your function with some different values to see if the results make sense.\n\nt_sim(100)\nt_sim(100, 0, 1, 0.5, 1)\n\n[1] 0.8460064\n[1] 0.0002446404\n\n\nUse replicate to calculate power for 100 subjects/group with an effect size of 0.2 (e.g., A: m = 0, SD = 1; B: m = 0.2, SD = 1). Use 1000 replications.\n\nreps &lt;- replicate(1000, t_sim(100, 0, 1, 0.2, 1))\npower &lt;- mean(reps &lt; .05)\npower\n\n[1] 0.284\n\n\nCompare this to power calculated from the power.t.test function.\n\npower.t.test(n = 100, delta = 0.2, sd = 1, type=\"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 100\n          delta = 0.2\n             sd = 1\n      sig.level = 0.05\n          power = 0.2902664\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\nCalculate power via simulation and power.t.test for the following tests:\n\n20 subjects/group, A: m = 0, SD = 1; B: m = 0.2, SD = 1\n40 subjects/group, A: m = 0, SD = 1; B: m = 0.2, SD = 1\n20 subjects/group, A: m = 10, SD = 1; B: m = 12, SD = 1.5",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Iteration & Functions</span>"
    ]
  },
  {
    "objectID": "08-func.html#sec-glossary-func",
    "href": "08-func.html#sec-glossary-func",
    "title": "8  Iteration & Functions",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\nterm\ndefinition\n\n\n\nargument\nA variable that provides input to a function.\n\n\ndata-type\nThe kind of data represented by an object.\n\n\ndouble\nA data type representing a real decimal number\n\n\nfunction\nA named section of code that can be reused.\n\n\niteration\nRepeating a process or function\n\n\nmatrix\nA container data type consisting of numbers arranged into a fixed number of rows and columns",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Iteration & Functions</span>"
    ]
  },
  {
    "objectID": "08-func.html#sec-resources-func",
    "href": "08-func.html#sec-resources-func",
    "title": "8  Iteration & Functions",
    "section": "Further Resources",
    "text": "Further Resources\n\nChapters 19 and 21 of R for Data Science\n\nApply functions with purrr cheat sheet",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Iteration & Functions</span>"
    ]
  },
  {
    "objectID": "09-sim.html",
    "href": "09-sim.html",
    "title": "9  Probability & Simulation",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "09-sim.html#sec-ilo-sim",
    "href": "09-sim.html#sec-ilo-sim",
    "title": "9  Probability & Simulation",
    "section": "",
    "text": "Generate and plot data randomly sampled from common distributions\nGenerate related variables from a multivariate distribution\nDefine the following statistical terms: p-value, alpha, power, smallest effect size of interest (SESOI), false positive (type I error), false negative (type II error), confidence interval (CI)\nTest sampled distributions against a null hypothesis\nCalculate power using iteration and a sampling function",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "09-sim.html#sec-setup-sim",
    "href": "09-sim.html#sec-setup-sim",
    "title": "9  Probability & Simulation",
    "section": "Setup",
    "text": "Setup\n\nOpen your reprores project\nCreate a new quarto file called 09-sim.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\nlibrary(tidyverse) # for data wrangling\nlibrary(faux)      # data simulation\nlibrary(plotly)    # create a 3D plot to visualise correlations\n# MASS::mvrnorm() is used without loading MASS\n\nset.seed(8675309) # makes sure random numbers are reproducible\n```\n\nSimulating data is a very powerful way to test your understanding of statistical concepts. We are going to use simulations to learn the basics of probability.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "09-sim.html#univariate-distributions",
    "href": "09-sim.html#univariate-distributions",
    "title": "9  Probability & Simulation",
    "section": "\n9.1 Univariate Distributions",
    "text": "9.1 Univariate Distributions\nFirst, we need to understand some different ways data might be distributed and how to simulate data from these distributions. A univariate distribution is the distribution of a single variable.\n\n9.1.1 Uniform Distribution\nThe uniform distribution is the simplest distribution. All numbers in the range have an equal probability of being sampled.\n\nTake a minute to think of things in your own research that are uniformly distributed.\n\n\n9.1.1.1 Continuous distribution\nrunif(n, min=0, max=1)\nUse runif() to sample from a continuous uniform distribution.\n\nu &lt;- runif(100000, min = 0, max = 1)\n\n# plot to visualise\nggplot() + \n  geom_histogram(aes(u), binwidth = 0.05, boundary = 0,\n                 fill = \"white\", colour = \"black\")\n\n\n\n\n\n\n\n\n9.1.1.2 Discrete\nsample(x, size, replace = FALSE, prob = NULL)\nUse sample() to sample from a discrete distribution.\nYou can use sample() to simulate events like rolling dice or choosing from a deck of cards. The code below simulates rolling a 6-sided die 10000 times. We set replace to TRUE so that each event is independent. See what happens if you set replace to FALSE.\n\nrolls &lt;- sample(1:6, 10000, replace = TRUE)\n\n# plot the results\nggplot() + \n  geom_histogram(aes(rolls), binwidth = 1, \n                 fill = \"white\", color = \"black\")\n\n\n\nDistribution of dice rolls.\n\n\n\nYou can also use sample to sample from a list of named outcomes.\n\npet_types &lt;- c(\"cat\", \"dog\", \"ferret\", \"bird\", \"fish\")\nsample(pet_types, 10, replace = TRUE)\n\n [1] \"fish\"   \"ferret\" \"ferret\" \"bird\"   \"ferret\" \"cat\"    \"bird\"   \"dog\"   \n [9] \"dog\"    \"fish\"  \n\n\nFerrets are a much less common pet than cats and dogs, so our sample isn’t very realistic. You can set the probabilities of each item in the list with the prob argument.\n\npet_types &lt;- c(\"cat\", \"dog\", \"ferret\", \"bird\", \"fish\")\npet_prob &lt;- c(0.3, 0.4, 0.1, 0.1, 0.1)\nsample(pet_types, 10, replace = TRUE, prob = pet_prob)\n\n [1] \"cat\"    \"dog\"    \"cat\"    \"bird\"   \"dog\"    \"fish\"   \"dog\"    \"cat\"   \n [9] \"ferret\" \"fish\"  \n\n\n\n9.1.2 Binomial Distribution\nThe binomial distribution is useful for modelling binary data, where each observation can have one of two outcomes, like success/failure, yes/no or head/tails.\nrbinom(n, size, prob)\nThe rbinom function will generate a random binomial distribution.\n\n\nn = number of observations\n\nsize = number of trials\n\nprob = probability of success on each trial\n\nCoin flips are a typical example of a binomial distribution, where we can assign heads to 1 and tails to 0.\n\n# 20 individual coin flips of a fair coin\nrbinom(20, 1, 0.5)\n\n [1] 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0\n\n\n\n# 20 individual coin flips of a baised (0.75) coin\nrbinom(20, 1, 0.75)\n\n [1] 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1\n\n\nYou can generate the total number of heads in 1 set of 20 coin flips by setting size to 20 and n to 1.\n\nrbinom(1, 20, 0.75)\n\n[1] 16\n\n\nYou can generate more sets of 20 coin flips by increasing the n.\n\nrbinom(10, 20, 0.5)\n\n [1]  9  8  9 12 10 11 12 12  8 12\n\n\nYou should always check your randomly generated data to check that it makes sense. For large samples, it’s easiest to do that graphically. A histogram is usually the best choice for plotting binomial data.\n\nflips &lt;- rbinom(1000, 20, 0.5)\n\nggplot() +\n  geom_histogram(\n    aes(flips), \n    binwidth = 1, \n    fill = \"white\", \n    color = \"black\"\n  )\n\n\n\n\n\n\n\n\nRun the simulation above several times, noting how the histogram changes. Try changing the values of n, size, and prob.\n\n\n9.1.3 Normal Distribution\nrnorm(n, mean, sd)\nWe can simulate a normal distribution of size n if we know the mean and standard deviation (sd). A density plot is usually the best way to visualise this type of data if your n is large.\n\ndv &lt;- rnorm(1e5, 10, 2)\n\n# proportions of normally-distributed data \n# within 1, 2, or 3 SD of the mean\nsd1 &lt;- .6827 \nsd2 &lt;- .9545\nsd3 &lt;- .9973\n\nggplot() +\n  geom_density(aes(dv), fill = \"white\") +\n  geom_vline(xintercept = mean(dv), color = \"red\") +\n  geom_vline(xintercept = quantile(dv, .5 - sd1/2), color = \"darkgreen\") +\n  geom_vline(xintercept = quantile(dv, .5 + sd1/2), color = \"darkgreen\") +\n  geom_vline(xintercept = quantile(dv, .5 - sd2/2), color = \"blue\") +\n  geom_vline(xintercept = quantile(dv, .5 + sd2/2), color = \"blue\") +\n  geom_vline(xintercept = quantile(dv, .5 - sd3/2), color = \"purple\") +\n  geom_vline(xintercept = quantile(dv, .5 + sd3/2), color = \"purple\") +\n  scale_x_continuous(\n    limits = c(0,20), \n    breaks = seq(0,20)\n  )\n\n\n\n\n\n\n\n\nRun the simulation above several times, noting how the density plot changes. What do the vertical lines represent? Try changing the values of n, mean, and sd.\n\n\n9.1.4 Poisson Distribution\nThe Poisson distribution is useful for modelling events, like how many times something happens over a unit of time, as long as the events are independent (e.g., an event having happened in one time period doesn’t make it more or less likely to happen in the next).\nrpois(n, lambda)\nThe rpois function will generate a random Poisson distribution.\n\n\nn = number of observations\n\nlambda = the mean number of events per observation\n\nLet’s say we want to model how many texts you get each day for a whole. You know that you get an average of 20 texts per day. So we set n = 365 and lambda = 20. Lambda is a parameter that describes the Poisson distribution, just like mean and standard deviation are parameters that describe the normal distribution.\n\ntexts &lt;- rpois(n = 365, lambda = 20)\n\nggplot() +\n  geom_histogram(\n    aes(texts), \n    binwidth = 1, \n    fill = \"white\", \n    color = \"black\"\n  )\n\n\n\n\n\n\n\nSo we can see that over a year, you’re unlikely to get fewer than 5 texts in a day, or more than 35 (although it’s not impossible).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "09-sim.html#mvdist",
    "href": "09-sim.html#mvdist",
    "title": "9  Probability & Simulation",
    "section": "\n9.2 Multivariate Distributions",
    "text": "9.2 Multivariate Distributions\n\n9.2.1 Bivariate Normal\nA bivariate normal distribution is two normally distributed vectors that have a specified relationship, or correlation to each other.\nWhat if we want to sample from a population with specific relationships between variables? We can sample from a bivariate normal distribution using mvrnorm() from the MASS package.\n\nDon’t load MASS with the library() function because it will create a conflict with the select() function from dplyr and you will always need to preface it with dplyr::. Just use MASS::mvrnorm().\n\nYou need to know how many observations you want to simulate (n) the means of the two variables (mu) and you need to calculate a covariance matrix (sigma) from the correlation between the variables (rho) and their standard deviations (sd).\n\nn   &lt;- 1000 # number of random samples\n# name the mu values to give the resulting columns names\nmu     &lt;- c(x = 10, y = 20) # the means of the samples\nsd &lt;- c(5, 6)   # the SDs of the samples\n\nrho &lt;- 0.5  # population correlation between the two variables\n\n# correlation matrix\ncor_mat &lt;- matrix(c(  1, rho, \n                    rho,   1), 2) \n\n# create the covariance matrix\nsigma &lt;- (sd %*% t(sd)) * cor_mat\n\n# sample from bivariate normal distribution\nbvn &lt;- MASS::mvrnorm(n, mu, sigma) \n\nPlot your sampled variables to check everything worked like you expect. It’s easiest to convert the output of mvnorm into a tibble in order to use it in ggplot.\n\nbvn |&gt;\n  as_tibble() |&gt;\n  ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) + \n    geom_smooth(method = \"lm\") +\n    geom_density2d()\n\n\n\n\n\n\n\n\n9.2.2 Multivariate Normal\nYou can generate more than 2 correlated variables, but it gets a little trickier to create the correlation matrix.\n\nn      &lt;- 200 # number of random samples\nmu     &lt;- c(x = 10, y = 20, z = 30) # the means of the samples\nsd &lt;- c(8, 9, 10)   # the SDs of the samples\n\nrho1_2 &lt;- 0.5 # correlation between x and y\nrho1_3 &lt;- 0   # correlation between x and z\nrho2_3 &lt;- 0.7 # correlation between y and z\n\n# correlation matrix\ncor_mat &lt;- matrix(c(     1, rho1_2, rho1_3, \n                    rho1_2,      1, rho2_3,\n                    rho1_3, rho2_3,      1), 3) \n\nsigma &lt;- (sd %*% t(sd)) * cor_mat\nbvn3 &lt;- MASS::mvrnorm(n, mu, sigma)\n\ncor(bvn3) # check correlation matrix\n\n             x         y            z\nx  1.000000000 0.5020677 -0.001287912\ny  0.502067687 1.0000000  0.721283397\nz -0.001287912 0.7212834  1.000000000\n\n\nYou can use the plotly library to make a 3D graph.\n\n#set up the marker style\nmarker_style = list(\n    color = \"#ff0000\", \n    line = list(\n      color = \"#444\", \n      width = 1\n    ), \n    opacity = 0.5,\n    size = 5\n  )\n\n# convert bvn3 to a tibble, plot and add markers\nbvn3 |&gt;\n  as_tibble() |&gt;\n  plot_ly(x = ~x, y = ~y, z = ~z, marker = marker_style) |&gt;\n  add_markers()\n\n\n\n\n\n\n9.2.3 Faux\nAlternatively, you can use the package faux to generate any number of correlated variables. It also has a function for checking the parameters of your new simulated data (check_sim_stats()).\n\nbvn3 &lt;- rnorm_multi(\n  n = n, \n  vars = 3,\n  mu = mu, \n  sd = sd,\n  r = c(rho1_2, rho1_3, rho2_3),\n  varnames = c(\"x\", \"y\", \"z\")\n)\n\ncheck_sim_stats(bvn3)\n\n\n\n\nn\nvar\nx\ny\nz\nmean\nsd\n\n\n\n200\nx\n1.00\n0.53\n-0.01\n9.73\n7.59\n\n\n200\ny\n0.53\n1.00\n0.70\n18.75\n9.16\n\n\n200\nz\n-0.01\n0.70\n1.00\n28.08\n10.14\n\n\n\n\n\n\nYou can also use faux to simulate data for factorial designs. Set up the between-subject and within-subject factors as lists with the levels as (named) vectors. Means and standard deviations can be included as vectors or data frames. The function calculates sigma for you, structures your dataset, and outputs a plot of the design.\n\nb &lt;- list(pet = c(cat = \"Cat Owners\",\n                  dog = \"Dog Owners\"))\nw &lt;- list(time = c(\"morning\",\n                   \"noon\",\n                   \"night\"))\nmu &lt;- data.frame(\n  cat    = c(10, 12, 14),\n  dog    = c(10, 15, 20),\n  row.names = w$time\n)\nsd &lt;- c(3, 3, 3, 5, 5, 5)\n\npet_data &lt;- sim_design(\n  within = w, \n  between = b,\n  n = 100, \n  mu = mu,\n  sd = sd, \n  r = .5)\n\n\n\n\n\n\n\nYou can use the check_sim_stats() function, but you need to set the argument between to a vector of all the between-subject factor columns.\n\ncheck_sim_stats(pet_data, between = \"pet\")\n\n\n\n\npet\nn\nvar\nmorning\nnoon\nnight\nmean\nsd\n\n\n\ncat\n100\nmorning\n1.00\n0.51\n0.50\n10.09\n2.83\n\n\ncat\n100\nnoon\n0.51\n1.00\n0.52\n12.13\n3.33\n\n\ncat\n100\nnight\n0.50\n0.52\n1.00\n14.14\n3.09\n\n\ndog\n100\nmorning\n1.00\n0.52\n0.49\n9.57\n5.48\n\n\ndog\n100\nnoon\n0.52\n1.00\n0.53\n15.32\n5.13\n\n\ndog\n100\nnight\n0.49\n0.53\n1.00\n19.54\n5.09\n\n\n\n\n\n\nSee the faux website for more detailed tutorials.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "09-sim.html#stat-terms",
    "href": "09-sim.html#stat-terms",
    "title": "9  Probability & Simulation",
    "section": "\n9.3 Statistical terms",
    "text": "9.3 Statistical terms\nLet’s review some important statistical terms before we review tests of distributions.\n\n9.3.1 Effect\nThe effect is some measure of your data. This will depend on the type of data you have and the type of statistical test you are using. For example, if you flipped a coin 100 times and it landed heads 66 times, the effect would be 66/100. You can then use the exact binomial test to compare this effect to the null effect you would expect from a fair coin (50/100) or to any other effect you choose. The effect size refers to the difference between the effect in your data and the null effect (usually a chance value).\n\n\n\n\n9.3.2 P-value\nThe p-value of a test is the probability of seeing an effect at least as extreme as what you have, if the real effect was the value you are testing against (e.g., a null effect). So if you used a binomial test to test against a chance probability of 1/6 (e.g., the probability of rolling 1 with a 6-sided die), then a p-value of 0.17 means that you could expect to see effects at least as extreme as your data 17% of the time just by chance alone.\n\n9.3.3 Alpha\nIf you are using null hypothesis significance testing (NHST), then you need to decide on a cutoff value (alpha) for making a decision to reject the null hypothesis. We call p-values below the alpha cutoff significant. In psychology, alpha is traditionally set at 0.05, but there are good arguments for setting a different criterion in some circumstances.\n\n9.3.4 False Positive/Negative\nThe probability that a test concludes there is an effect when there is really no effect (e.g., concludes a fair coin is biased) is called the false positive rate (or Type I Error Rate). The alpha is the false positive rate we accept for a test. The probability that a test concludes there is no effect when there really is one (e.g., concludes a biased coin is fair) is called the false negative rate (or Type II Error Rate). The beta is the false negative rate we accept for a test.\n\nThe false positive rate is not the overall probability of getting a false positive, but the probability of a false positive under the null hypothesis. Similarly, the false negative rate is the probability of a false negative under the alternative hypothesis. Unless we know the probability that we are testing a null effect, we can’t say anything about the overall probability of false positives or negatives. If 100% of the hypotheses we test are false, then all significant effects are false positives, but if all of the hypotheses we test are true, then all of the positives are true positives and the overall false positive rate is 0.\n\n\n9.3.5 Power and SESOI\nPower is equal to 1 minus beta (i.e., the true positive rate), and depends on the effect size, how many samples we take (n), and what we set alpha to. For any test, if you specify all but one of these values, you can calculate the last. The effect size you use in power calculations should be the smallest effect size of interest (SESOI). See Daniël Lakens et al. (2018) for a tutorial on methods for choosing an SESOI.\n\nLet’s say you want to be able to detect at least a 15% difference from chance (50%) in a coin’s fairness, and you want your test to have a 5% chance of false positives and a 10% chance of false negatives. What are the following values?\n\nalpha = \n\nbeta = \n\nfalse positive rate = \n\nfalse negative rate = \n\npower = \n\nSESOI = \n\n\n\n\n9.3.6 Confidence Intervals\nThe confidence interval is a range around some value (such as a mean) that has some probability of containing the parameter, if you repeated the process many times. Traditionally in psychology, we use 95% confidence intervals, but you can calculate CIs for any percentage.\n\nA 95% CI does not mean that there is a 95% probability that the true mean lies within this range, but that, if you repeated the study many times and calculated the CI this same way every time, you’d expect the true mean to be inside the CI in 95% of the studies. This seems like a subtle distinction, but can lead to some misunderstandings. See Morey et al. (2016) for more detailed discussion.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "09-sim.html#tests",
    "href": "09-sim.html#tests",
    "title": "9  Probability & Simulation",
    "section": "\n9.4 Tests",
    "text": "9.4 Tests\n\n9.4.1 Exact binomial test\nbinom.test(x, n, p)\nYou can test a binomial distribution against a specific probability using the exact binomial test.\n\n\nx = the number of successes\n\nn = the number of trials\n\np = hypothesised probability of success\n\nHere we can test a series of 10 coin flips from a fair coin and a biased coin against the hypothesised probability of 0.5 (even odds).\n\nn &lt;- 10\nfair_coin &lt;- rbinom(1, n, 0.5)\nbiased_coin &lt;- rbinom(1, n, 0.6)\n\nbinom.test(fair_coin, n, p = 0.5)\nbinom.test(biased_coin, n, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  fair_coin and n\nnumber of successes = 8, number of trials = 10, p-value = 0.1094\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4439045 0.9747893\nsample estimates:\nprobability of success \n                   0.8 \n\n\n    Exact binomial test\n\ndata:  biased_coin and n\nnumber of successes = 7, number of trials = 10, p-value = 0.3438\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.3475471 0.9332605\nsample estimates:\nprobability of success \n                   0.7 \n\n\n\nRun the code above several times, noting the p-values for the fair and biased coins. Alternatively, you can simulate coin flips online and build up a graph of results and p-values.\n\nHow does the p-value vary for the fair and biased coins?\nWhat happens to the confidence intervals if you increase n from 10 to 100?\nWhat criterion would you use to tell if the observed data indicate the coin is fair or biased?\nHow often do you conclude the fair coin is biased (false positives)?\nHow often do you conclude the biased coin is fair (false negatives)?\n\n\n\n9.4.1.1 Sampling function\nTo estimate these rates, we need to repeat the sampling above many times. A function is ideal for repeating the exact same procedure over and over. Set the arguments of the function to variables that you might want to change. Here, we will want to estimate power for:\n\ndifferent sample sizes (n)\ndifferent effects (bias)\ndifferent hypothesised probabilities (p, defaults to 0.5)\n\n\nsim_binom_test &lt;- function(n, bias, p = 0.5) {\n  # simulate 1 coin flip n times with the specified bias\n  coin &lt;- rbinom(1, n, bias)\n  # run a binomial test on the simulated data for the specified p\n  btest &lt;- binom.test(coin, n, p)\n  # return the p-value of this test\n  btest$p.value\n}\n\nOnce you’ve created your function, test it a few times, changing the values.\n\nsim_binom_test(100, 0.6)\n\n[1] 0.08862608\n\n\n\n9.4.1.2 Calculate power\nThen you can use the replicate() function to run it many times and save all the output values. You can calculate the power of your analysis by checking the proportion of your simulated analyses that have a p-value less than your alpha (the probability of rejecting the null hypothesis when the null hypothesis is true).\n\nmy_reps &lt;- replicate(1e4, sim_binom_test(100, 0.6))\n\nalpha &lt;- 0.05 # this does not always have to be 0.05\n\nmean(my_reps &lt; alpha)\n\n[1] 0.456\n\n\n\n1e4 is just scientific notation for a 1 followed by 4 zeros (10000). When you’re running simulations, you usually want to run a lot of them. It’s a pain to keep track of whether you’ve typed 5 or 6 zeros (100000 vs 1000000) and this will change your running time by an order of magnitude.\n\nYou can plot the distribution of p-values.\n\nggplot() + \n  geom_histogram(\n    aes(my_reps), \n    binwidth = 0.05, \n    boundary = 0,\n    fill = \"white\", \n    color = \"black\"\n  )\n\n\n\n\n\n\n\n\n9.4.2 T-test\nt.test(x, y, alternative, mu, paired)\nUse a t-test to compare the mean of one distribution to a null hypothesis (one-sample t-test), compare the means of two samples (independent-samples t-test), or compare pairs of values (paired-samples t-test).\nYou can run a one-sample t-test comparing the mean of your data to mu. Here is a simulated distribution with a mean of 0.5 and an SD of 1, creating an effect size of 0.5 SD when tested against a mu of 0. Run the simulation a few times to see how often the t-test returns a significant p-value (or run it in the shiny app).\n\nsim_norm &lt;- rnorm(100, 0.5, 1)\nt.test(sim_norm, mu = 0)\n\n\n    One Sample t-test\n\ndata:  sim_norm\nt = 4.004, df = 99, p-value = 0.0001205\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.2148352 0.6369382\nsample estimates:\nmean of x \n0.4258867 \n\n\nRun an independent-samples t-test by comparing two lists of values.\n\na &lt;- rnorm(100, 0.5, 1)\nb &lt;- rnorm(100, 0.7, 1)\nt_ind &lt;- t.test(a, b, paired = FALSE)\nt_ind\n\n\n    Welch Two Sample t-test\n\ndata:  a and b\nt = -1.779, df = 197.29, p-value = 0.07678\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.53382988  0.02747648\nsample estimates:\nmean of x mean of y \n0.5073348 0.7605115 \n\n\n\nThe paired argument defaults to FALSE, but it’s good practice to always explicitly set it so you are never confused about what type of test you are performing.\n\n\n9.4.2.1 Sampling function\nWe can use the names() function to find out the names of all the t.test parameters and use this to just get one type of data, like the test statistic (e.g., t-value).\n\nnames(t_ind)\nt_ind$statistic\n\n [1] \"statistic\"   \"parameter\"   \"p.value\"     \"conf.int\"    \"estimate\"   \n [6] \"null.value\"  \"stderr\"      \"alternative\" \"method\"      \"data.name\"  \n        t \n-1.778993 \n\n\nIf you want to run the simulation many times and record information each time, first you need to turn your simulation into a function.\n\nsim_t_ind &lt;- function(n, m1, sd1, m2, sd2) {\n  # simulate v1\n  v1 &lt;- rnorm(n, m1, sd1)\n  \n  #simulate v2\n  v2 &lt;- rnorm(n, m2, sd2)\n    \n  # compare using an independent samples t-test\n  t_ind &lt;- t.test(v1, v2, paired = FALSE)\n  \n  # return the p-value\n  return(t_ind$p.value)\n}\n\nRun it a few times to check that it gives you sensible values.\n\nsim_t_ind(100, 0.7, 1, 0.5, 1)\n\n[1] 0.007540945\n\n\n\n9.4.2.2 Calculate power\nNow replicate the simulation 1000 times.\n\nmy_reps &lt;- replicate(1e4, sim_t_ind(100, 0.7, 1, 0.5, 1))\n\nalpha &lt;- 0.05\npower &lt;- mean(my_reps &lt; alpha)\npower\n\n[1] 0.2929\n\n\n\nRun the code above several times. How much does the power value fluctuate? How many replications do you need to run to get a reliable estimate of power?\n\nCompare your power estimate from simluation to a power calculation using power.t.test(). Here, delta is the difference between m1 and m2 above.\n\npower.t.test(n = 100, \n             delta = 0.2, \n             sd = 1, \n             sig.level = alpha, \n             type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 100\n          delta = 0.2\n             sd = 1\n      sig.level = 0.05\n          power = 0.2902664\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nYou can plot the distribution of p-values.\n\nggplot() + \n  geom_histogram(\n    aes(my_reps), \n    binwidth = 0.05, \n    boundary = 0,\n    fill = \"white\", \n    color = \"black\"\n  )\n\n\n\n\n\n\n\n\nWhat do you think the distribution of p-values is when there is no effect (i.e., the means are identical)? Check this yourself.\n\n\nMake sure the boundary argument is set to 0 for p-value histograms. See what happens with a null effect if boundary is not set.\n\n\n9.4.3 Correlation\nYou can test if continuous variables are related to each other using the cor() function. Let’s use rnorm_multi() to make a quick table of correlated values.\n\ndat &lt;- rnorm_multi(\n  n = 100, \n  vars = 2, \n  r = -0.5,\n  varnames = c(\"x\", \"y\")\n)\n\ncor(dat$x, dat$y)\n\n[1] -0.5207637\n\n\n\nSet n to a large number like 1e6 so that the correlations are less affected by chance. Change the value of the mean for a, x, or y. Does it change the correlation between x and y? What happens when you increase or decrease the sd? Can you work out any rules here?\n\ncor() defaults to Pearson’s correlations. Set the method argument to use Kendall or Spearman correlations.\n\ncor(dat$x, dat$y, method = \"spearman\")\n\n[1] -0.4257186\n\n\n\n9.4.3.1 Sampling function\nCreate a function that creates two variables with n observations and r correlation. Use the function cor.test() to give you p-values for the correlation.\n\nsim_cor_test &lt;- function(n = 100, r = 0) {\n  dat &lt;- rnorm_multi(\n    n = n, \n    vars = 2, \n    r = r,\n    varnames = c(\"x\", \"y\")\n  )\n\n  ctest &lt;- cor.test(dat$x, dat$y)\n  ctest$p.value\n}\n\nOnce you’ve created your function, test it a few times, changing the values.\n\nsim_cor_test(50, .5)\n\n[1] 1.89748e-11\n\n\n\n9.4.3.2 Calculate power\nNow replicate the simulation 1000 times.\n\nmy_reps &lt;- replicate(1e4, sim_cor_test(50, 0.5))\n\nalpha &lt;- 0.05\npower &lt;- mean(my_reps &lt; alpha)\npower\n\n[1] 0.9673\n\n\nCompare to the value calcuated by the pwr package.\n\npwr::pwr.r.test(n = 50, r = 0.5)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 50\n              r = 0.5\n      sig.level = 0.05\n          power = 0.9669813\n    alternative = two.sided",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "09-sim.html#example",
    "href": "09-sim.html#example",
    "title": "9  Probability & Simulation",
    "section": "\n9.5 Example",
    "text": "9.5 Example\nThis example uses the Growth Chart Data Tables from the US CDC. The data consist of height in centimeters for the z-scores of –2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, and 2 by sex (1=male; 2=female) and half-month of age (from 24.0 to 240.5 months).\n\n9.5.1 Load & wrangle\nWe have to do a little data wrangling first. Have a look at the data after you import it and relabel Sex to male and female instead of 1 and 2. Also convert Agemos (age in months) to years. Relabel the column 0 as mean and calculate a new column named sd as the difference between columns 1 and 0.\n\norig_height_age &lt;- read_csv(\"https://www.cdc.gov/growthcharts/data/zscore/zstatage.csv\") \n\nheight_age &lt;- orig_height_age |&gt;\n  filter(Sex %in% c(1,2)) |&gt;\n  mutate(\n    sex = recode(Sex, \"1\" = \"male\", \"2\" = \"female\"),\n    age = as.numeric(Agemos)/12,\n    sd = `1` - `0`\n  ) |&gt;\n  select(sex, age, mean = `0`, sd)\n\n\n9.5.2 Plot\nPlot your new data frame to see how mean height changes with age for boys and girls.\n\nggplot(height_age, aes(age, mean, color = sex)) +\n  geom_smooth(aes(ymin = mean - sd, \n                  ymax = mean + sd),\n              stat=\"identity\")\n\n\n\n\n\n\n\n\n9.5.3 Simulate a population\nSimulate 50 random male heights and 50 random female heights for 20-year-olds using the rnorm() function and the means and SDs from the height_age table. Plot the data.\n\nage_filter &lt;- 20\nm &lt;- filter(height_age, age == age_filter, sex == \"male\")\nf &lt;- filter(height_age, age == age_filter, sex == \"female\")\n\nsim_height &lt;- tibble(\n  male = rnorm(50, m$mean, m$sd),\n  female = rnorm(50, f$mean, f$sd)\n) |&gt;\n  gather(\"sex\", \"height\", male:female)\n\nggplot(sim_height) +\n  geom_density(aes(height, fill = sex), alpha = 0.5) +\n  xlim(125, 225)\n\n\n\n\n\n\n\n\nRun the simulation above several times, noting how the density plot changes. Try changing the age you’re simulating.\n\n\n9.5.4 Analyse simulated data\nUse the sim_t_ind(n, m1, sd1, m2, sd2) function we created above to generate one simulation with a sample size of 50 in each group using the means and SDs of male and female 14-year-olds.\n\nage_filter &lt;- 14\nm &lt;- filter(height_age, age == age_filter, sex == \"male\")\nf &lt;- filter(height_age, age == age_filter, sex == \"female\")\n\nsim_t_ind(50, m$mean, m$sd, f$mean, f$sd)\n\n[1] 0.05431407\n\n\n\n9.5.5 Replicate simulation\nNow replicate this 1e4 times using the replicate() function. This function will save the returned p-values in a list (my_reps). We can then check what proportion of those p-values are less than our alpha value. This is the power of our test.\n\nmy_reps &lt;- replicate(1e4, sim_t_ind(50, m$mean, m$sd, f$mean, f$sd))\n\nalpha &lt;- 0.05\npower &lt;- mean(my_reps &lt; alpha)\npower\n\n[1] 0.653\n\n\n\n9.5.6 One-tailed prediction\nThis design has about 65% power to detect the sex difference in height (with a 2-tailed test). Modify the sim_t_ind function for a 1-tailed prediction.\nYou could just set alternative equal to “greater” in the function, but it might be better to add the alt argument to your function (giving it the same default value as t.test) and change the value of alternative in the function to alt.\n\nsim_t_ind &lt;- function(n, m1, sd1, m2, sd2, alt = \"two.sided\") {\n  v1 &lt;- rnorm(n, m1, sd1)\n  v2 &lt;- rnorm(n, m2, sd2)\n  t_ind &lt;- t.test(v1, v2, paired = FALSE, alternative = alt)\n  \n  return(t_ind$p.value)\n}\n\nalpha &lt;- 0.05\nmy_reps &lt;- replicate(1e4, sim_t_ind(50, m$mean, m$sd, f$mean, f$sd, \"greater\"))\nmean(my_reps &lt; alpha)\n\n[1] 0.7588\n\n\n\n9.5.7 Range of sample sizes\nWhat if we want to find out what sample size will give us 80% power? We can try trial and error. We know the number should be slightly larger than 50. But you can search more systematically by repeating your power calculation for a range of sample sizes.\n\nThis might seem like overkill for a t-test, where you can easily look up sample size calculators online, but it is a valuable skill to learn for when your analyses become more complicated.\n\nStart with a relatively low number of replications and/or more spread-out samples to estimate where you should be looking more specifically. Then you can repeat with a narrower/denser range of sample sizes and more iterations.\n\n# make another custom function to return power\npwr_func &lt;- function(n, reps = 100, alpha = 0.05) {\n  ps &lt;- replicate(reps, sim_t_ind(n, m$mean, m$sd, f$mean, f$sd, \"greater\"))\n  mean(ps &lt; alpha)\n}\n\n# make a table of the n values you want to check\npower_table &lt;- tibble(\n  n = seq(20, 100, by = 5)\n) |&gt;\n  # run the power function for each n\n  mutate(power = map_dbl(n, pwr_func))\n\n# plot the results\nggplot(power_table, aes(n, power)) +\n  geom_smooth() +\n  geom_point() +\n  geom_hline(yintercept = 0.8)\n\n\n\n\n\n\n\nNow we can narrow down our search to values around 55 (plus or minus 5) and increase the number of replications from 1e3 to 1e4.\n\npower_table &lt;- tibble(\n  n = seq(50, 60)\n) |&gt;\n  mutate(power = map_dbl(n, pwr_func, reps = 1e4))\n\nggplot(power_table, aes(n, power)) +\n geom_smooth() +\n geom_point() +\n geom_hline(yintercept = 0.8)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "09-sim.html#sec-glossary-sim",
    "href": "09-sim.html#sec-glossary-sim",
    "title": "9  Probability & Simulation",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\nterm\ndefinition\n\n\n\nalpha\n(stats) The cutoff value for making a decision to reject the null hypothesis; (graphics) A value between 0 and 1 used to control the levels of transparency in a plot\n\n\nbeta\nThe false negative rate we accept for a statistical test.\n\n\nbinomial-distribution\nThe distribution of data where each observation can have one of two outcomes, like success/failure, yes/no or head/tails.\n\n\nbivariate-normal\nTwo normally distributed vectors that have a specified correlation with each other.\n\n\nconfidence-interval\nA type of interval estimate used to summarise a given statistic or measurement where a proportion of intervals calculated from the sample(s) will contain the true value of the statistic.\n\n\ncorrelation\nThe relationship two vectors have to each other.\n\n\ncovariance-matrix\nParameters showing how a set of vectors vary and covary.\n\n\ndiscrete\nData that can only take certain values, such as integers.\n\n\neffect\nSome measure of your data, such as the mean value, or the number of standard deviations the mean differs from a chance value.\n\n\neffect-size\nThe difference between the effect in your data and the null effect (usually a chance value)\n\n\nfalse-negative\nWhen a test concludes there is no effect when there really is an effect\n\n\nfalse-positive\nWhen a test concludes there is an effect when there really is no effect\n\n\nfunction\nA named section of code that can be reused.\n\n\nnhst\nNull Hypothesis Signficance Testing\n\n\nnormal-distribution\nA symmetric distribution of data where values near the centre are most probable.\n\n\nnull-effect\nAn outcome that does not show an otherwise expected effect.\n\n\np-value\nThe probability of seeing an effect at least as extreme as what you have, if the real effect was the value you are testing against (e.g., a null effect)\n\n\nparameter\nA quantity characterizing a population.\n\n\npoisson-distribution\nA distribution that models independent events happening over a unit of time\n\n\npower\nThe probability of rejecting the null hypothesis when it is false.\n\n\npower\nThe probability of rejecting the null hypothesis when it is false.\n\n\nprobability\nA number between 0 and 1 where 0 indicates impossibility of the event and 1 indicates certainty\n\n\nsesoi\nSmallest Effect Size of Interest: the smallest effect that is theoretically or practically meaningful\n\n\nsignificant\nThe conclusion when the p-value is less than the critical alpha.\n\n\nsimulation\nGenerating data from summary parameters\n\n\ntrue-positive\nWhen a test concludes there is an effect when there is really is an effect\n\n\ntype-i-error\nA false positive; When a test concludes there is an effect when there really is no effect\n\n\ntype-ii-error\nA false negative; When a test concludes there is no effect when there really is an effect\n\n\nuniform-distribution\nA distribution where all numbers in the range have an equal probability of being sampled\n\n\nunivariate\nRelating to a single variable.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "09-sim.html#sec-resources-sim",
    "href": "09-sim.html#sec-resources-sim",
    "title": "9  Probability & Simulation",
    "section": "Further Resources",
    "text": "Further Resources\n\n\nDistribution Shiny App (or run reprores::app(\"simulate\")\n\n\nSimulation tutorials by Lisa DeBruine\n\nChapter 21: Iteration of R for Data Science\n\n\nImproving your statistical inferences on Coursera (week 1)\n\nFaux package for data simulation\n\nSimulation-Based Power-Analysis for Factorial ANOVA Designs (Daniel Lakens & Caldwell, 2019)\n\n\nUnderstanding mixed effects models through data simulation (DeBruine & Barr, 2019)\n\n\n\n\n\n\nDeBruine, L. M., & Barr, D. J. (2019). Understanding mixed effects models through data simulation. https://doi.org/10.31234/osf.io/xp5cy\n\n\nLakens, Daniel, & Caldwell, A. R. (2019). Simulation-based power-analysis for factorial ANOVA designs. https://doi.org/10.31234/osf.io/baxsf\n\n\nLakens, Daniël, Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963\n\n\nMorey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The fallacy of placing confidence in confidence intervals. Psychonomic Bulletin & Review, 23(1), 103–123. https://doi.org/10.3758/s13423-015-0947-8",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability & Simulation</span>"
    ]
  },
  {
    "objectID": "10-next.html",
    "href": "10-next.html",
    "title": "10  Next Steps",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Next Steps</span>"
    ]
  },
  {
    "objectID": "10-next.html#sec-ilo-next",
    "href": "10-next.html#sec-ilo-next",
    "title": "10  Next Steps",
    "section": "",
    "text": "Create and customise advanced types of plots\nStructure data in report, presentation, and dashboard formats\nBe aware of the ways to continue developing computational reproducibility skills",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Next Steps</span>"
    ]
  },
  {
    "objectID": "10-next.html#sec-custom-viz",
    "href": "10-next.html#sec-custom-viz",
    "title": "10  Next Steps",
    "section": "\n10.1 Visualisation",
    "text": "10.1 Visualisation\n\n10.1.1 Set-up\n\nOpen your reprores project\nCreate a new quarto file called 10-viz.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\n\n# packages needed for this chapter section\nlibrary(tidyverse)   # for data wrangling\nlibrary(ggthemes)    # for themes\nlibrary(patchwork)   # for combining plots\nlibrary(plotly)      # for interactive plots\n# devtools::install_github(\"hrbrmstr/waffle\")\nlibrary(waffle)      # for waffle plots\nlibrary(ggbump)      # for bump plots\nlibrary(treemap)     # for treemap plots\nlibrary(ggwordcloud) # for word clouds\nlibrary(tidytext)    # for manipulating text for word clouds\nlibrary(gganimate)   # for animated plots\n\n#install.packages(\"rnaturalearthhires\", repos = \"http://packages.ropensci.org\", type = \"source\")\nlibrary(sf)          # for mapping geoms\nlibrary(rnaturalearth) # for map data\nlibrary(rnaturalearthdata) # extra mapping data\n\ntheme_set(theme_light())\n```\n\nDownload the ggplot2 cheat sheet.\n\n10.1.2 Defaults\nThe code below creates two plots using the default (light) theme and palettes. First, load the data and set issue_category to a factor so you can control the order of the categories.\n\n# update column specification\nct &lt;- cols(issue_category = col_factor(\n        levels = c(\"tech\", \"returns\", \"sales\", \"other\")\n      ))\n\n# load data\nsurvey_data &lt;- read_csv(file = \"data/survey_data.csv\",\n                        col_types = ct)\n\nNext, create a bar plot of number of calls by issue category.\n\n# create bar plot\nbar &lt;- ggplot(data = survey_data, \n              mapping = aes(x = issue_category,\n                            fill = issue_category)) +\n  geom_bar(show.legend = FALSE) +\n  labs(x = \"Issue Category\", \n       y = \"Count\",\n       title = \"Calls by Issue Category\")\n\nAnd create a scatterplot of wait time by call time, distinguished by issue category.\n\n#create scatterplot\npoint &lt;- ggplot(data = survey_data, \n                mapping = aes(x = wait_time, \n                              y = call_time,\n                              color = issue_category)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = lm, formula = y~x) +\n  labs(x = \"Wait Time\",\n       y = \"Call Time\",\n       color = \"Issue Category\",\n       title = \"Wait Time by Call Time\")\n\nFinally, combine the two plots using the + from patchwork to see the default styles for these plots.\n\nbar + point\n\n\n\nDefault plot styles.\n\n\n\n\nTry changing the theme using built-in themes or customising the colours or linetypes with scale_* functions. See Appendix @ref(plotstyle) for details.\n\n\n10.1.3 Annotations\nIt’s often useful to add annotations to a plot, for example, to highlight an important part of the plot or add labels. The annotate() function creates a specific geom at x- and y-coordinates you specify.\n\n10.1.3.1 Text annotations\nAdd a text annotation by setting the geom argument to “text” or “label” and adding a label. Labels have padding and a background, while text is just text.\n\nBackslash-n \\n in the label text controls where the line breaks are. Try removing or changing the position of these to see what happens.\n\nx and y control the coordinates of the label. You will likely have to play around with these values to get them right.\nThe argument hjust is the horizontal justification of text, and vjust is the vertical justification. The default values are 0.5, where the text is centred on the x and y coordinates. 0 will justify to the left and bottom, while 1 justifies to the right and top.\nYou can change the angle of text, but not labels.\n\n\nbar +\n  # add left-justified text to the second bar\n  annotate(geom = \"text\",\n           label = \"Our goal is to\\nreduce this\\ncategory\",\n           x = 1.65, y = 150,\n           hjust = 0, vjust = 1, \n           color = \"white\", fontface = \"bold\",\n           angle = 45) +\n  # add a centred label to the third bar\n  annotate(geom = \"label\",\n           label = \"Our goal is\\nto increase this\\ncategory\",\n           x = 3, y = 75,\n           hjust = 0.5, vjust = 1, \n           color = \" darkturquoise\", fontface = \"bold\")\n\n\n\nAn example of annotation text and label.\n\n\n\n\nSee if you can work out how to make the figure below, starting with the following:\n\ntibble(x = c(0, 0, 1, 1),\n       y = c(0, 1, 0, 1)) |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.25, size = 4, color = \"red\")\n\nHint: you will need to add 1 label annotation and 8 separate text annotations.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\ntibble(x = c(0, 0, 1, 1),\n       y = c(0, 1, 0, 1)) |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.25, size = 4, color = \"red\") +\n  annotate(\"label\", label = \"In the\\nmiddle\",\n           x = 0.5, y = 0.5,\n           fill = \"dodgerblue\", color = \"white\",\n           label.padding = unit(1, \"lines\"),\n           label.r = unit(1.5, \"lines\")) +\n  annotate(\"text\", label = \"Bottom\\nLeft\",\n           x = 0, y = 0, hjust = 0, vjust = 0) +\n  annotate(\"text\", label = \"Top\\nLeft\", \n           x = 0, y = 1, hjust = 0, vjust = 1) +\n  annotate(\"text\", label = \"Bottom\\nRight\",\n           x = 1, y = 0, hjust = 1, vjust = 0) +\n  annotate(\"text\", label = \"Top\\nRight\",\n           x = 1, y = 1, hjust = 1, vjust = 1) +\n  annotate(\"text\", label = \"45 degrees\",\n           x = 0, y = 0.5, hjust = 0, angle = 45) +\n  annotate(\"text\", label = \"90 degrees\",\n           x = 0.25, y = 0.5, angle = 90) +\n  annotate(\"text\", label = \"270 degrees\",\n           x = 0.75, y = 0.5, angle = 270)+\n  annotate(\"text\", label = \"-45 degrees\",\n           x = 1, y = 0.5, hjust = 1, angle = -45)\n\n\n\n\n10.1.3.2 Other annotations\nYou can add other geoms to highlight parts of a plot. The example below adds a rectangle around a group of points, a text label, a straight arrow from the label to the rectangle, and a curved arrow from the label to an individual point.\n\npoint +\n  # add a rectangle surrounding long call times\n  annotate(geom = \"rect\",\n           xmin = 100, xmax = 275,\n           ymin = 140, ymax = 180,\n           fill = \"transparent\", color = \"red\") +\n  # add a text label\n  annotate(\"text\",\n           x = 260, y = 120,\n           label = \"outliers\") +\n  # add an line with an arrow from the text to the box\n  annotate(geom = \"segment\", \n           x = 240, y = 120, \n           xend = 200, yend = 135,\n           arrow = arrow(length = unit(0.5, \"lines\"))) +\n  # add a curved line with an arrow \n  # from the text to a wait time outlier\n  annotate(geom = \"curve\", \n          x = 280, y = 120, \n          xend = 320, yend = 45,\n          curvature = -0.5,\n          arrow = arrow(length = unit(0.5, \"lines\")))\n\n\n\nExample of annotatins with the rect, text, segment, and curve geoms.\n\n\n\nSee the ggforce package for more sophisticated options, such as highlighting a group of points with an ellipse.\n\n10.1.4 Other Plots\n\n10.1.4.1 Interactive Plots\nThe plotly package can be used to make interactive graphs. Assign your ggplot to a variable and then use the function ggplotly() on the plot object. Note that interactive plots only work in HTML files, not PDFs or Word files.\n\nggplotly(point)\n\n\nInteractive graph using plotly\n\n\n\nHover over the data points above and click on the legend items.\n\n\n10.1.4.2 Waffle Plots\nIn Chapter @ref(ggplot), we mentioned that pie charts are such a poor way to visualise proportions that we refused to even show you how to make one. Waffle plots are a delicious alternative.\n\nUse waffle by hrbrmstr on GitHub using the install_github() function below, rather than the one on CRAN you get from using install.packages().\n\ndevtools::install_github(\"hrbrmstr/waffle\")\n\n\nBy default, geom_waffle() represents each observation with a tile and splits these across 10 rows. You can play about with the n_rows argument to determine what works best for your data.\n\nsurvey_data |&gt; \n  count(issue_category) |&gt;\n  ggplot(aes(fill = issue_category, values = n)) +\n  geom_waffle(\n    n_rows = 23, # try setting this to 10 (the default)\n    size = 0.33, # line size\n    make_proportional = FALSE, # use raw values\n    colour = \"white\", # line colour\n    flip = FALSE, # bottom-top or left-right\n    radius = grid::unit(0.1, \"npc\") # set to 0.5 for circles\n  ) +\n  theme_enhance_waffle() + # gets rid of axes\n  scale_fill_colorblind(name = \"Issue Category\")\n\n\n\nWaffle plot.\n\n\n\nThe waffle plot can also be used to display the counts as proportions To achieve these, set n_rows = 10 and make_proportional = TRUE. Now, rather than each tile representing one observation, each tile represents 1% of the data.\n\nsurvey_data |&gt; \n  count(issue_category) |&gt;\n  ggplot(aes(fill = issue_category, values = n)) +\n  geom_waffle(\n    n_rows = 10, \n    size = 0.33, \n    make_proportional = TRUE, # compute proportions\n    colour = \"white\", \n    flip = FALSE, \n    radius = grid::unit(0.1, \"npc\") \n  ) +\n  theme_enhance_waffle() + \n  scale_fill_colorblind(name = \"Issue Category\")\n\n\n\nProportional waffle plot.\n\n\n\n\n10.1.4.3 Treemap\nTreemap plots are another way to visualise proportions. Like the waffle plots, you need to count the data by category first. You can use any brewer palette for the fill.\n\nsurvey_data |&gt; \n  count(issue_category) |&gt;\n  treemap(\n    index = \"issue_category\", # column with number of rectangles\n    vSize = \"n\", # column with size of rectangle\n    title = \"\",\n    palette = \"BuPu\",\n    inflate.labels = TRUE # expand labels to size of rectangle\n  )\n\n\n\nTreemap plot.\n\n\n\nYou can also represent multiple categories with treemaps\n\nsurvey_data |&gt; \n  count(issue_category, employee_id) |&gt;\n  arrange(employee_id) |&gt;\n  treemap(\n    # use c() to specify two variables\n    index = c(\"employee_id\", \"issue_category\"), \n    vSize = \"n\", \n    title = \"\",\n    palette = \"Dark2\",\n    # set different label sizes for each type of label\n    fontsize.labels = c(30, 10), \n    # set different alignments for two label types\n    align.labels = list(c(\"left\", \"top\"), c(\"center\", \"center\")) \n  )\n\n\n\nTreemap with two variables\n\n\n\n\n10.1.4.4 Bump Plots\nBump plots are very useful for visualising how rankings change over time. So first, we need to get some ranking data. Let’s start with a more typical raw data table, containing an identifying column of person and three columns for their scores each week\n\n# make a small dataset of scores for 3 people over 3 weeks\nscore_data &lt;- tribble(\n  ~person, ~week_1, ~week_2, ~week_3,\n  \"Abeni\",      80,     75,       90,\n  \"Beth\",       75,     85,       75,\n  \"Carmen\",     60,     70,       80\n)\n\nNow we make the table long, group by week, and use the rank() function to find the rank of each person’s score each week. Use n() - rank(score) + 1 to reverse the ranks so that the highest score gets rank 1. We also need to make the week variable a number.\n\n# calculate ranks\nrank_data &lt;- score_data |&gt;\n  pivot_longer(cols = -person,\n               names_to = \"week\",\n               values_to = \"score\") |&gt;\n  group_by(week) |&gt;\n  mutate(rank = n() - rank(score) + 1) |&gt;\n  ungroup() |&gt;\n  arrange(week, rank) |&gt;\n  mutate(week = str_replace(week, \"week_\", \"\") |&gt; as.integer())\n\nrank_data\n\n\n\n\nperson\nweek\nscore\nrank\n\n\n\nAbeni\n1\n80\n1\n\n\nBeth\n1\n75\n2\n\n\nCarmen\n1\n60\n3\n\n\nBeth\n2\n85\n1\n\n\nAbeni\n2\n75\n2\n\n\nCarmen\n2\n70\n3\n\n\nAbeni\n3\n90\n1\n\n\nCarmen\n3\n80\n2\n\n\nBeth\n3\n75\n3\n\n\n\n\n\n\nA typical mapping for a bump plot puts the time variable in the x-axis, the rank variable on the y-axis, and sets colour to the identifying variable.\n\nggplot(data = rank_data, \n       mapping = aes(x = week, \n                     y = rank, \n                     colour = person)) +\n  ggbump::geom_bump()\n\n\n\nBasic bump plot\n\n\n\nWe can make this more attractive by customising the axes and adding text labels. Try running each line of this code to see how it builds up.\n\nAdd label = person to the mapping so we can add in text labels.\nIncrease the size of the lines with the size argument to geom_bump()\n\nWe don’t need labels for weeks 1.5 and 2.5, so change the x-axis breaks\n\nThe expand argument for the two scale_ functions expands the plot area so we can fit text labels to the right.\nIt makes more sense to have first place at the top, so reverse the order of the y-axis with scale_y_reverse() and fix the breaks and expansion.\nAdd text labels with geom_text(), but just for week 3, so set data =  filter(rank_data, week == 3) for this geom.\nSet x = 3.05 to move the text labels just to the right of week 3, and set hjust = 0 to right-justify the text labels (the default is hjust = 0.5, which would center them on 3.05).\nRemove the legend and grid lines. Increase the x-axis text size.\n\n\nggplot(data = rank_data, \n       mapping = aes(x = week, \n                     y = rank, \n                     colour = person,\n                     label = person)) +\n  ggbump::geom_bump(size = 10) +\n  scale_x_continuous(name = \"\",\n                     breaks = 1:3, \n                     labels = c(\"Week 1\", \"Week 2\", \"Week 3\"),\n                     expand = expansion(c(.05, .2))) +\n  scale_y_reverse(name = \"Ranking\",\n                  breaks = 1:3, \n                  expand = expansion(.2)) +\n  geom_text(data = filter(rank_data, week == 3),\n            color = \"black\", x = 3.05, hjust = 0) +\n  theme(legend.position = \"none\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text.x = element_text(size = 12))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\nBump plot with added features.\n\n\n\n\n10.1.4.5 Word Clouds\nWord clouds are a common way to summarise text data. First, download amazon_alexa.csv into your data folder and then load it into an object. This dataset contains text reviews as well as the 1-5 rating from customers who bought an Alexa device on Amazon.\n\n# https://www.kaggle.com/sid321axn/amazon-alexa-reviews\n# extracted from Amazon by Manu Siddhartha & Anurag Bhatt\nalexa &lt;- rio::import(\"data/amazon_alexa.csv\")\n\nWe can use this data to look at how the words used differ depending on the rating given. To make the text data easy to work with, the function tidytext::unnest_tokens() splits the words in the input column into individual words in a new output column. unnnest_tokens() is particularly helpful because it also does things like removes punctuation and transforms all words to lower case to make it easier to work with. Compare words and alexa to see how they map on to each other.\n\nwords &lt;- alexa |&gt;\n  unnest_tokens(output = \"word\", input = \"verified_reviews\")\n\nWe can then add another line of code using a pipe that counts how many instances of each word there is by rating to give us the most popular words.\n\nwords &lt;- alexa |&gt;\n  unnest_tokens(output = \"word\", input = \"verified_reviews\") |&gt;\n  count(word, rating, sort = TRUE) \n\n\n\n\n\n\nword\nrating\nn\n\n\n\ni\n5\n1859\n\n\nthe\n5\n1839\n\n\nto\n5\n1633\n\n\nit\n5\n1571\n\n\nand\n5\n1477\n\n\nmy\n5\n980\n\n\n\n\n\n\nThe problem is that the most common words are all function words rather than content words, which makes sense because these words have the highest word frequency in natural language.\nHelpfully, tidytext contains a list of common “stop words”, i.e., words that you want to ignore, that are stored in an object named stop_words. It is also very useful to define a list of custom stop words based upon the unique properties of your data (it can sometimes take a few attempts to identify what’s appropriate for your dataset). This dataset contains a lot of numbers that aren’t informative, and it also contains “https” from website links, so we’ll get rid of both with a custom stop list.\nOnce you have defined your stop words, you can then use anti_join() to remove any word that is present in the stop word list.\nTo get the top 25 words, we then group by rating and use dplyr::slice_max(), ordered by the column n.\n\ncustom_stop &lt;- tibble(word = c(0:9, \"https\", 34))\n\nwords &lt;- alexa |&gt;\n  unnest_tokens(output = \"word\", input = \"verified_reviews\") |&gt;\n  count(word, rating) |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  anti_join(custom_stop, by = \"word\") |&gt;\n  group_by(rating) |&gt;\n  slice_max(order_by = n, n = 25, with_ties = FALSE) |&gt;\n  ungroup()\n\nFirst, let’s make a word cloud for customers who gave a 1-star rating:\n\nFilter retains only the data for 1-star ratings.\n\nlabel comes from the word column and is the data to plot (i.e., the words).\n\ncolour makes the words red (you could also set this to word to give each word a different colour or n to vary colour continuously by frequency).\n\nsize makes the size of the word proportional to n, the number of times the word appeared.\n\nggwordcloud::geom_text_wordcloud_area() is the word cloud geom.\n\nggwordcloud::scale_size_area() controls how big the word cloud is (this usually takes some trial-and-error).\n\n\nrating1 &lt;- filter(words, rating == 1) |&gt;\n  ggplot(aes(label = word, colour = \"red\", size = n)) +\n  geom_text_wordcloud_area() +\n  scale_size_area(max_size = 10) +\n  ggtitle(\"Rating = 1\") +\n  theme_minimal(base_size = 14)\n\nrating1\n\n\n\n\n\n\n\nWe can now do the same but for 5-star ratings and paste the plots together with patchwork (word clouds don’t play well with facets).\n\nrating5 &lt;- filter(words, rating == 5) |&gt;\n  ggplot(aes(label = word, size = n)) +\n  geom_text_wordcloud_area(colour = \"darkolivegreen3\") +\n  scale_size_area(max_size = 12) +\n  ggtitle(\"Rating = 5\") +\n  theme_minimal(base_size = 14)\n\nrating1 + rating5\n\n\n\nWord cloud.\n\n\n\n\nIt’s worth highlighting that whilst word clouds are very common, they’re really the equivalent of pie charts for text data because we’re not very good at making accurate comparisons based on size. You might be able to see what’s the most popular word, but can you accurately determine the 2nd, 3rd, 4th or 5th most popular word based on the clouds alone? There’s also the issue that just because it’s text data doesn’t make it a qualitative analysis and just because something is said a lot doesn’t mean it’s useful or important. But, this argument is outwith the scope of this book, even if it is a recurring part of Emily’s life thanks to her qualitative wife.\n\n\n10.1.4.6 Maps\nWorking with maps can be tricky. The sf package provides functions that work with ggplot2, such as geom_sf(). The rnaturalearth package (and associated data packages that you may be prompted to download) provide high-quality mapping coordinates.\n\n\nne_countries() returns world country polygons (i.e., a world map). We specify the object should be returned as a “simple feature” class sf so that it will work with geom_sf(). If you would like a deep dive on simple feature objects, check out a vignette from the sf package.\nIt’s worth checking out what the object ne_countries() returns to see just how much information is available.\nTry changing the values and colours below to get a sense of how the code works.\n\n\n# get the world map coordinates\nworld_sf &lt;- ne_countries(returnclass = \"sf\", scale = \"medium\")\n\n# plot them on a light blue background\nggplot() + \n  geom_sf(data = world_sf, size = 0.3) +\n  theme(panel.background = element_rect(fill = \"lightskyblue2\"))\n\n\n\n\n\n\n\nYou can combine multiple countries using bind_rows() and visualise them with different colours for each country.\n\n# get and bind country data\nuk_sf &lt;- ne_states(country = \"united kingdom\", returnclass = \"sf\")\nireland_sf &lt;- ne_states(country = \"ireland\", returnclass = \"sf\")\nislands &lt;- bind_rows(uk_sf, ireland_sf) |&gt;\n  filter(!is.na(geonunit))\n\n# set colours\ncountry_colours &lt;- c(\"Scotland\" = \"#0962BA\",\n                     \"Wales\" = \"#00AC48\",\n                     \"England\" = \"#FF0000\",\n                     \"Northern Ireland\" = \"#FFCD2C\",\n                     \"Ireland\" = \"#F77613\")\n\nggplot() + \n  geom_sf(data = islands,\n          mapping = aes(fill = geonunit),\n          colour = NA,\n          alpha = 0.75) +\n  coord_sf(crs = sf::st_crs(4326),\n           xlim = c(-10.7, 2.1), \n           ylim = c(49.7, 61)) +\n  scale_fill_manual(name = \"Country\", \n                    values = country_colours)\n\n\n\nMap coloured by country.\n\n\n\nYou can join Scottish population data to the map table to visualise data on the map using colours or labels.\n\n# load map data\nscotland_sf &lt;- ne_states(geounit = \"Scotland\", \n                         returnclass = \"sf\")\n\n# load population data from\n# https://www.indexmundi.com/facts/united-kingdom/quick-facts/scotland/population\nscotpop &lt;- read_csv(\"data/scottish_population.csv\", \n                    show_col_types = FALSE)\n\n# join data and fix typo in the map\nscotmap_pop &lt;- scotland_sf |&gt;\n  mutate(name = ifelse(name == \"North Ayshire\", \n                       yes = \"North Ayrshire\", \n                       no = name)) |&gt;\n  left_join(scotpop, by = \"name\") |&gt;\n  select(name, population, geometry)\n\n\nThere is a typo in the data from rnaturalearth, so you need to change “North Ayshire” to “North Ayrshire” before you join the population data.\n\n\nSetting the fill to population in geom_sf() gives each region a colour based on its population.\nThe colours are customised with scale_fill_viridis_c(). The breaks of the fill scale are set to increments of 100K (1e5 in scientific notation) and the scale is set to span 0 to 600K.\n\npaste0() creates the labels by taking the numbers 0 through 6 and adding “00 k” to them.\nFinally, the position of the legend is moved into the sea using legend.position().\n\n\n# plot\nggplot() + \n  geom_sf(data = scotmap_pop,\n          mapping = aes(fill = population),\n          color = \"white\", \n          size = .1) +\n  coord_sf(xlim = c(-8, 0), ylim = c(54, 61)) +\n  scale_fill_viridis_c(name = \"Population\",\n                       breaks = seq(from = 0, to = 6e5, by = 1e5), \n                       limits = c(0, 6e5),\n                       labels = paste0(0:6, \"00 K\")) +\n  theme(legend.position = c(0.16, 0.84))\n\n\n\nMap coloured by population.\n\n\n\n\n10.1.4.7 Animated Plots\nAnimated plots are a great way to add a wow factor to your reports, but they can be complex to make, distracting, and not very accessible, so use them sparingly and only for data visualisation where the animation really adds something. The package gganimate has many functions for animating ggplots.\nHere, we’ll load some population data from the United Nations. Download the file into your data folder and open it in Excel first to see what it looks like. The code below gets the data from the first tab, filters it to just the 6 world regions, makes the data long, and makes sure the year column is numeric and the pop column shows population in whole numbers (the original data is in 1000s).\n\n# load and process data\nworldpop &lt;- readxl::read_excel(\"data/WPP2019_POP_F01_1_TOTAL_POPULATION_BOTH_SEXES.xlsx\", skip = 16) |&gt;\n  filter(Type == \"Region\") |&gt;\n  select(region = 3, `1950`:`1992`) |&gt;\n  pivot_longer(cols = -region, \n               names_to = \"year\",\n               values_to = \"pop\") |&gt;\n  mutate(year = as.integer(year),\n         pop = round(1000 * as.numeric(pop)))\n\nLet’s make an animated plot showing how the population in each region changes with year. First, make a static plot. Filter the data to the most recent year so you can see what a single frame of the animation will look like.\n\nworldpop |&gt;\n  filter(year == 1992) |&gt;\n  ggplot(aes(x = region, y = pop, fill = region)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d() +\n  scale_x_discrete(name = \"\", \n                   guide = guide_axis(n.dodge=2))+\n  scale_y_continuous(name = \"Population\",\n                     breaks = seq(0, 3e9, 1e9),\n                     labels = paste0(0:3, \"B\")) +\n  ggtitle('Year: 1992')\n\n\n\n\n\n\n\nTo convert this to an animated plot that shows the data from multiple years:\n\nRemove the filter and add transition_time(year).\nUse the {} syntax to include the frame_time in the title.\nUse anim_save() to save the animation to a GIF file and set this code chunk to eval = FALSE because creating an animation takes a long time and you don’t want to have to run it every time you knit your report.\n\n\nanim &lt;- worldpop |&gt;\n  ggplot(aes(x = region, y = pop, fill = region)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d() +\n  scale_x_discrete(name = \"\",\n                   guide = guide_axis(n.dodge=2))+\n  scale_y_continuous(name = \"Population\",\n                     breaks = seq(0, 3e9, 1e9),\n                     labels = paste0(0:3, \"B\")) +\n  ggtitle('Year: {frame_time}') +\n  transition_time(year)\n  \ndir.create(\"images\", FALSE) # creates an images directory if needed\n\nanim_save(filename = \"images/gganim-demo.gif\",\n          animation = anim,\n          width = 8, height = 5, units = \"in\", res = 150)\n\nYou can show your animated gif in an html report (animations don’t work in Word or a PDF) using include_graphics(), or include the GIF in a dynamic document like PowerPoint.\n\nknitr::include_graphics(\"images/gganim-demo.gif\")\n\n\n\nAnimated gif.\n\n\n\n\nThere are actually not many plots that are really improved by animating them. The plot below gives the same information at a single glance.\n\n\n\n\n\n\n\n\n\n\n10.1.5 Resources\nThere are so many more options for data visualisation in R than we have time to cover here. The following resources will get you started on your journey to informative, intuitive visualisations.\n\n\nThe R Graph Gallery (this is really useful)\n\nLook at Data from Data Vizualization for Social Science\n\n\nGraphs in Cookbook for R\n\nTop 50 ggplot2 Visualizations\n\nR Graphics Cookbook by Winston Chang\nggplot extensions\n\nplotly for creating interactive graphs\nDrawing Beautiful Maps Programmatically\ngganimate",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Next Steps</span>"
    ]
  },
  {
    "objectID": "10-next.html#sec-custom-reports",
    "href": "10-next.html#sec-custom-reports",
    "title": "10  Next Steps",
    "section": "\n10.2 Reports",
    "text": "10.2 Reports\n\n10.2.1 Set-up\n\nClose the file 10-viz.qmd\n\nCreate a new quarto file called 11-reports.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\n\n# packages needed for this chapter section\nlibrary(tidyverse)     # data wrangling functions\nlibrary(bookdown)      # for chaptered reports\nlibrary(flexdashboard) # for dashboards\nlibrary(DT)            # for interactive tables\n```\n\n\n10.2.2 Interactive tables\nOne way to make your reports more exciting is to use interactive tables. The DT::datatable() function displays a table with some extra interactive elements to allow readers to search and reorder the data, as well as controlling the number of rows shown at once. This can be especially helpful. This only works with HTML output types. The DT website has extensive tutorials, but we’ll cover the basics here.\n\nlibrary(DT)\n\nscotpop &lt;- read_csv(\"data/scottish_population.csv\", \n                    show_col_types = FALSE)\n\ndatatable(data = scotpop)\n\n\n\n\n\nYou can customise the display, such as changing column names, adding a caption, moving the location of the filter boxes, removing row names, applying classes to change table appearance, and applying advanced options.\n\n# https://datatables.net/reference/option/\nmy_options &lt;- list(\n  pageLength = 5, # how many rows are displayed\n  lengthChange = FALSE, # whether pageLength can change\n  info = TRUE, # text with the total number of rows\n  paging = TRUE, # if FALSE, the whole table displays\n  ordering = FALSE, # whether you can reorder columns\n  searching = FALSE # whether you can search the table\n)\n\ndatatable(\n  data = scotpop,\n  colnames = c(\"County\", \"Population\"),\n  caption = \"The population of Scottish counties.\",\n  filter = \"none\", # \"none\", \"bottom\" or \"top\"\n  rownames = FALSE, # removes the number at the left\n  class = \"cell-border hover stripe\", # default is \"display\"\n  options = my_options\n)\n\n\n\n\n\n\nCreate an interactive table like the one below from the diamonds dataset of diamonds where the table value is greater than 65 (the whole table is much too large to display with an interactive table). Show 20 items by default and remove the search box, but leave in the filter and other default options.\n\n\n\n\n\n\n\n\n\nSolution\n\nmy_options &lt;- list(\n  pageLength = 20, # how many rows are displayed\n  searching = FALSE # whether you can search the table\n)\n\ndiamonds |&gt; \n  filter(table &gt; 65) |&gt;\n  select(-table, -(x:z)) |&gt;\n  DT::datatable(\n    caption = \"All diamonds with table &gt; 65.\",\n    options = my_options\n  )\n\n\n\n\n10.2.3 Other formats\nYou can create more than just reports with R Markdown. You can also create presentations, interactive dashboards, books, websites, and web applications.\n\n10.2.3.1 Presentations\nYou can choose a presentation template when you create a new R Markdown document. We’ll use ioslides for this example, but the other formats work similarly.\n\n\n\n\nIoslides RMarkdown template.\n\n\n\nThe main differences between this and the Rmd files you’ve been working with until now are that the output type in the YAML header is ioslides_presentation instead of html_document and this format requires a specific title structure. Each slide starts with a level-2 header.\nThe template provides you with examples of text, bullet point, code, and plot slides. You can knit this template to create an HTML document with your presentation. It often looks odd in the RStudio built-in browser, so click the button to open it in a web browser. You can use the space bar or arrow keys to advance slides.\nThe code below shows how to load some packages and display text, a table, and a plot. You can see the HTML output here.\n\n\nSolution\n\n---\ntitle: \"Presentation Demo\"\nauthor: \"Lisa DeBruine\"\noutput: ioslides_presentation\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nlibrary(tidyverse)\nlibrary(kableExtra)\n```\n\n## Slide with Markdown\n\nThe following slides will present some data from the `diamonds` dataset from **ggplot2**.\n\nDiamond price depends on many features, such as:\n\n- cut\n- colour\n- clarity\n- carats\n\n## Slide with a Table\n\n```{r}\ndiamonds %&gt;%\n  group_by(cut, color) %&gt;%\n  summarise(avg_price = mean(price),\n            .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = cut, values_from = avg_price) %&gt;%\n  kable(digits = 0, caption = \"Mean diamond price by cut and colour.\") %&gt;%\n  kable_material()\n```\n\n## Slide with a Plot\n\n```{r pressure}\nggplot(diamonds, aes(x = cut, y = price, color = color)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(aes(x = as.integer(cut)), \n               fun = mean, geom = \"line\") +\n  scale_x_discrete(position = \"top\") +\n  scale_color_viridis_d(guide = guide_legend(reverse = TRUE)) +\n  theme_minimal() \n```\n\n\n10.2.3.2 Dashboards\nDashboards are a way to display text, tables, and plots with dynamic formatting. After you install flexdashboard, you can choose a flexdashboard template when you create a new R Markdown document.\n\n\n\n\nFlexdashboard RMarkdown template.\n\n\n\nThe code below shows how to load some packages, display two tables in a tabset, and display two plots in a column. You can see the HTML output here.\n\n\nSolution\n\n---\ntitle: \"Flexdashboard Demo\"\noutput: \n  flexdashboard::flex_dashboard:\n    social: [ \"twitter\", \"facebook\", \"linkedin\", \"pinterest\" ]\n    source_code: embed\n    orientation: columns\n    vertical_layout: fill\n---\n\n```{r setup, include=FALSE}\nlibrary(flexdashboard)\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(DT) # for interactive tables\ntheme_set(theme_minimal())\n```\n\nColumn {data-width=350, .tabset}\n--------------------------------\n\n### By Cut\n\nThis table uses `kableExtra` to render the table with a specific theme.\n\n```{r}\ndiamonds %&gt;%\n  group_by(cut) %&gt;%\n  summarise(avg = mean(price),\n            .groups = \"drop\") %&gt;%\n  kable(digits = 0, \n        col.names = c(\"Cut\", \"Average Price\"),\n        caption = \"Mean diamond price by cut.\") %&gt;%\n  kable_classic()\n```\n\n### By Colour\n\nThis table uses `DT::datatable()` to render the table with a searchable interface.\n\n```{r}\ndiamonds %&gt;%\n  group_by(color) %&gt;%\n  summarise(avg = mean(price),\n            .groups = \"drop\") %&gt;%\n  DT::datatable(colnames = c(\"Colour\", \"Average Price\"), \n                caption = \"Mean diamond price by colour\",\n                options = list(pageLength = 5),\n                rownames = FALSE) %&gt;%\n  DT::formatRound(columns=2, digits=0)\n```\n\nColumn {data-width=350}\n-----------------------\n\n### By Clarity\n\n```{r by-clarity, fig.cap = \"Diamond price by clarity\"}\nggplot(diamonds, aes(x = clarity, y = price)) +\n  geom_boxplot() \n```\n\n\n### By Carats\n\n```{r by-carat, fig.cap = \"Diamond price by carat\"}\nggplot(diamonds, aes(x = carat, y = price)) +\n  stat_smooth()\n```\n\nChange the size of your web browser to see how the boxes, tables and figures change.\nThe best way to figure out how to format a dashboard is trial and error, but you can also look at some sample layouts.\n\n10.2.3.3 Books\nYou can create online books with bookdown. In fact, the book you’re reading was created using bookdown. After you download the package, start a new project and choose “Book project using bookdown” from the list of project templates.\n\n\n\n\nBookdown project template.\n\n\n\nEach chapter is written in a separate .Rmd file and the general book settings can be changed in the _bookdown.yml and _output.yml files.\n\n10.2.3.4 Websites\nYou can create a simple website the same way you create any R Markdown document. Choose “Simple R Markdown Website” from the project templates to get started. See Appendix @ref(webpages) for a step-by-step tutorial.\nFor more complex, blog-style websites, you can investigate blogdown. After you install this package, you will also be able to create template blogdown projects to get you started.\n\n10.2.3.5 Shiny\nTo get truly interactive, you can take your R coding to the next level and learn Shiny. Shiny apps let your R code react to user input. You can do things like make a word cloud, search a google spreadsheet, or conduct a survey.\nThis is well outside the scope of this class, but the skills you’ve learned here provide a good start. The free book Building Web Apps with R Shiny by one of the authors of this book can get you started creating shiny apps.\nResources\n\nRStudio Formats\nR Markdown Cookbook\nDT\nFlexdashboard\nBookdown\nBlogdown\nShiny\nBuilding Web Apps with R Shiny",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Next Steps</span>"
    ]
  },
  {
    "objectID": "assessment-1.html",
    "href": "assessment-1.html",
    "title": "\n11  Assessment 1\n",
    "section": "",
    "text": "11.1 Instructions\nYour task is to write the outline structure of your thesis report in a reproducible quarto script. The report must contain the following:\nSee this report for an example.",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessment 1</span>"
    ]
  },
  {
    "objectID": "assessment-1.html#instructions",
    "href": "assessment-1.html#instructions",
    "title": "\n11  Assessment 1\n",
    "section": "",
    "text": "A project structure with all needed files (Section 2.2.3)\nA setup code chunk that loads the tidyverse (Section 2.5.1)\nThe main section headers that you expect to use in your report (i.e., Abstract, Introduction, Methods, Results, Discussion, References) (Section 2.4.3)\nA short paragraph about your research topic in the Introduction section, using markdown syntax for any text formatting (Section 2.4.3)\nUse inline R to include the version of R you are using in the Methods section (Section 2.4.7)\nA table in the Methods section that gives a rough timetable for your thesis, created using markdown or data frames in R (Section 2.5.8)\nOne relevant image (or your favourite stats/coding meme) with a figure caption (Section 2.5.7) in the Results section\nCross reference the image in the text with automatic figure numbering (Section 2.5.9)\nAt least two bibliographic references cited in the text, created and displayed in the reference section using reproducible techniques (Section 2.7)\n\nCustomise the YAML header (Section 2.6.3) to:\n\na table of contents (Section 2.6.4)\ndisplay tables using kable\nuse a non-default theme of your choosing\nhide all code chunks\n\n\nThe quarto file should be renderable by the marker (Section 2.4.8)",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessment 1</span>"
    ]
  },
  {
    "objectID": "assessment-1.html#submission",
    "href": "assessment-1.html#submission",
    "title": "\n11  Assessment 1\n",
    "section": "\n11.2 Submission",
    "text": "11.2 Submission\n\nCovers: chapters 1 and 2\nWorth: 10%\nDo not put your name in your report; use your student ID as the author.\nPlease submit a zip file containing:\n\nthe .rproj file\nyour reproducible script, named report1_studentID.qmd\n\nany additional files necessary to reproduce your report (e.g., images or bibliography files),\nthe rendered html report, named report1_studentID.html.",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessment 1</span>"
    ]
  },
  {
    "objectID": "assessment-1.html#marking-rubric",
    "href": "assessment-1.html#marking-rubric",
    "title": "\n11  Assessment 1\n",
    "section": "\n11.3 Marking Rubric",
    "text": "11.3 Marking Rubric\nYou will receive green/amber/red lights for each of 11 elements, and a generic feedback document explaining common issues. Each rubric element is weighted equally.\n\nGreen: Perfect or nearly perfect\nAmber: Needs some improvement\nRed: Incorrect or absent\n\n\n\n\n\n\n\n\n\n\n\nElements\nGreen\nAmber\nRed\n\n\n\nproject structure\n\n\n\n\n\nsetup code chunk\n\n\n\n\n\nsection headers\n\n\n\n\n\nparagraph\n\n\n\n\n\nimage\n\n\n\n\n\ncross reference\n\n\n\n\n\ninline R\n\n\n\n\n\ntable\n\n\n\n\n\nreferences\n\n\n\n\n\nYAML header\n\n\n\n\n\nrenderable\n\n\n\n\n\n\n\n\nTotal: 0/22",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assessment 1</span>"
    ]
  },
  {
    "objectID": "assessment-2.html",
    "href": "assessment-2.html",
    "title": "\n12  Assessment 2\n",
    "section": "",
    "text": "12.1 Instructions\nYour task is to replicate this html report using this dataset and code, and to generate an appropriate visualisation for the final plot. The submission must contain the following:",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assessment 2</span>"
    ]
  },
  {
    "objectID": "assessment-2.html#instructions",
    "href": "assessment-2.html#instructions",
    "title": "\n12  Assessment 2\n",
    "section": "",
    "text": "A reproducible project structure with all needed files (Section 2.2.3)\nDocument structure and text that replicate the report (Chapter 2)\nImport the data with correct data types (Section 4.1)\nPlots that replicate Figures 1 and 2 (Section 3.2)\nA table that replicates Table 2 (Section 4.4)\nA plot that is appropriate to the description of Figure 3 (Section 3.3)\nConsistent plot themes, colours and labels (Section 3.2.3.4)\nCross-references in the text to the figures and tables (Section 2.5.9)\nReferences created using bibtex (Section 2.7)",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assessment 2</span>"
    ]
  },
  {
    "objectID": "assessment-2.html#hints-and-tips",
    "href": "assessment-2.html#hints-and-tips",
    "title": "\n12  Assessment 2\n",
    "section": "\n12.2 Hints and Tips",
    "text": "12.2 Hints and Tips\n\nThe html theme used by the demo report is flatly\nThe ggplot theme is theme_bw()\n\nMake Table 1 using markdown\nYou will need to use summarise() to create the data for Table 2; do not just type the numbers from the demo table\nDisplay Table 2 using knitr::kable()\n\nYou shouldn’t need any packages apart from tidyverse and knitr\nThe colours for Figure 2 are “firebrick”, “darkgreen”, and “dodgerblue3”",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assessment 2</span>"
    ]
  },
  {
    "objectID": "assessment-2.html#submission",
    "href": "assessment-2.html#submission",
    "title": "\n12  Assessment 2\n",
    "section": "\n12.3 Submission",
    "text": "12.3 Submission\n\nCovers: chapters 1-4, emphasising 3 and 4\nWorth: 20%\nDo not put your name in your report; use your student ID as the author.\nPlease submit a zip file containing:\n\nthe .rproj file\nyour reproducible script, named report2_studentID.qmd\n\nany additional files necessary to reproduce your report (e.g., images or bibliography files),\nthe rendered html report, named report2_studentID.html.",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assessment 2</span>"
    ]
  },
  {
    "objectID": "assessment-2.html#marking-rubric",
    "href": "assessment-2.html#marking-rubric",
    "title": "\n12  Assessment 2\n",
    "section": "\n12.4 Marking Rubric",
    "text": "12.4 Marking Rubric\nYou will receive green/amber/red lights for each of the elements below, and a generic feedback document explaining common issues. Each rubric element is weighted equally.\n\nGreen: Perfect or nearly perfect\nAmber: Needs some improvement\nRed: Incorrect or absent\n\n\n\n\n\n\n\n\n\n\n\nElements\nGreen\nAmber\nRed\n\n\n\nReproducible project\n\n\n\n\n\nDocument replicated\n\n\n\n\n\nCaptions/references\n\n\n\n\n\nData import\n\n\n\n\n\nFigure 1 replicated\n\n\n\n\n\nFigure 2 replicated\n\n\n\n\n\nTable 2 replicated\n\n\n\n\n\nFigure 3 appropriate\n\n\n\n\n\nConsistent visualisations\n\n\n\n\n\nClear code\n\n\n\n\n\nEfficient code\n\n\n\n\n\n\n\n\nTotal: 0/22\n\n12.4.1 Reproducible project\nThe project contains all of the necessary files and can be rendered by the marker.\n\n12.4.2 Document replicated\nThe non-code elements of the document are replicated, such as table of contents, text paragraphs, and Table 1.\n\n12.4.3 Captions/references\nThe figure and table captions are created correctly and quarto cross referencing is used for figures, tables, and bibliographic references.\n\n12.4.4 Data import\nYou imported the data with correct data types, including putting factors in sensible order for later plots or tables.\n\n12.4.5 Figure 1 replicated\nFigure 1 is created using code, and has all of the features of the example plot, such as label, axis or colour customisation.\n\n12.4.6 Figure 2 replicated\nFigure 2 is created using code, and has all of the features of the example plot, such as label, axis or colour customisation.\n\n12.4.7 Table 2 replicated\nTable 2 is created by summarising the data using code, and has all of the features of the example table, such as the same rounding.\n\n12.4.8 Figure 3 appropriate\nYou chose an appropriate geom (or set of geoms) for the relationship being demonstrated.\n\n12.4.9 Consistent visualisations\nFigure 3 has consistent features with Figures 1 and 2, such as the same labels or same colours for the same levels (or different colours if you use colour to highlight different levels).\n\n12.4.10 Clear code\nYour code is organised in the quarto script cleanly and clearly, using separate code chunks to intersperse text and relevant code. Your code chunks contain comments that clarify the purpose of the code, but not overly-explaining each step. The names you use for objects are clear, consistent, and concise.\nFor example, if the marker wants to assess the code that creates Table 2, they should be easily able to identify that object in the environment from the name, and easily able to find the code that creates it.\n\n12.4.11 Efficient code\nWhile there are many ways to do the same things in R, some ways are more efficient than others. These avoid unnecessary code (e.g., do not load packages you do not use) and redundancy (e.g., do not load or process the data the same way in several places).\nYou will not be asked to do anything “tricky” in this assessment, so if you find yourself needing 20 lines of code just to customise the labels in a plot, try looking for more efficient alternatives.",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Assessment 2</span>"
    ]
  },
  {
    "objectID": "assessment-3.html",
    "href": "assessment-3.html",
    "title": "\n13  Assessment 3\n",
    "section": "",
    "text": "13.1 Instructions\nYour task is to replicate this html report using these datasets and code, and to generate an appropriate visualisation for the final plot. Nearly identical to assessment 2, the submission must contain the following:\nIn addition, your code should demonstrate the following skills:",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assessment 3</span>"
    ]
  },
  {
    "objectID": "assessment-3.html#instructions",
    "href": "assessment-3.html#instructions",
    "title": "\n13  Assessment 3\n",
    "section": "",
    "text": "A reproducible project structure with all needed files (Section 2.2.3)\nDocument structure and text that replicate the report (Chapter 2)\nImport the data with correct data types (Section 4.1)\nPlots that replicate Figures 1 (Section 3.2)\nA table that replicates Table 1 and 2 (Section 4.4)\nA plot that is appropriate to the description of Figure 2 (Section 3.3)\nConsistent plot themes, colours and labels (Section 3.2.3.4)\nCross-references in the text to the figures and tables (Section 2.5.9)\nReferences created using bibtex (Section 2.7)\n\n\n\nBe able to match related data across multiple tables (Section 5.4)\nBe able to combine data from multiple files (Section 5.7.3)\nBe able to reshape data between long and wide formats (Section 6.3)\nSeparate, change, reorder, and rename columns (Section 6.4)\nUse pipes to chain together functions (Section 6.5)\nBe able to select and filter data for relevance (Section 7.1)\nBe able to create new columns and edit existing ones (Section 7.1.4)",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assessment 3</span>"
    ]
  },
  {
    "objectID": "assessment-3.html#hints-and-tips",
    "href": "assessment-3.html#hints-and-tips",
    "title": "\n13  Assessment 3\n",
    "section": "\n13.2 Hints and Tips",
    "text": "13.2 Hints and Tips\n\nThe html theme used by the demo report is flatly\nThe ggplot theme is theme_bw()\n\nYou will not be asked to do anything ‘tricky’ in this assessment, so if you find yourself needing 20 lines of code just to customise the labels in a plot, try looking for more efficient alternatives.\nYou will definitely need to join data, and convert between wide and long. Make sure you do this efficiently, creating the minimum number of extra data tables needed, and not re-creating the same table for different tasks.\nIf you find yourself doing the same thing to many columns, you could probably do it more efficiently by reshaping the data longer first\nUse pipes to avoid creating many single-use tables.\nThe figure width and height of Figure 1 are 10 and 7 (it is fine if your fonts are a slightly different size)",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assessment 3</span>"
    ]
  },
  {
    "objectID": "assessment-3.html#submission",
    "href": "assessment-3.html#submission",
    "title": "\n13  Assessment 3\n",
    "section": "\n13.3 Submission",
    "text": "13.3 Submission\n\nCovers: chapters 1-7, emphasising 5-7\nWorth: 30%\nDo not put your name in your report; use your student ID as the author.\nPlease submit a zip file containing:\n\nthe .rproj file\nyour reproducible script, named report3_studentID.qmd\n\nany additional files necessary to reproduce your report(e.g., images or bibliography files),\nthe rendered html report, named report3_studentID.html.",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assessment 3</span>"
    ]
  },
  {
    "objectID": "assessment-3.html#marking-rubric",
    "href": "assessment-3.html#marking-rubric",
    "title": "\n13  Assessment 3\n",
    "section": "\n13.4 Marking Rubric",
    "text": "13.4 Marking Rubric\n\n\n\n\n\n\nILO\n      A: Excellent\n      B: Very Good\n      C: Good\n      D: Satisfactory\n      E: Poor\n    \n\n\nResearch & Knowledge\n    \n\nSkills from Chapters 1-3: You demonstrate skills to create reproducible reports and visualise data\n\n\n\n\n\n\n\nData Import and Joining: You import and join together data clearly and correctly\n\n\n\n\n\n\n\nData Reshaping: You can reshape data between long and wide formats where approriate\n\n\n\n\n\n\n\nData Wrangling: You demonstrate the ability to select and filter data, create new data columns, and edit existing data columns\n\n\n\n\n\n\n\nEvaluation\n    \n\nOriginal Plot: The original plot is appropriate to the question asked\n\n\n\n\n\n\n\nCommunication\n    \n\nCode Clarity: Your code is organised in the quarto script cleanly and clearly, using separate code chunks to intersperse text and relevant code. Your code chunks contain comments that clarify the purpose of the code, but not overly-explaining each step. The names you use for objects are clear, consistent, and concise.\n\n\n\n\n\n\n\nCode Efficiency: While there are many ways to do the same things in R, some ways are more efficient than others. These avoid unnecessary code (e.g., do not load packages you do not use) and redundancy (e.g., do not load or process the data the same way in several places).",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Assessment 3</span>"
    ]
  },
  {
    "objectID": "assessment-4.html",
    "href": "assessment-4.html",
    "title": "\n14  Assessment 4\n",
    "section": "",
    "text": "14.1 Instructions\nTBA",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assessment 4</span>"
    ]
  },
  {
    "objectID": "assessment-4.html#submission",
    "href": "assessment-4.html#submission",
    "title": "\n14  Assessment 4\n",
    "section": "\n14.2 Submission",
    "text": "14.2 Submission\n\nCovers: chapters 1-10, emphasising 8 and 9\nWorth: 40%\nDo not put your name in your report; use your student ID as the author.\nPlease submit a zip file containing:\n\nthe .rproj file\nyour reproducible script, named report4_studentID.qmd\n\nany additional files necessary to reproduce your report(e.g., images or bibliography files),\nthe rendered html report, named report4_studentID.html.",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assessment 4</span>"
    ]
  },
  {
    "objectID": "assessment-4.html#marking-rubric",
    "href": "assessment-4.html#marking-rubric",
    "title": "\n14  Assessment 4\n",
    "section": "\n14.3 Marking Rubric",
    "text": "14.3 Marking Rubric\n\n\n\n\n\n\nILO\n      A: Excellent\n      B: Very Good\n      C: Good\n      D: Satisfactory\n      E: Poor\n    \n\n\nResearch & Knowledge\n    \n\nTopic1: Description\n\n\n\n\n\n\n\nEvaluation\n    \n\nTopic2: Description\n\n\n\n\n\n\n\nCommunication\n    \n\nTopic3: Description",
    "crumbs": [
      "Assessments",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Assessment 4</span>"
    ]
  },
  {
    "objectID": "12-license.html",
    "href": "12-license.html",
    "title": "Licence/Citation",
    "section": "",
    "text": "Citation",
    "crumbs": [
      "Licence/Citation"
    ]
  },
  {
    "objectID": "12-license.html#citation",
    "href": "12-license.html#citation",
    "title": "Licence/Citation",
    "section": "",
    "text": "DOI",
    "crumbs": [
      "Licence/Citation"
    ]
  },
  {
    "objectID": "12-license.html#acknowledgements",
    "href": "12-license.html#acknowledgements",
    "title": "Licence/Citation",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe whole psyTeachR team at the University of Glasgow School of Psychology deserves enormous thanks for making it possible and rewarding to teach methods with a focus on reproducibility and open research. Particularly:\nHeather Cleland Woods, Phil McAleer, Helena Paterson, Emily Nordmann, Carolina Keuper-Tetzel, and Niamh Stack.\nWe greatly appreciate Iris Holzleitner’s volunteer in-class assistance with the first year of this course. We were ever so lucky to get Rebecca Lai as a teaching assistant in the second year; her kind and patient approach to teaching technical skills is an inspiration. Benedict Jones made invaluable contributions to the ethos of reproducible research at Glasgow. Thanks to Daniël Lakens for many inspirational discussions and resources.",
    "crumbs": [
      "Licence/Citation"
    ]
  },
  {
    "objectID": "12-license.html#contributors",
    "href": "12-license.html#contributors",
    "title": "Licence/Citation",
    "section": "Contributors",
    "text": "Contributors\nSeveral people contributed to testing these materials.\n\nRebecca Lai\nSean Westwood\nDavid Pharis\nRichard Morey\nMossa Merhi Reimert",
    "crumbs": [
      "Licence/Citation"
    ]
  },
  {
    "objectID": "12-license.html#references",
    "href": "12-license.html#references",
    "title": "Licence/Citation",
    "section": "References",
    "text": "References\n\n\n\n\nNordmann, E., & DeBruine, L. (2023). Applied data skills (Version 3.0). https://doi.org/10.5281/zenodo.6365077",
    "crumbs": [
      "Licence/Citation"
    ]
  },
  {
    "objectID": "app-installing-r.html",
    "href": "app-installing-r.html",
    "title": "Appendix A — Installing & Updating R",
    "section": "",
    "text": "A.1 Getting Started\nInstalling R and RStudio is usually straightforward. The sections below explain how and there is a helpful YouTube video here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing & Updating R</span>"
    ]
  },
  {
    "objectID": "app-installing-r.html#getting-started",
    "href": "app-installing-r.html#getting-started",
    "title": "Appendix A — Installing & Updating R",
    "section": "",
    "text": "A.1.1 Installing Base R\nInstall base R. Choose the download link for your operating system (Linux, Mac OS X, or Windows).\nIf you have a Mac, install the latest release from the newest R-x.x.x.pkg link (or a legacy version if you have an older operating system). You may also need to install XQuartz to be able to use some visualisation packages.\nIf you are installing the Windows version, choose the “base” subdirectory and click on the download link at the top of the page.\nIf you are using Linux, choose your specific operating system and follow the installation instructions.\n\n\n\n\n\n\nInstallation Location\n\n\n\nIt can often cause problems to install R on a network or cloud drive, such as OneDrive or DropBox. It’s better to install these programs on your computer’s drive. Depending on your computer’s settings, you may have to get IT support to give you access to installing programs.\nIt can also cause rare, but hard-to-debug problems if any of the folders in the path where you install R have non-Latin characters, including Chinese characters or Latin characters with accents (e.g., C:\\\\Daniël\\Programs\\).\n\n\n\nA.1.2 Installing RStudio\nGo to rstudio.com and download the RStudio Desktop (Open Source License) version for your operating system under the list titled Installers for Supported Platforms.\n\nA.1.2.1 Installing RTools\nIf you are using Windows, after you install R, you should also install RTools; use the “recommended” version highlighted near the top of the list. RTools is used for installing and loading some packages. You can get started without installing RTools, but if you’re having problems with installing and loading some packages, this should be the first thing you try.\nRTools will require you to put it “on the PATH”. The instructions for this can seem a bit vague - the easiest way to do it is to open RStudio, run the below code in the console:\n\nwrite('PATH=\"${RTOOLS40_HOME}\\\\usr\\\\bin;${PATH}\"', file = \"~/.Renviron\", append = TRUE)\n\nOnce you’ve done that, restart R by clicking Session - Restart R and then run the below code in the console which should give you the path to your RTools installation:\n\nSys.which(\"make\")\n\n           make \n\"/usr/bin/make\" \n\n\n\nA.1.2.2 RStudio Settings\nThere are a few settings you should fix immediately after updating RStudio. Go to Tools &gt; Global Options… (⌘,), and in the General tab, uncheck the box that says Restore .RData into workspace at startup. If you keep things around in your workspace, things will get messy, and unexpected things will happen. You should always start with a clear workspace. This also means that you never want to save your workspace when you exit, so set this to Never. The only thing you want to save are your scripts.\nYou may also want to change the appearance of your code. Different fonts and themes can sometimes help with visual difficulties or dyslexia.\n\n\n\n\nRStudio General and Appearance settings\n\n\n\nYou may also want to change the settings in the Code tab. For example, Lisa prefers two spaces instead of tabs for my code and likes to be able to see the whitespace characters. But these are all a matter of personal preference.\n\n\n\n\nRStudio Code settings\n\n\n\n\nA.1.3 Installing LaTeX\nYou can install the LaTeX typesetting system to produce PDF reports from RStudio. Without this additional installation, you will be able to produce reports in HTML but not PDF. To generate PDF reports, you will additionally need to install tinytex(Xie, 2022) and run the following code:\n\n# run this in the console\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing & Updating R</span>"
    ]
  },
  {
    "objectID": "app-installing-r.html#sec-updating-r",
    "href": "app-installing-r.html#sec-updating-r",
    "title": "Appendix A — Installing & Updating R",
    "section": "\nA.2 Updating R, RStudio, and packages",
    "text": "A.2 Updating R, RStudio, and packages\nFrom time-to-time, updated version of R, RStudio, and the packages you use (e.g., ggplot) will become available. Remember that each of these are separate, so they each have a different process and come with different considerations. We recommend updating to the latest version of all three at the start of each academic year.\n\nA.2.1 Updating RStudio\nRStudio is the easiest component to update. Typically, updates to RStudio won’t affect your code, instead they add in new features, like spell-check or upgrades to what RStudio can do. There’s usually very little downside to updating RStudio and it’s easy to do.\nClick Help &gt; Check for updates\n\n\n\n\nUpdating RStudio\n\n\n\nIf an update is available, it will prompt you to download it and you can install it as usual.\n\nA.2.2 Updating R\nFinally, you may also wish to update R itself. The key thing to be aware of is that when you update R, if you just download the latest version from the website, you will lose all your packages.\n\nA.2.2.1 Windows\nThe easiest way to update R on Windows and not cause yourself a huge headache is to use the installr package. When you use the updateR() function, a series of dialogue boxes will appear. These should be fairly self-explanatory but there is a full step-by-step guide available for how to use installr, the important bit is to select “Yes” when it asked if you would like to copy your packages from the older version of R.\n\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Run the update function\ninstallR::updateR()\n\n\nA.2.2.2 Mac\nFor a Mac, you can use the updateR package. You’ll need to install this from GitHub. You will be asked to type your system password (that you use to log into your computer) in the console pane. If relevant, it will ask you if you want to restore your packages for a new major version.\n\n# install from github\ndevtools::install_github(\"AndreaCirilloAC/updateR\")\n\n# update your R version, you will need your system password\nupdateR::updateR()\n\n\nA.2.3 Updating packages\nPackage developers will occasionally release updates to their packages. This is typically to add in new functions to the package, or to fix or amend existing functions. Be aware that some package updates may cause your previous code to stop working. This does not tend to happen with minor updates to packages, but occasionally with major updates, you can have serious issues if the developer has made fundamental changes to how the code works. For this reason, we recommend updating all your packages once at the beginning of each academic year (or semester) - don’t do it before an assessment or deadline just in case!\nTo update an individual package, the easiest way is to use the install.packages() function, as this always installs the most recent version of the package.\n\ninstall.packages(\"tidyverse\")\n\nTo update multiple packages, or indeed all packages, RStudio provides helpful tools. Click Tools &gt; Check for Package Updates. A dialogue box will appear and you can select the packages you wish to update. Be aware that if you select all packages, this may take some time and you will be unable to use R whilst the process completes.\n\n\n\n\nUpdating packages with RStudio\n\n\n\n\nA.2.4 Troubleshooting\nOccasionally, you might have a few problem packages that seemingly refuse to update. For me, rlang and vctrs cause me no end of trouble. These aren’t packages that you will likely every explicitly load, but they’re required beneath the surface for R to do things like knit your Markdown files etc.\n\nA.2.4.1 Non-zero exit status\nIf you try to update a package and get an error message that says something like Warning in install.packages : installation of package ‘vctrs’ had non-zero exit status or perhaps Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :  namespace 'rlang' 0.4.9 is being loaded, but &gt;= 0.4.10 is required one solution I have found is to manually uninstall the package, restart R, and then install the package new, rather than trying to update an existing version. The installr package also has a useful function for uninstalling packages.\n\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")\n\n\nA.2.4.2 Cannot open file\nYou may get the following error after trying to install any packages at all:\n\nError in install packages : Cannot open file ‘C:/…..’: Permission denied\n\nThis usually indicates a permissions problem with writing to the default library (the folder that packages are kept in). Sometimes this means that you need to install R and RStudio as administrator or run it as administrator.\nOne other fix may be to change the library location using the following code (check in “C:/Program Files/R” for what version you should have instead of “R-3.5.2”):\n\n# change the library path\n.libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nIf that works and you can install packages, set this library path permanently:\n\nInstall the usethis package\nRun usethis::edit_r_profile() in the console; it will open up a blank file\nPaste into the file (your version of): .libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nSave and close the file\nRestart R for changes to take effect\n\nThe code in your .Rprofile will now run every time you start up R.\nAs always, if you’re having issues, please ask on Teams or come to office hours.\n\n\n\n\nXie, Y. (2022). Tinytex: Helper functions to install and maintain TeX live, and compile LaTeX documents. https://github.com/rstudio/tinytex",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing & Updating R</span>"
    ]
  },
  {
    "objectID": "app-symbols.html",
    "href": "app-symbols.html",
    "title": "Appendix B — Symbols",
    "section": "",
    "text": "Symbol\npsyTeachR Term\nAlso Known As\n\n\n\n()\n(round) brackets\nparentheses\n\n\n[]\nsquare brackets\nbrackets\n\n\n{}\ncurly brackets\nsquiggly brackets\n\n\n&lt;&gt;\nchevrons\nangled brackets / guillemets\n\n\n&lt;\nless than\n\n\n\n&gt;\ngreater than\n\n\n\n&\nampersand\n“and” symbol\n\n\n#\nhash\npound / octothorpe\n\n\n/\nslash\nforward slash\n\n\n\\\nbackslash\n\n\n\n-\ndash\nhyphen / minus\n\n\n_\nunderscore\n\n\n\n*\nasterisk\nstar\n\n\n^\ncaret\npower symbol\n\n\n~\ntilde\ntwiddle / squiggle\n\n\n=\nequal sign\n\n\n\n==\ndouble equal sign\n\n\n\n.\nfull stop\nperiod / point\n\n\n!\nexclamation mark\nbang / not\n\n\n?\nquestion mark\n\n\n\n’\nsingle quote\nquote / apostrophe\n\n\n”\ndouble quote\nquote\n\n\n%&gt;%\npipe\nmagrittr pipe\n\n\n|\nvertical bar\npipe\n\n\n,\ncomma\n\n\n\n;\nsemi-colon\n\n\n\n:\ncolon\n\n\n\n@\n“at” symbol\nvarious hilarious regional terms\n\n\n…\nglossary(\"ellipsis\")\ndots\n\n\n\n\n\n\n\n\n\n\nFigure B.1: Image by James Chapman/Soundimals",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Symbols</span>"
    ]
  },
  {
    "objectID": "app-conventions.html",
    "href": "app-conventions.html",
    "title": "Appendix C — Conventions",
    "section": "",
    "text": "C.1 Test Yourself\nI am going to learn a lot: \nTRUE\nFALSE\nHidden Solutions\n\n\n\n\n\nYou found it!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Conventions</span>"
    ]
  },
  {
    "objectID": "app-conventions.html#test-yourself",
    "href": "app-conventions.html#test-yourself",
    "title": "Appendix C — Conventions",
    "section": "",
    "text": "What is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Conventions</span>"
    ]
  },
  {
    "objectID": "app-conventions.html#callout-boxes",
    "href": "app-conventions.html#callout-boxes",
    "title": "Appendix C — Conventions",
    "section": "\nC.2 Callout boxes",
    "text": "C.2 Callout boxes\nSee the quarto reference for more options.\n\n\n\n\n\n\nNote\n\n\n\nInformational asides.\n\n\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\nTips\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNotes to warn you about something.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nNotes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNotes about things that are important.\n\n\n\nTry it yourself.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Conventions</span>"
    ]
  },
  {
    "objectID": "app-conventions.html#code-and-output",
    "href": "app-conventions.html#code-and-output",
    "title": "Appendix C — Conventions",
    "section": "\nC.3 Code and Output",
    "text": "C.3 Code and Output\n## Markdown Example\n\n* Inline code: `r nrow(iris)`\n* *Italics*\n* **Bold**\n\n# code chunks\npaste(\"Code\", \"Output\", 1, sep = \" \")\n\n[1] \"Code Output 1\"\n\n\n\n```{r}\n#| label: fig-fenced-example\n#‎| fig-cap: \"#| Set `echo: fenced` for code chunks with headers\"\n#‎| echo: false\n\nhist(rnorm(100000))\n```\n\n\n\n\n\n\nFigure C.1: Set #| echo: fenced for code chunks with headers\n\n\n\n\n\n\n\n\n\n\nShowing echo in fenced code blocks\n\n\n\n\n\nUse a version of #‎| with an invisible character to show options in a fenced code block that you don’t actually want to apply to this chunk, such as a second echo or a duplicate label. Copy and edit the text below into RStudio and you’ll see a red highlighted dot between # and |. These lines have to go below any lines that you do want to apply to this code block.\n#‎| echo: false\n\n\n\n\n\n\n\nWickham, H. (2022). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Conventions</span>"
    ]
  },
  {
    "objectID": "app-teams.html",
    "href": "app-teams.html",
    "title": "Appendix D — Using Teams",
    "section": "",
    "text": "D.1 What is the best way to share R code on Microsoft Teams?\nYou have run into a problem and need to get help on MS Teams. What is the right way to share your code?\nPlease do not share only a screenshot unless you are asked, or if it is not the code that is giving you problems, but something weird is happening with the RStudio IDE.\nIf it’s your code that is not working, it is almost always better to copy and paste the code, because then people who are trying to help you can copy and paste the code exactly to try it out, rather than having to re-type everything from the image. Let’s look at an example. Below is a screenshot of how the RStudio IDE might look when your code throws an error. Here the code block labelled cars is causing the error.\nThe particular error that our code threw was\nAnd the code that threw it was\nmtcars %&gt;%\n  filter(mpg &gt; 20)\nNote that you can select and copy the code above if you wanted to run it yourself, but you could not do that if all you had to rely on was the screenshot.\nCopying the code and/or error in RStudio is easy; just highlight the code using the mouse and press Ctrl-C.\nIf you just paste the code into a Teams channel, the formatting is not so nice; you lose the formatting that allows you to read the code easily.\nYou can share a screenshot and the code if you like, but please don’t just send the screenshot.\nHere are two ways to get your code into Teams, one that is quick and easy but not very flexible, and another that is far more flexible but requires more steps.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Using Teams</span>"
    ]
  },
  {
    "objectID": "app-teams.html#what-is-the-best-way-to-share-r-code-on-microsoft-teams",
    "href": "app-teams.html#what-is-the-best-way-to-share-r-code-on-microsoft-teams",
    "title": "Appendix D — Using Teams",
    "section": "",
    "text": "a screenshot of RStudio IDE showing an error indicated by the red arrow\n\n\nError in mtcars %&gt;% filter(mpg &gt; 20) : could not find function \"%&gt;%\"\n\n\n\n\n\n\n\nA screenshot of MS Teams with the code pasted directly in. Not pretty!\n\n\n\n\nD.1.1 Quick and easy method\nFirst, if it is just a short function call, a single line, or an error, you can signal that text is meant to appear as code by surrounding it by single backticks—i.e., putting a backtick (`) right before and right after the text that you want to be formatted as code. Teams will automatically format it for you.\nFor multi-line code, the easiest and fastest way is just to type three backticks inside your message at the beginning of a line. Any subsequent text you enter will be treated as code. To get to the beginning of a line without submitting your post, press Ctrl-Enter while typing your message. Then type the three backticks, and paste your code right into the gray box that automatically appears. Press Enter twice in a row to get back out of the code entry box. So your message might look like this. \nAbove, I surrounded the error message Error in mtcars %&gt;% filter(mpg &gt; 20) : could not find function \"%&gt;%\" with single backticks to indicate code, and we typed triple backticks at the start of the line to create a code chunk. (The next method might be easier for making multi-line posts.)\n\nD.1.2 More flexible method\nThere is a more flexible (and possibly easier) way. Before pasting any text, click on the icon that looks like the letter “A”, highlighted below.\n\n\nScreenshot of Teams showing the icon that looks like an “A”\n\nThis will open up options for text formatting and will allow you to easily create a multi-line post. From those options, select the icon that looks like &lt;/&gt;, which stands for code.\n\n\nScreenshot of Teams formatting icons, with code icon highlighted\n\nThe code icon will open a window where you can paste your code. In the dropdown menu on the top right, select ‘R’ as the type of code. This will give you syntax highlighting.\n\n\nScreenshot of Teams formatting icons, with code icon highlighted\n\nHere is how you might begin your post.\n\n\nScreenshot of Teams with unsubmitted post\n\n\nD.1.3 Reprex\nYou might see people in coding forums like StackOverflow asking for a “reprex”, or a reproducible example. This is the smallest, completely self-contained example of your problem or question.\nFor example, you may have a question about how to figure out how to select rows that contain the value “test” in a certain column, but it isn’t working. It’s clearer if you can provide a concrete example, but you don’t want to have to type out the whole table you’re using or all the code that got you to this point in your script.\nYou can include a very small table with just the basics or a smaller version of your problem. Make comments at each step about what you expect and what you actually got.\nWhich version is easier for you to figure out the solution?\n\n# this doesn't work\nno_test_data &lt;- data %&gt;%\n  filter(!str_detect(type, \"test\"))\n\n\n# with a minimal example table\ndata &lt;- tribble(\n  ~id, ~type, ~x,\n  1, \"test\", 12,\n  2, \"testosterone\", 15,\n  3, \"estrogen\", 10\n)\n\n# this should keep IDs 2 and 3, but removes ID 2\nno_test_data &lt;- data %&gt;%\n  filter(!str_detect(type, \"test\"))\n\nOne of the big benefits to creating a reprex is that you often solve your own problem while you’re trying to break it down to explain to someone else.\nIf you really want to go down the rabbit hole, you can create a reproducible example using the reprex package from tidyverse.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Using Teams</span>"
    ]
  },
  {
    "objectID": "app-teams.html#screenshots",
    "href": "app-teams.html#screenshots",
    "title": "Appendix D — Using Teams",
    "section": "\nD.2 Screenshots",
    "text": "D.2 Screenshots\nIf you do need to take a screenshot, for example, if something goes wrong during installation, please use the screenshot functions built-in to your computer rather than taking a photo of your screen using your phone.\n\nD.2.1 Taking a screenshot on Windows\n\nUse the Windows search function to search for “Snip & Sketch”\nClick “New” then “Snip now”\nUse the tool to select the area on the screen you want to take a screenshot of. This photo will automatically be copied to your clipboard, so you can paste it into e.g., a Teams chat or a document using Ctrl + V but you can also click the Save icon in the top right to save the screenshot as an image file.\nThe shortcut for the snipping tool is Win + Shift + S.\n\nD.2.2 Taking a screenshot on Mac\n\nPress Shift + Command (⌘) + 4 to bring up the Screenshot app.\nUse the tool to select the area on the screen you want to take a screenshot of.\nIf you see a thumbnail in the corner of your screen, click it to edit the screenshot or drag it into e.g., a Teams chat.\nThis photo will also automatically save to your desktop.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Using Teams</span>"
    ]
  },
  {
    "objectID": "app-datasets.html",
    "href": "app-datasets.html",
    "title": "Appendix E — Datasets",
    "section": "",
    "text": "You can download a zip file of the datasets. Each data table comes with a codebook in Psych-DS JSON format.\n\n12.1_delivery.csv\n5factor.csv\namazon_alexa.csv\navatar.csv\nbudget.csv\ncountry_codes.csv\ndate_formats.csv\ndemo.csv\ndisgust_cors.csv\ndisgust_scores.csv\ndisgust.csv\nEMBU_mother.csv\nempathizing.csv\neq_data.csv\nexperimentum_exps.csv\nexperimentum_quests.csv\neye_descriptions.csv\nfac_2w2b_5n.csv\nfac_2w2b_small.csv\nfamily_composition.csv\ngrade_data1.csv\ngrade_data2.csv\ninfmort.csv\nmatmort.xls\nmess.csv\npersonality_scores.csv\npersonality.csv\npets.csv\npsa001_agg.csv\npsa001_cfd_faces.csv\nreview_data.csv\nsales_data_sample.csv\nscheduleA.csv\nscottish_population.csv\nsensation_seeking.csv\nsmalldata.csv\nsortmasc.csv\nsq_data.csv\nstroop.csv\nstroop.xlsx\nsunfact2021.csv\nsurvey_data.csv\nsystemizing.csv\ntidy_data.csv\nuntidy_data.csv\nusers.csv\nusers2.csv\nweekly_ae_activity_20240303.csv\nwide_exercise-1.csv\nwide_exercise-2.csv\nwidgets_gadgets.xlsx\nWPP2019_POP_F01_1_TOTAL_POPULATION_BOTH_SEXES.xlsx\nwtageinf.csv",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "app-debugging.html",
    "href": "app-debugging.html",
    "title": "Appendix F — Debugging",
    "section": "",
    "text": "F.1 Report Setup\nCreate a new quarto file and delete everything below the setup chunk. Edit the YAML header to use a floating table of contents and add the outline of your report.\nSave this file and knit it. Ideally, this will generate some output in a new tab in the console pane called “Render” that starts with processing file: report-demo.Rmd and ends with Output created: report-demo.html. There will be a lot of output in between, but you don’t need to worry about it until something goes wrong.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "app-debugging.html#report-setup",
    "href": "app-debugging.html#report-setup",
    "title": "Appendix F — Debugging",
    "section": "",
    "text": "---\ntitle: \"Report\"\ndate: \"2024-09-05\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n---\n\n\n\n## Introduction\n\n## Data\n\n### Term 1\n\n### Term 2\n\n## Analysis\n\n## References",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "app-debugging.html#yaml-errors",
    "href": "app-debugging.html#yaml-errors",
    "title": "Appendix F — Debugging",
    "section": "\nF.2 YAML Errors",
    "text": "F.2 YAML Errors\nOne of the more frequent problems is errors in the YAML header. Let’s create a few to see how to deal with them.\n\nF.2.1 YAML borders\nDelete the last dash below the header and knit.\n---\ntitle: \"Report\"\ndate: \"2024-09-05\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n--\nThis will actually knit without error (and look odd), but you’ll get a warning about the empty title. This is because R Markdown doesn’t recognise that there even is a YAML header if the three dashes to start and end it aren’t right.\n\nF.2.2 Spaces\nUnlike R and markdown, YAML is extremely picky about spaces. Try removing the space after the colon after “toc”.\n---\ntitle: \"Report\"\ndate: \"2024-09-05\"\noutput: \n  html_document:\n    toc:true\n    toc_float: true\n---\nYou should get an error that looks like this:\nError in yaml::yaml.load(..., eval.expr = TRUE) : \n  Scanner error: mapping values are not allowed in this context at line 6, column 14\nCalls: &lt;Anonymous&gt; ... parse_yaml_front_matter -&gt; yaml_load -&gt; &lt;Anonymous&gt;\nExecution halted\nIf you see Error in yaml and it gives you a line and column number, this refers to the YAML line, so start counting with 1 at the title line. Sometimes the actual problem is in the line above or below the reference. Here, the problem is a missing space in the toc line, but that doesn’t cause an error in the YAML parsing until it gets to the next line.\n\nF.2.3 Indenting\nYAML is also extremely picky about indenting. A common error is not putting html_document: on a separate line when adding options like a table of contents.\n---\ntitle: \"Report\"\ndate: \"2024-09-05\"\noutput: html_document:\n    toc: true\n    toc_float: true\n---\nYou should get an error that looks like this:\nError in yaml::yaml.load(..., eval.expr = TRUE) : \n  Scanner error: mapping values are not allowed in this context at line 3, column 22\nSome indenting problems don’t cause an error, but result in an output that isn’t doing what you expect. Try removing the indent for the table of contents lines and knitting.\n---\ntitle: \"Report\"\ndate: \"2024-09-05\"\noutput: \n  html_document:\n  toc: true\n  toc_float: true\n---",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "app-debugging.html#common-errors",
    "href": "app-debugging.html#common-errors",
    "title": "Appendix F — Debugging",
    "section": "\nF.3 Common Errors",
    "text": "F.3 Common Errors\nThe best way to learn to deal with errors is to make a lot of them. That way, the next time you encounter a similar error, you’ll have some experience solving it.\nRun the following code in the console; don’t add it to the report script.\n\n\n\nRun in the console\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNow, make a code chunk somewhere in your report like this and run it interactively (not by knitting). It should create a new table called droids with 6 rows.\n```{r}\ndroids &lt;- starwars %&gt;% filter(species == \"Droid\")\n```\nNow try to knit the report. Because you didn’t load the tidyverse package bundle in the script, you’ll get an error about not being able to find the function %&gt;% (you’ll learn about the pipe in Section 4.2). When you knit, any objects in your global environment or packages that you’ve loaded are unavailable and the script only has access to objects it creates and packages it loads.\nAdd library(tidyverse) to the setup chunk and knit to confirm this works.\n\nF.3.1 Could not find function\n\ntitle &lt;- pasteO(\"Lavendar\", \"Haze\")\n\nError in pasteO(\"Lavendar\", \"Haze\"): could not find function \"pasteO\"\n\n\nWhen you get the message could not find function \"func\", usually one of two things has happened: you haven’t loaded the package that the function is from or you’ve made a typo in the function name. In this example, the function is actually paste0() with a zero.\n\nF.3.2 Unused argument\n\nrnorm(N = 10)\n\nError in rnorm(N = 10): unused argument (N = 10)\n\n\nWhen you get the error “unused argument”, it usually means either that you’ve made a typo in an argument name, or the function doesn’t have that argument. Remember that argument, like functions and objects, are case-sensitive. Check the arguments with tab-autocomplete or checking the help for that function.\n\nF.3.3 Non-numeric argument to binary operator\n\n1 + \"A\"\n\nError in 1 + \"A\": non-numeric argument to binary operator\n\n\nWhen you try to apply mathematical operations to objects that aren’t numbers, you get this error. You might see this from a function that internally applies these operators; it just means that the person who wrote the function didn’t specifically check that the arguments you input were numeric and write a more specific error message, they just used what you provided and relied on the error messages from the binary operators. Either way, to solve this you need to figure out what should be numeric, but isn’t.\n\nF.3.4 Tibble columns must have compatible sizes\n\ntibble::tibble(\n  x = 1:2,\n  y = 1:3\n)\n\nError in `tibble::tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 2: Existing data.\n• Size 3: Column `y`.\nℹ Only values of size one are recycled.\n\n\nThis error occurs when you’re creating a table using tibble() and the columns have different lengths. You can set a column to a single value (i.e., a vector with length 1) and it will be “recycled” for every row, but you can’t give two columns values with different lengths if their lengths are greater than 1.\nThe same problem occurs if the function you’re using adds columns to a tibble. The tidyverse error messages are generall very useful in this case.\n\nmtcars3 &lt;- mutate(mtcars, newcol = 1:3)\n\nError in `mutate()`:\nℹ In argument: `newcol = 1:3`.\nCaused by error:\n! `newcol` must be size 32 or 1, not 3.\n\n\n\nF.3.5 Arguments imply differing number of rows\n\ndata.frame(\n  x = 1:2,\n  y = 1:3\n)\n\nError in data.frame(x = 1:2, y = 1:3): arguments imply differing number of rows: 2, 3\n\n\nA similar problem occurs if you’re using the base R function data.frame() (or the function you’re using does). The error message is different, but it’s the same problem. You will also see a related error message if you use base R techniques to add a column with a different length to the data frame.\n\nmtcars$newcol &lt;- 1:3\n\nError in `$&lt;-.data.frame`(`*tmp*`, newcol, value = 1:3): replacement has 3 rows, data has 32",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "app-debugging.html#debugging-methods",
    "href": "app-debugging.html#debugging-methods",
    "title": "Appendix F — Debugging",
    "section": "\nF.4 Debugging methods",
    "text": "F.4 Debugging methods\n\nF.4.1 Restart and rerun\nIt’s very useful to be able to run code interactively, but this can sometimes lead to confusion about what objects are available in your code. You might have made a data table called profits, and then decided to edit the code to make it slightly differently. If you forgot to re-run the code, you’ll be using the old table in your interactive code, but the new table when you knit.\nRestart R (under the Session menu) and run the code in order up to the chunk where you’re having a problem. You can use the Run menu in the upper right of the source pane to run all chunks above your cursor position.\n\nF.4.2 Comment out\nA useful method of debugging a tricky error is commenting out parts of your code and re-running the code to figure out exactly which code is causing the problem. Try\n\ndat &lt;- starwars %&gt;%\n  select(name, height, mass, species) %&gt;%\n  filter(Species == \"Droid\") %&gt;%\n  select(-species) %&gt;%\n  filter(mass &lt; 100)\n\nError in `filter()`:\nℹ In argument: `Species == \"Droid\"`.\nCaused by error:\n! object 'Species' not found\n\n\nImagine the error message was a bit less helpful. You can try running the code line by line. Either select just the code you want to run, or comment out the code you don’t want to run. Remember to also comment out linking functions at the end of lines, like the pipe (%&gt;%) or the ggplot plus (+).\n\ndat &lt;- starwars #%&gt;%\n  # select(name, height, mass, species) %&gt;%\n  # filter(Species == \"Droid\") %&gt;%\n  # select(-species) %&gt;%\n  # filter(mass &lt; 100)\n\n\n\n\n\n\n\nTip\n\n\n\nYou can comment out multiple lines by selecting them with your cursor and choosing Code &gt; Comment/Uncomment Lines (or using the keyboard shortcut).\n\n\nSelect more code or delete the comments until you locate the error.\n\ndat &lt;- starwars %&gt;%\n  select(name, height, mass, species) %&gt;%\n  filter(Species == \"Droid\") #%&gt;%\n\nError in `filter()`:\nℹ In argument: `Species == \"Droid\"`.\nCaused by error:\n! object 'Species' not found\n\n  # select(-species) %&gt;%\n  # filter(mass &lt; 100)\n\n\nF.4.3 Google the error\nMany error messages seem incomprehensible. Googling this message can often lead you to solutions. Take the famous example of “object of type ‘closure’ is not subsettable”.\n\ndata$x &lt;- 1\n\nError in data$x &lt;- 1: object of type 'closure' is not subsettable\n\n\nA Google search will show several sources explaining this confounding message and how to fix it. Although you may also find Jenny Bryan’s famous talk of the same name, which is an excellent discussion of troubleshooting in R.\n\n\n\n\n\n\nNote\n\n\n\nAn “object of type ‘closure’” is coding jargon for a function (like the type of 1 is numeric or the type of \"A\" is character). And “subsetting” is accessing part of a table using $ or square brackets. Here, it means that data isn’t a table, but actually a function, so you can’t add a column to it.\n\n\n\nF.4.4 Reproducible examples\nYou might see people in coding forums like StackOverflow asking for a “reprex”, or a reproducible example. This is the smallest, completely self-contained example of your problem or question.\nFor example, you may have a question about how to figure out how to select rows that contain the value “test” in a certain column, but it isn’t working. It’s clearer if you can provide a concrete example, but you don’t want to have to type out the whole table you’re using or all the code that got you to this point in your script.\nYou can include a very small table with just the basics or a smaller version of your problem. Make comments at each step about what you expect and what you actually got.\nWhich version is easier for you to figure out the solution?\n\n# this doesn't work\nno_test_data &lt;- data |&gt;\n  filter(!str_detect(type, \"test\"))\n\n… OR …\n\nlibrary(tidyverse)\n\n# with a minimal example table\ndata &lt;- tribble(\n  ~id, ~type, ~x,\n  1, \"test\", 12,\n  2, \"testosterone\", 15,\n  3, \"estrogen\", 10\n)\n\n# this should keep IDs 2 and 3, but removes ID 2\nno_test_data &lt;- data |&gt;\n  filter(!str_detect(type, \"test\"))\n\n# expected to be true\nall(no_test_data$type == c(\"testosterone\", \"estrogen\"))\n\nOne of the big benefits to creating a reprex is that you often solve your own problem while you’re trying to break it down to explain to someone else.\nIf you really want to go down the rabbit hole, you can create a reproducible example using the reprex package from tidyverse.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Debugging</span>"
    ]
  },
  {
    "objectID": "app-import.html",
    "href": "app-import.html",
    "title": "Appendix G — Data Import",
    "section": "",
    "text": "Intended Learning Outcomes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#sec-ilo-data",
    "href": "app-import.html#sec-ilo-data",
    "title": "Appendix G — Data Import",
    "section": "",
    "text": "Be able to inspect data\nImport data from a range of sources\nIdentify and handle common problems with data import",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#sec-setup-data",
    "href": "app-import.html#sec-setup-data",
    "title": "Appendix G — Data Import",
    "section": "Set-up",
    "text": "Set-up",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#sec-setup-func",
    "href": "app-import.html#sec-setup-func",
    "title": "Appendix G — Data Import",
    "section": "Setup",
    "text": "Setup\n\nOpen your reprores project\nCreate a new quarto file called app-data-import.qmd\n\nUpdate the YAML header\nReplace the setup chunk with the one below:\n\n\n```{r}\n#‎| label: setup\n#‎| include: false\nlibrary(tidyverse)     # includes readr & tibble\nlibrary(rio)           # for almost any data import/export\nlibrary(haven)         # for SPSS, Stata,and SAS files\nlibrary(readxl)        # for Excel files\nlibrary(googlesheets4) # for Google Sheets\n```\n\nDownload the Data import cheatsheet.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#sec-builtin",
    "href": "app-import.html#sec-builtin",
    "title": "Appendix G — Data Import",
    "section": "\nG.1 Built-in data",
    "text": "G.1 Built-in data\nYou’ll likely want to import you own data to work with, however, Base R also comes with built-in datasets and these can be very useful for learning new functions and packages. Additionally, some packages, like tidyr, also contain data. The data() function lists the datasets available.\n\n# list datasets built in to base R\ndata()\n\n# lists datasets in a specific package\ndata(package = \"tidyr\")\n\nType the name of a dataset into the console to see the data. For example, type ?table1 into the console to see the dataset description for table1, which is a dataset included with tidyr.\n\n?table1\n\nYou can also use the data() function to load a dataset into your global environment.\n\n# loads table1 into the environment\ndata(\"table1\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#looking-at-data",
    "href": "app-import.html#looking-at-data",
    "title": "Appendix G — Data Import",
    "section": "\nG.2 Looking at data",
    "text": "G.2 Looking at data\nNow that you’ve loaded some data, look the upper right hand window of RStudio, under the Environment tab. You will see the object table1 listed, along with the number of observations (rows) and variables (columns). This is your first check that everything went OK.\nAlways, always, always, look at your data once you’ve created or loaded a table. Also look at it after each step that transforms your table. There are three main ways to look at your table: View(), print(), tibble::glimpse().\n\nG.2.1 View()\nA familiar way to look at the table is given by View() (uppercase ‘V’), which opens up a data table in the console pane using a viewer that looks a bit like Excel. This command can be useful in the console, but don’t ever put this one in a script because it will create an annoying pop-up window when the user runs it. You can also click on an object in the environment pane to open it in the same interface. You can close the tab when you’re done looking at it; it won’t remove the object.\n\nView(table1)\n\n\nG.2.2 print()\nThe print() method can be run explicitly, but is more commonly called by just typing the variable name on a blank line. The default is not to print the entire table, but just the first 10 rows.\nLet’s look at the table1 table that we loaded above. Depending on how wide your screen is, you might need to click on an arrow at the right of the table to see the last column.\n\n# call print explicitly\nprint(table1)\n\n# more common method of just calling object name\ntable1\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\nG.2.3 glimpse()\nThe function tibble::glimpse() gives a sideways version of the table. This is useful if the table is very wide and you can’t easily see all of the columns. It also tells you the data type of each column in angled brackets after each column name.\n\nglimpse(table1)\n\nRows: 6\nColumns: 4\n$ country    &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Brazil\", \"Brazil\", \"China\", …\n$ year       &lt;dbl&gt; 1999, 2000, 1999, 2000, 1999, 2000\n$ cases      &lt;dbl&gt; 745, 2666, 37737, 80488, 212258, 213766\n$ population &lt;dbl&gt; 19987071, 20595360, 172006362, 174504898, 1272915272, 12804…\n\n\n\nG.2.4 summary()\nYou can get a quick summary of a dataset with the summary() function, which can be useful for spotting things like if the minimum or maximum values are clearly wrong, or if R thinks that a &lt;a href=‘https://psyteachr.github.io/glossary/n#nominal’ target=’_blank’ class=‘glossary’ title=‘Categorical variables that don’t have an inherent order, such as types of animal.’&gt;nominal variable is numeric. For example, if you had labelled gender as 1, 2, and 3 rather than male, female, and non-binary, summary() would calculate a mean and median even though this isn’t appropriate for the data. This can be a useful flag that you need to take further steps to correct your data.\nNote that because population is a very, very large number, R will use scientific notation.\n\nsummary(table1)\n\n   country               year          cases          population       \n Length:6           Min.   :1999   Min.   :   745   Min.   :1.999e+07  \n Class :character   1st Qu.:1999   1st Qu.: 11434   1st Qu.:5.845e+07  \n Mode  :character   Median :2000   Median : 59112   Median :1.733e+08  \n                    Mean   :2000   Mean   : 91277   Mean   :4.901e+08  \n                    3rd Qu.:2000   3rd Qu.:179316   3rd Qu.:9.983e+08  \n                    Max.   :2000   Max.   :213766   Max.   :1.280e+09",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#sec-import_data",
    "href": "app-import.html#sec-import_data",
    "title": "Appendix G — Data Import",
    "section": "\nG.3 Importing data",
    "text": "G.3 Importing data\nBuilt-in data are nice for examples, but you’re probably more interested in your own data. There are many different types of files that you might work with when doing data analysis. These different file types are usually distinguished by the three-letter extension following a period at the end of the file name (e.g., .xls).\nDownload this directory of data files, unzip the folder, and save the data directory in the 04-data project directory.\n\nG.3.1 rio::import()\nThe type of data files you have to work with will likely depend on the software that you typically use in your workflow. The rio package has very straightforward functions for reading and saving data in most common formats: rio::import() and rio::export().\n\ndemo_tsv  &lt;- import(\"data/demo.tsv\")  # tab-separated values\ndemo_csv  &lt;- import(\"data/demo.csv\")  # comma-separated values\ndemo_xls  &lt;- import(\"data/demo.xlsx\") # Excel format\ndemo_sav  &lt;- import(\"data/demo.sav\")  # SPSS format\n\n\nG.3.2 File type specific import\nHowever, it is also useful to know the specific functions that are used to import different file types because it is easier to discover features to deal with complicated cases, such as when you need to skip rows, rename columns, or choose which Excel sheet to use.\n\ndemo_tsv &lt;- readr::read_tsv(\"data/demo.tsv\")\ndemo_csv &lt;- readr::read_csv(\"data/demo.csv\")\ndemo_xls &lt;- readxl::read_excel(\"data/demo.xlsx\")\ndemo_sav &lt;- haven::read_sav(\"data/demo.sav\")\n\n\nLook at the help for each function above and read through the Arguments section to see how you can customise import.\n\nIf you keep data in Google Sheets, you can access it directly from R using &lt;pkg&gt;googlesheets4\", \"https://googlesheets4.tidyverse.org/\"). The code below imports data from a public sheet. You can set the ss argument to the entire URL for the target sheet, or just the section after “https://docs.google.com/spreadsheets/d/”.\n\ngs4_deauth() # skip authorisation for public data\n\ndemo_gs4  &lt;- googlesheets4::read_sheet(\n  ss = \"16dkq0YL0J7fyAwT1pdgj1bNNrheckAU_2-DKuuM6aGI\"\n)\n\n\nG.3.3 Column data types\nUse glimpse() to see how these different functions imported the data with slightly different data types. This is because the different file types store data slightly differently. For example, SPSS stores factors as numbers, so the factor column contains the values 1, 2, 3 rather than low, med, high. It also stores logical values as 0 and 1 instead or TRUE and FALSE.\n\nglimpse(demo_csv)\n\nRows: 6\nColumns: 6\n$ character &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    &lt;chr&gt; \"high\", \"low\", \"med\", \"high\", \"low\", \"med\"\n$ integer   &lt;dbl&gt; 1, 2, 3, 4, 5, 6\n$ double    &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, NA, TRUE\n$ date      &lt;date&gt; 2024-09-23, 2024-09-22, 2024-09-21, 2024-09-20, 2024-09-19, …\n\n\n\nglimpse(demo_xls)\n\nRows: 6\nColumns: 6\n$ character &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    &lt;chr&gt; \"high\", \"low\", \"med\", \"high\", \"low\", \"med\"\n$ integer   &lt;dbl&gt; 1, 2, 3, 4, 5, 6\n$ double    &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, NA, TRUE\n$ date      &lt;dttm&gt; 2024-09-23, 2024-09-22, 2024-09-21, 2024-09-20, 2024-09-19, …\n\n\n\nglimpse(demo_sav)\n\nRows: 6\nColumns: 6\n$ character &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    &lt;dbl+lbl&gt; 3, 1, 2, 3, 1, 2\n$ integer   &lt;dbl&gt; 1, 2, 3, 4, 5, 6\n$ double    &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   &lt;dbl&gt; 1, 1, 0, 0, NA, 1\n$ date      &lt;date&gt; 2024-09-23, 2024-09-22, 2024-09-21, 2024-09-20, 2024-09-19, …\n\n\n\nglimpse(demo_gs4)\n\nRows: 6\nColumns: 6\n$ character &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    &lt;chr&gt; \"high\", \"low\", \"med\", \"high\", \"low\", \"med\"\n$ integer   &lt;dbl&gt; 1, 2, 3, 4, 5, 6\n$ double    &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, NA, TRUE\n$ date      &lt;dttm&gt; 2021-11-22, 2021-11-21, 2021-11-20, 2021-11-19, 2021-11-18, …\n\n\nThe readr functions display a message when you import data explaining what data type each column is.\n\ndemo &lt;- readr::read_csv(\"data/demo.csv\")\n\nThe “Column specification” tells you which data type each column is. You can review data types in Appendix H. Options are:\n\n\nchr: character\n\n\ndbl: double\n\n\nlgl: logical\n\n\nint: integer\n\n\ndate: date\n\ndttm: date/time\n\nread_csv() will guess what type of data each variable is and normally it is pretty good at this. However, if it makes a mistake, such as reading the “date” column as a character, you can manually set the column data types.\nFirst, run spec() on the dataset which will give you the full column specification that you can copy and paste:\n\nspec(demo)\n\ncols(\n  character = col_character(),\n  factor = col_character(),\n  integer = col_double(),\n  double = col_double(),\n  logical = col_logical(),\n  date = col_date(format = \"\")\n)\n\n\nThen, we create an object using the code we just copied that lists the correct column types. Factor columns will always import as character data types, so you have to set their data type manually with col_factor() and set the order of levels with the levels argument. Otherwise, the order defaults to the order they appear in the dataset. For our demo dataset, we will tell R that the factor variable is a factor by using col_factor() and we can also specify the order of the levels so that they don’t just appear alphabetically. Additionally, we can also specify exactly what format our date variable is in using %Y-%m-%d.\nWe then save this column specification to an object, and then add this to the col_types argument when we call read_csv().\n\ncorrected_cols &lt;- cols(\n  character = col_character(),\n  factor = col_factor(levels = c(\"low\", \"med\", \"high\")),\n  integer = col_integer(),\n  double = col_double(),\n  logical = col_logical(),\n  date = col_date(format = \"%Y-%m-%d\")\n)\n\ndemo &lt;- readr::read_csv(\"data/demo.csv\", col_types = corrected_cols)\n\n\n\n\n\n\n\nNote\n\n\n\nFor dates, you might need to set the format your dates are in. See ?strptime for a list of the codes used to represent different date formats. For example, \"%d-%b-%y\" means that the dates are formatted like 31-Jan-21.\n\n\nThe functions from readxl for loading .xlsx sheets have a different, more limited way to specify the column types. You will have to convert factor columns and dates using mutate(), which you’ll learn about in Chapter 7, so most people let read_excel() guess data types and don’t set the col_types argument.\nFor SPSS data, whilst rio::import() will just read the numeric values of factors and not their labels, the function read_sav() from haven reads both. However, you have to convert factors from a haven-specific “labelled double” to a factor (we have no idea why haven doesn’t do this for you).\n\ndemo_sav$factor &lt;- haven::as_factor(demo_sav$factor)\n\nglimpse(demo_sav)\n\nRows: 6\nColumns: 6\n$ character &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n$ factor    &lt;fct&gt; high, low, med, high, low, med\n$ integer   &lt;dbl&gt; 1, 2, 3, 4, 5, 6\n$ double    &lt;dbl&gt; 1.5, 2.5, 3.5, 4.5, 5.5, 6.5\n$ logical   &lt;dbl&gt; 1, 1, 0, 0, NA, 1\n$ date      &lt;date&gt; 2024-09-23, 2024-09-22, 2024-09-21, 2024-09-20, 2024-09-19, …\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe way you specify column types for googlesheets4 is a little different from readr, although you can also use the shortcodes described in the help for read_sheet() with readr functions. There is currently no column specification for factors.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#creating-data",
    "href": "app-import.html#creating-data",
    "title": "Appendix G — Data Import",
    "section": "\nG.4 Creating data",
    "text": "G.4 Creating data\nIf you need to create a small data table from scratch in R, use the tibble::tibble() function, and type the data right in. The tibble package is part of the tidyverse package that we loaded at the start of this chapter.\nLet’s create a small table with the names of three Avatar characters and their bending type. The tibble() function takes arguments with the names that you want your columns to have. The values are vectors that list the column values in order.\nIf you don’t know the value for one of the cells, you can enter NA, which we have to do for Sokka because he doesn’t have any bending ability. If all the values in the column are the same, you can just enter one value and it will be copied for each row.\n\navatar &lt;- tibble(\n  name = c(\"Katara\", \"Toph\", \"Sokka\"),\n  bends = c(\"water\", \"earth\", NA),\n  friendly = TRUE\n)\n\n# print it\navatar\n\n\n\n\nname\nbends\nfriendly\n\n\n\nKatara\nwater\nTRUE\n\n\nToph\nearth\nTRUE\n\n\nSokka\nNA\nTRUE\n\n\n\n\n\n\nYou can also use the tibble::tribble() function to create a table by row, rather than by column. You start by listing the column names, each preceded by a tilde (~), then you list the values for each column, row by row, separated by commas (don’t forget a comma at the end of each row).\n\navatar_by_row &lt;- tribble(\n  ~name,    ~bends,  ~friendly,\n  \"Katara\", \"water\", TRUE,\n  \"Toph\",   \"earth\", TRUE,\n  \"Sokka\",  NA,      TRUE\n)\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t have to line up the columns in a tribble, but it can make it easier to spot errors.\n\n\nYou may not need to do this very often if you are primarily working with data that you import from spreadsheets, but it is useful to know how to do it anyway.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#writing-data",
    "href": "app-import.html#writing-data",
    "title": "Appendix G — Data Import",
    "section": "\nG.5 Writing data",
    "text": "G.5 Writing data\nIf you have data that you want to save, use rio::export(), as follows.\n\nexport(avatar, \"data/avatar.csv\")\n\nThis will save the data in CSV format to your working directory.\nWriting to Google Sheets is a little trickier (if you never use Google Sheets feel free to skip this section). Even if a Google Sheet is publicly editable, you can’t add data to it without authorising your account.\nYou can authorise interactively using the following code (and your own email), which will prompt you to authorise “Tidyverse API Packages” the first time you do this. If you don’t tick the checkbox authorising it to “See, edit, create, and delete all your Google Sheets spreadsheets”, the next steps will fail.\n\n# authorise your account \n# this only needs to be done once per script\ngs4_auth(email = \"myemail@gmail.com\")\n\n# create a new sheet\nsheet_id &lt;- gs4_create(name = \"demo-file\", \n                       sheets = \"letters\")\n\n# define the data table to save\nletter_data &lt;- tibble(\n  character = LETTERS[1:5],\n  integer = 1:5,\n  double = c(1.1, 2.2, 3.3, 4.4, 5.5),\n  logical = c(T, F, T, F, T),\n  date = lubridate::today()\n)\n\nwrite_sheet(data = letter_data, \n            ss = sheet_id, \n            sheet = \"letters\")\n\n## append some data\nnew_data &lt;- tibble(\n  character = \"F\",\n  integer = 6L,\n  double = 6.6,\n  logical = FALSE,\n  date = lubridate::today()\n)\nsheet_append(data = new_data,\n             ss = sheet_id,\n             sheet = \"letters\")\n\n# read the data\ndemo &lt;- read_sheet(ss = sheet_id, sheet = \"letters\")\n\n\n\nCreate a new table called family with the first name, last name, and age of your family members (biological, adopted, or chosen).\nSave it to a CSV file called “family.csv”.\nClear the object from your environment by restarting R or with the code remove(family).\nLoad the data back in and view it.\n\n\n\n\nSolution\n\n# create the table\nfamily &lt;- tribble(\n  ~first_name, ~last_name, ~age,\n  \"Lisa\", \"DeBruine\", 45,\n  \"Robbie\", \"Jones\", 14\n)\n\n# save the data to CSV\nexport(family, \"data/family.csv\")\n\n# remove the object from the environment\nremove(family)\n\n# load the data\nfamily &lt;- import(\"data/family.csv\")\n\n\n\nWe’ll be working with tabular data a lot in this class, but tabular data is made up of vectors, which groups together data with the same basic data type. Appendix H explains some of this terminology to help you understand the functions we’ll be learning to process and analyse data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#troubleshooting",
    "href": "app-import.html#troubleshooting",
    "title": "Appendix G — Data Import",
    "section": "\nG.6 Troubleshooting",
    "text": "G.6 Troubleshooting\nWhat if you import some data and it guesses the wrong column type? The most common reason is that a numeric column has some non-numbers in it somewhere. Maybe someone wrote a note in an otherwise numeric column. Columns have to be all one data type, so if there are any characters, the whole column is converted to character strings, and numbers like 1.2 get represented as \"1.2\", which will cause very weird errors like \"100\" &lt; \"9\" == TRUE. You can catch this by using glimpse() to check your data.\nThe data directory you downloaded contains a file called “mess.csv”. Let’s try loading this dataset.\n\nmess &lt;- rio::import(\"data/mess.csv\")\n\nWarning in (function (input = \"\", file = NULL, text = NULL, cmd = NULL, :\nStopped early on line 5. Expected 7 fields but found 0. Consider fill=TRUE and\ncomment.char=. First discarded non-empty line: &lt;&lt;junk,missing,0.72,b,1,2 -\n3,2020-01-2&gt;&gt;\n\n\nWhen importing goes wrong, it’s often easier to fix it using the specific importing function for that file type (e.g., use read_csv() rather than rio::import(). This is because the problems tend to be specific to the file format and you can look up the help for these functions more easily. For CSV files, the import function is readr::read_csv.\n\n# lazy = FALSE loads the data right away so you can see error messages\n# this default changed in late 2021 and might change back soon\nmess &lt;- read_csv(\"data/mess.csv\", lazy = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nYou’ll get a warning about parsing issues and the data table is just a single column. View the file data/mess.csv by clicking on it in the File pane, and choosing “View File”. Here are the first 10 lines. What went wrong?\n\n\n\nThis is my messy dataset\n\n\n\njunk,order,score,letter,good,min_max,date\n\n\njunk,1,-1,a,1,1 - 2,2020-01-1\n\n\njunk,missing,0.72,b,1,2 - 3,2020-01-2\n\n\njunk,3,-0.62,c,FALSE,3 - 4,2020-01-3\n\n\njunk,4,2.03,d,T,4 - 5,2020-01-4\n\n\njunk,5,NA,e,1,5 - 6,2020-01-5\n\n\n\n\nFirst, the file starts with a note: “This is my messy dataset” and then a blank line. The first line of data should be the column headings, so we want to skip the first two lines. You can do this with the argument skip in read_csv().\n\nmess &lt;- read_csv(\"data/mess.csv\", \n                 skip = 2,\n                 lazy = FALSE)\nglimpse(mess)\n\nRows: 26\nColumns: 7\n$ junk    &lt;chr&gt; \"junk\", \"junk\", \"junk\", \"junk\", \"junk\", \"junk\", \"junk\", \"junk\"…\n$ order   &lt;chr&gt; \"1\", \"missing\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\",…\n$ score   &lt;dbl&gt; -1.00, 0.72, -0.62, 2.03, NA, 0.99, 0.03, 0.67, 0.57, 0.90, -1…\n$ letter  &lt;chr&gt; \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m…\n$ good    &lt;chr&gt; \"1\", \"1\", \"FALSE\", \"T\", \"1\", \"0\", \"T\", \"TRUE\", \"1\", \"T\", \"F\", …\n$ min_max &lt;chr&gt; \"1 - 2\", \"2 - 3\", \"3 - 4\", \"4 - 5\", \"5 - 6\", \"6 - 7\", \"7 - 8\",…\n$ date    &lt;chr&gt; \"2020-01-1\", \"2020-01-2\", \"2020-01-3\", \"2020-01-4\", \"2020-01-5…\n\n\nOK, that’s a little better, but this table is still a serious mess in several ways:\n\n\njunk is a column that we don’t need\n\norder should be an integer column\n\ngood should be a logical column\n\ngood uses all kinds of different ways to record TRUE and FALSE values\n\nmin_max contains two pieces of numeric information, but is a character column\n\ndate should be a date column\n\nWe’ll learn how to deal with this mess in Chapter 6 and Chapter 7, but we can fix a few things by setting the col_types argument in read_csv() to specify the column types for our two columns that were guessed wrong and skip the “junk” column. The argument col_types takes a list where the name of each item in the list is a column name and the value is from the table below. You can use the function, like col_double() or the abbreviation, like \"d\"; for consistency with earlier in this chapter we will use the function names. Omitted column names are guessed.\n\n\nfunction\n\nabbreviation\n\n\n\ncol_logical()\nl\nlogical values\n\n\ncol_integer()\ni\ninteger values\n\n\ncol_double()\nd\nnumeric values\n\n\ncol_character()\nc\nstrings\n\n\ncol_factor(levels, ordered)\nf\na fixed set of values\n\n\ncol_date(format = ““)\nD\nwith the locale’s date_format\n\n\ncol_time(format = ““)\nt\nwith the locale’s time_format\n\n\ncol_datetime(format = ““)\nT\nISO8601 date time\n\n\ncol_number()\nn\nnumbers containing the grouping_mark\n\n\ncol_skip()\n_, -\ndon’t import this column\n\n\ncol_guess()\n?\nparse using the “best” type based on the input\n\n\n\n\n# omitted values are guessed\n# ?col_date for format options\nct &lt;- cols(\n  junk = col_skip(), # skip this column\n  order = col_integer(),\n  good = col_logical(),\n  date = col_date(format = \"%Y-%m-%d\")\n)\n\ntidier &lt;- read_csv(\"data/mess.csv\", \n                   skip = 2,\n                   col_types = ct,\n                   lazy = FALSE)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nYou will get a message about parsing issues when you run this that tells you to run the problems() function to see a table of the problems. Warnings look scary at first, but always start by reading the message.\n\nproblems()\n\n\n\n\n\n\nrow\ncol\nexpected\nactual\nfile\n\n\n3\n2\nan integer\nmissing\ndata/mess.csv\n\n\n\n\n\nThe output of problems() tells you what row (3) and column (2) the error was found in, what kind of data was expected (an integer), and what the actual value was (missing). If you specifically tell read_csv() to import a column as an integer, any characters (i.e., not numbers) in the column will produce a warning like this and then be recorded as NA. You can manually set what missing values are recorded as with the na argument.\n\ntidiest &lt;- read_csv(\"data/mess.csv\", \n                   skip = 2,\n                   na = \"missing\",\n                   col_types = ct,\n                   lazy = FALSE)\n\nNow order is an integer variable where any empty cells contain NA. The variable good is a logical value, where 0 and F are converted to FALSE, while 1 and T are converted to TRUE. The variable date is a date type (adding leading zeros to the day). We’ll learn in later chapters how to fix other problems, such as the min_max column containing two different types of data.\n\n\n\n\n\norder\nscore\nletter\ngood\nmin_max\ndate\n\n\n\n1\n-1\na\nTRUE\n1 - 2\n2020-01-01\n\n\nNA\n0.72\nb\nTRUE\n2 - 3\n2020-01-02\n\n\n3\n-0.62\nc\nFALSE\n3 - 4\n2020-01-03\n\n\n4\n2.03\nd\nTRUE\n4 - 5\n2020-01-04\n\n\n5\nNA\ne\nTRUE\n5 - 6\n2020-01-05\n\n\n6\n0.99\nf\nFALSE\n6 - 7\n2020-01-06",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#working-with-real-data",
    "href": "app-import.html#working-with-real-data",
    "title": "Appendix G — Data Import",
    "section": "\nG.7 Working with real data",
    "text": "G.7 Working with real data\nIt’s worth highlighting at this point that working with real data can be difficult because each dataset can be messy in its own way. Throughout this course we will show you common errors and how to fix them, but be prepared that when you start with working your own data, you’ll likely come across problems we don’t cover in the course and that’s just part of joy of learning programming. You’ll also get better at looking up solutions using sites like Stack Overflow and there’s a fantastic #rstats community on Twitter you can ask for help.\nYou may also be tempted to fix messy datasets by, for example, opening up Excel and editing them there. Whilst this might seem easier in the short term, there’s two serious issues with doing this. First, you will likely work with datasets that have recurring messy problems. By taking the time to solve these problems with code, you can apply the same solutions to a large number of future datasets so it’s more efficient in the long run. Second, if you edit the spreadsheet, there’s no record of what you did. By solving these problems with code, you do so reproducibly and you don’t edit the original data file. This means that if you make an error, you haven’t lost the original data and can recover.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#exercises",
    "href": "app-import.html#exercises",
    "title": "Appendix G — Data Import",
    "section": "\nG.8 Exercises",
    "text": "G.8 Exercises\nFor the final step in this chapter, we will create a report using one of the in-built datasets to practice the skills you have used so far. You may need to refer back to previous chapters to help you complete these exercises and you may also want to take a break before you work through this section. We’d also recommend you knit at every step so that you can see how your output changes.\n\nG.8.1 New Markdown\nCreate and save a new R Markdown document named starwars_report.Rmd. In the set-up code chunk load the packages tidyverse and rio.\nWe’re going to use the built-in starwars dataset that contains data about Star Wars characters. You can learn more about the dataset by using the ?help function.\n\nG.8.2 Import and export the dataset\n\nFirst, load the in-built dataset into the environment. Type and run the code to do this in the console; do not save it in your Markdown.\n\nThen, export the dataset to a .csv file and save it in your data directory. Again, do this in the console.\nFinally, import this version of the dataset using read_csv() to an object named starwars - you can put this code in your Markdown.\n\n\n\nSolution\n\n\ndata(starwars)\nexport(starwars, \"data/starwars.csv\")\nstarwars &lt;- read_csv(\"data/starwars.csv\")\n\n\n\nG.8.3 Convert column types\n\nCheck the column specification of starwars.\nCreate a new column specification that lists the following columns as factors: hair_color, skin_color, eye_color, sex, gender, homeworld, and species and skips the following columns: films, vehicles, and starships (this is because these columns contain multiple values and are stored as lists, which we haven’t covered how to work with). You do not have to set the factor orders (although you can if you wish).\nRe-import the dataset, this time with the corrected column types.\n\n\n\nSolution\n\n\nspec(starwars)\ncorrected_cols &lt;- cols(\n  name = col_character(),\n  height = col_double(),\n  mass = col_double(),\n  hair_color = col_factor(),\n  skin_color = col_factor(),\n  eye_color = col_factor(),\n  birth_year = col_double(),\n  sex = col_factor(),\n  gender = col_factor(),\n  homeworld = col_factor(),\n  species = col_factor(),\n  films = col_skip(),\n  vehicles = col_skip(),\n  starships = col_skip()\n)\n\nstarwars &lt;- read_csv(\"data/starwars.csv\", col_types = corrected_cols)\n\ncols(\n  name = col_character(),\n  height = col_double(),\n  mass = col_double(),\n  hair_color = col_character(),\n  skin_color = col_character(),\n  eye_color = col_character(),\n  birth_year = col_double(),\n  sex = col_character(),\n  gender = col_character(),\n  homeworld = col_character(),\n  species = col_character(),\n  films = col_character(),\n  vehicles = col_character(),\n  starships = col_character()\n)\n\n\n\n\nG.8.4 Plots\nProduce the following plots and one plot of your own choosing. Write a brief summary of what each plot shows and any conclusions you might reach from the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\nggplot(starwars, aes(height)) +\n  geom_histogram(binwidth = 25, colour = \"black\", alpha = .3) +\n  scale_x_continuous(breaks = seq(from = 50, to = 300, by = 25)) +\n  labs(title = \"Height (cm) distribution of Star Wars Characters\") +\n  theme_classic()\n\n\nggplot(starwars, aes(height, mass)) +\n  geom_point() +\n  labs(title = \"Mass (kg) by height (cm) distribution of Star Wars Characters\") +\n  theme_classic() +\n  scale_x_continuous(breaks = seq(from = 0, to = 300, by = 50)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 2000, by = 100)) +\n  coord_cartesian(xlim = c(0, 300))\n\n\nggplot(starwars, aes(x = gender, fill = gender)) +\n  geom_bar(show.legend = FALSE, colour = \"black\") +\n  scale_x_discrete(name = \"Gender of character\", labels = (c(\"Masculine\", \"Feminine\", \"Missing\"))) +\n  scale_fill_brewer(palette = 2) +\n  labs(title = \"Number of Star Wars characters of each gender\") +\n  theme_bw()\n\n\n\nG.8.5 Make it look nice\n\nAdd at least one Star Wars related image from an online source\nHide the code and any messages from the knitted output\nResize any images as you see fit\n\n\n\nSolution\n\n\nknitr::include_graphics(\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Star_wars2.svg/2880px-Star_wars2.svg.png\")\n\n\n\n\n\nAdaptation of Star Wars logo created by Weweje; original logo by Suzy Rice, 1976. CC-BY-3.0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#sec-glossary-data",
    "href": "app-import.html#sec-glossary-data",
    "title": "Appendix G — Data Import",
    "section": "Glossary",
    "text": "Glossary\n\n\nterm\ndefinition\n\n\n\nargument\nA variable that provides input to a function.\n\n\ncharacter\nA data type representing strings of text.\n\n\nconsole\nThe pane in RStudio where you can type in commands and view output messages.\n\n\ndata-type\nThe kind of data represented by an object.\n\n\ndouble\nA data type representing a real decimal number\n\n\nextension\nThe end part of a file name that tells you what type of file it is (e.g., .R or .Rmd).\n\n\nglobal-environment\nThe interactive workspace where your script runs\n\n\ninteger\nA data type representing whole numbers.\n\n\nlogical\nA data type representing TRUE or FALSE values.\n\n\nna\nA missing value that is “Not Available”\n\n\nnominal\nCategorical variables that don't have an inherent order, such as types of animal.\n\n\nnumeric\nA data type representing a real decimal number or integer.\n\n\npanes\nRStudio is arranged with four window “panes”.\n\n\ntabular-data\nData in a rectangular table format, where each row has an entry for each column.\n\n\ntidyverse\nA set of R packages that help you create and work with tidy data\n\n\nurl\nThe address of a web page (uniform resource locator)\n\n\nvector\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-import.html#sec-resources-data",
    "href": "app-import.html#sec-resources-data",
    "title": "Appendix G — Data Import",
    "section": "Further resources",
    "text": "Further resources\n\nData import cheatsheet\n\nChapter 11: Data Import in R for Data Science\n\nMulti-row headers",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Data Import</span>"
    ]
  },
  {
    "objectID": "app-datatypes.html",
    "href": "app-datatypes.html",
    "title": "Appendix H — Data Types",
    "section": "",
    "text": "H.1 Basic data types\nData can be numbers, words, true/false values or combinations of these. The basic data types in R are: numeric, character, and logical, as well as the special classes of factor and date/times.\nData types are like the categories when you format cells in Excel.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "app-datatypes.html#basic-data-types",
    "href": "app-datatypes.html#basic-data-types",
    "title": "Appendix H — Data Types",
    "section": "",
    "text": "H.1.1 Numeric data\nAll of the numbers are numeric data types. There are two types of numeric data, integer and double. Integers are the whole numbers, like -1, 0 and 1. Doubles are numbers that can have fractional amounts. If you just type a plain number such as 10, it is stored as a double, even if it doesn’t have a decimal point. If you want it to be an exact integer, you can use the L suffix (10L), but this distinction doesn’t make much difference in practice.\nIf you ever want to know the data type of something, use the typeof function.\n\ntypeof(10)   # double\ntypeof(10.0) # double\ntypeof(10L)  # integer\n\n[1] \"double\"\n[1] \"double\"\n[1] \"integer\"\n\n\nIf you want to know if something is numeric (a double or an integer), you can use the function is.numeric() and it will tell you if it is numeric (TRUE) or not (FALSE).\n\nis.numeric(10L)\nis.numeric(10.0)\nis.numeric(\"Not a number\")\n\n[1] TRUE\n[1] TRUE\n[1] FALSE\n\n\n\nH.1.2 Character data\nCharacters (also called “strings”) are any text between quotation marks.\n\ntypeof(\"This is a character string\")\ntypeof('You can use double or single quotes')\n\n[1] \"character\"\n[1] \"character\"\n\n\nThis can include quotes, but you have to escape quotes using a backslash to signal that the quote isn’t meant to be the end of the string.\n\nmy_string &lt;- \"The instructor said, \\\"R is cool,\\\" and the class agreed.\"\ncat(my_string) # cat() prints the arguments\n\nThe instructor said, \"R is cool,\" and the class agreed.\n\n\n\nH.1.3 Logical Data\nLogical data (also sometimes called “boolean” values) is one of two values: true or false. In R, we always write them in uppercase: TRUE and FALSE.\n\nclass(TRUE)\nclass(FALSE)\n\n[1] \"logical\"\n[1] \"logical\"\n\n\nWhen you compare two values with an operator, such as checking to see if 10 is greater than 5, the resulting value is logical.\n\nis.logical(10 &gt; 5)\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might also see logical values abbreviated as T and F, or 0 and 1. This can cause some problems down the road, so we will always spell out the whole thing.\n\n\n\nH.1.4 Factors\nA factor is a specific type of integer that lets you specify the categories and their order. This is useful in data tables to make plots display with categories in the correct order.\n\nmyfactor &lt;- factor(\"B\", levels = c(\"A\", \"B\",\"C\"))\nmyfactor\n\n[1] B\nLevels: A B C\n\n\nFactors are a type of integer, but you can tell that they are factors by checking their class().\n\ntypeof(myfactor)\nclass(myfactor)\n\n[1] \"integer\"\n[1] \"factor\"\n\n\n\nH.1.5 Dates and Times\nDates and times are represented by doubles with special classes. Although typeof() will tell you they are a double, you can tell that they are dates by checking their class(). Datetimes can have one or more of a few classes that start with POSIX.\n\ndate &lt;- as.Date(\"2022-01-24\")\ndatetime &lt;- ISOdatetime(2022, 1, 24, 10, 35, 00, \"GMT\")\ntypeof(date)\ntypeof(datetime)\nclass(date)\nclass(datetime)\n\n[1] \"double\"\n[1] \"double\"\n[1] \"Date\"\n[1] \"POSIXct\" \"POSIXt\" \n\n\nSee Appendix I for how to use lubridate to work with dates and times.\n\nWhat data types are these:\n\n\n100 \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n100L \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n\"100\" \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n100.0 \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n-100L \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\nfactor(100) \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\nTRUE \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n\"TRUE\" \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\nFALSE \ninteger\ndouble\ncharacter\nlogical\nfactor\n\n\n1 == 2 \ninteger\ndouble\ncharacter\nlogical\nfactor",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "app-datatypes.html#sec-containers",
    "href": "app-datatypes.html#sec-containers",
    "title": "Appendix H — Data Types",
    "section": "\nH.2 Basic container types",
    "text": "H.2 Basic container types\nIndividual data values can be grouped together into containers. The main types of containers we’ll work with are vectors, lists, and data tables.\n\nH.2.1 Vectors\nA vector in R is a set of items (or ‘elements’) in a specific order. All of the elements in a vector must be of the same data type (numeric, character, logical). You can create a vector by enclosing the elements in the function c().\n\n## put information into a vector using c(...)\nc(1, 2, 3, 4)\nc(\"this\", \"is\", \"cool\")\n1:6 # shortcut to make a vector of all integers x:y\n\n[1] 1 2 3 4\n[1] \"this\" \"is\"   \"cool\"\n[1] 1 2 3 4 5 6\n\n\n\nWhat happens when you mix types? What class is the variable mixed?\n\nmixed &lt;- c(2, \"good\", 2L, \"b\", TRUE)\n\n\n\n\nSolution\n\ntypeof(mixed)\n\n[1] \"character\"\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can’t mix data types in a vector; all elements of the vector must be the same data type. If you mix them, R will coerce them so that they are all the same. If you mix doubles and integers, the integers will be changed to doubles. If you mix characters and numeric types, the numbers will be coerced to characters, so 10 would turn into \"10\".\n\n\n\nH.2.1.1 Selecting values from a vector\nIf we wanted to pick specific values out of a vector by position, we can use square brackets (an extract operator, or []) after the vector.\n\nvalues &lt;- c(10, 20, 30, 40, 50)\nvalues[2] # selects the second value\n\n[1] 20\n\n\nYou can select more than one value from the vector by putting a vector of numbers inside the square brackets. For example, you can select the 18th, 19th, 20th, 21st, 4th, 9th and 15th letter from the built-in vector LETTERS (which gives all the uppercase letters in the Latin alphabet).\n\nword &lt;- c(18, 19, 20, 21, 4, 9, 15)\nLETTERS[word]\n\n[1] \"R\" \"S\" \"T\" \"U\" \"D\" \"I\" \"O\"\n\n\n\nCan you decode the secret message?\n\nsecret &lt;- c(14, 5, 22, 5, 18, 7, 15, 14, 14, 1, 7, 9, 22, 5, 25, 15, 21, 21, 16)\n\n\n\n\nSolution\n\nLETTERS[secret]\n\n [1] \"N\" \"E\" \"V\" \"E\" \"R\" \"G\" \"O\" \"N\" \"N\" \"A\" \"G\" \"I\" \"V\" \"E\" \"Y\" \"O\" \"U\" \"U\" \"P\"\n\n\n\n\nYou can also create ‘named’ vectors, where each element has a name. For example:\n\nvec &lt;- c(first = 77.9, second = -13.2, third = 100.1)\nvec\n\n first second  third \n  77.9  -13.2  100.1 \n\n\nWe can then access elements by name using a character vector within the square brackets. We can put them in any order we want, and we can repeat elements:\n\nvec[c(\"third\", \"second\", \"second\")]\n\n third second second \n 100.1  -13.2  -13.2 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can get the vector of names using the names() function, and we can set or change them using something like names(vec2) &lt;- c(\"n1\", \"n2\", \"n3\").\n\n\nAnother way to access elements is by using a logical vector within the square brackets. This will pull out the elements of the vector for which the corresponding element of the logical vector is TRUE. If the logical vector doesn’t have the same length as the original, it will repeat. You can find out how long a vector is using the length() function.\n\nlength(LETTERS)\nLETTERS[c(TRUE, FALSE)]\n\n[1] 26\n [1] \"A\" \"C\" \"E\" \"G\" \"I\" \"K\" \"M\" \"O\" \"Q\" \"S\" \"U\" \"W\" \"Y\"\n\n\n\nH.2.1.2 Repeating Sequences\nHere are some useful tricks to save typing when creating vectors.\nIn the command x:y the : operator would give you the sequence of number starting at x, and going to y in increments of 1.\n\n1:10\n15.3:20.5\n0:-10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n[1] 15.3 16.3 17.3 18.3 19.3 20.3\n [1]   0  -1  -2  -3  -4  -5  -6  -7  -8  -9 -10\n\n\nWhat if you want to create a sequence but with something other than integer steps? You can use the seq() function. Look at the examples below and work out what the arguments do.\n\nseq(from = -1, to = 1, by = 0.2)\nseq(0, 100, length.out = 11)\nseq(0, 10, along.with = LETTERS)\n\n [1] -1.0 -0.8 -0.6 -0.4 -0.2  0.0  0.2  0.4  0.6  0.8  1.0\n [1]   0  10  20  30  40  50  60  70  80  90 100\n [1]  0.0  0.4  0.8  1.2  1.6  2.0  2.4  2.8  3.2  3.6  4.0  4.4  4.8  5.2  5.6\n[16]  6.0  6.4  6.8  7.2  7.6  8.0  8.4  8.8  9.2  9.6 10.0\n\n\nWhat if you want to repeat a vector many times? You could either type it out (painful) or use the rep() function, which can repeat vectors in different ways.\n\nrep(0, 10)                      # ten zeroes\nrep(c(1L, 3L), times = 7)       # alternating 1 and 3, 7 times\nrep(c(\"A\", \"B\", \"C\"), each = 2) # A to C, 2 times each\n\n [1] 0 0 0 0 0 0 0 0 0 0\n [1] 1 3 1 3 1 3 1 3 1 3 1 3 1 3\n[1] \"A\" \"A\" \"B\" \"B\" \"C\" \"C\"\n\n\nThe rep() function is useful to create a vector of logical values (TRUE/FALSE or 1/0) to select values from another vector.\n\n# Get IDs in the pattern Y Y N N ...\nids &lt;- 1:40\nyynn &lt;- rep(c(TRUE, FALSE), each = 2, \n            length.out = length(ids))\nids[yynn]\n\n [1]  1  2  5  6  9 10 13 14 17 18 21 22 25 26 29 30 33 34 37 38\n\n\n\nH.2.2 Lists\nRecall that vectors can contain data of only one type. What if you want to store a collection of data of different data types? For that purpose you would use a list. Define a list using the list() function.\n\ndata_types &lt;- list(\n  double = 10.0,\n  integer = 10L,\n  character = \"10\",\n  logical = TRUE\n)\n\nstr(data_types) # str() prints lists in a condensed format\n\nList of 4\n $ double   : num 10\n $ integer  : int 10\n $ character: chr \"10\"\n $ logical  : logi TRUE\n\n\nYou can refer to elements of a list using square brackets like a vector, but you can also use the dollar sign notation ($) if the list items have names.\n\ndata_types$logical\n\n[1] TRUE\n\n\n\nExplore the 5 ways shown below to extract a value from a list. What data type is each object? What is the difference between the single and double brackets? Which one is the same as the dollar sign?\n\nbracket1 &lt;- data_types[1]\nbracket2 &lt;- data_types[[1]]\nname1    &lt;- data_types[\"double\"]\nname2    &lt;- data_types[[\"double\"]]\ndollar   &lt;- data_types$double\n\n\nThe single brackets (bracket1 and name1) return a list with the subset of items inside the brackets. In this case, that’s just one item, but can be more (try data_types[1:2]). The items keep their names if they have them, so the returned value is list(double = 10).\nThe double brackets (bracket2 and name2 return a single item as a vector. You can’t select more than one item; data_types[[1:2]] will give you a “subscript out of bounds” error.\nThe dollar-sign notation is the same as double-brackets. If the name has spaces or any characters other than letters, numbers, underscores, and full stops, you need to surround the name with backticks (e.g., sales$`Customer ID`).\n\nH.2.3 Tables\nTabular data structures allow for a collection of data of different types (characters, integers, logical, etc.) but subject to the constraint that each “column” of the table (element of the list) must have the same number of elements. The base R version of a table is called a data.frame, while the ‘tidyverse’ version is called a tibble. Tibbles are far easier to work with, so we’ll be using those. To learn more about differences between these two data structures, see vignette(\"tibble\").\n\nlibrary(tidyverse) # loads the tibble package\n\n# construct a table by column with tibble\navatar &lt;- tibble(\n  name = c(\"Katara\", \"Toph\", \"Sokka\"),\n  bends = c(\"water\", \"earth\", NA),\n  friendly = TRUE\n)\n\n# or by row with tribble\navatar &lt;- tribble(\n  ~name,    ~bends,  ~friendly,\n  \"Katara\", \"water\", TRUE,\n  \"Toph\",   \"earth\", TRUE,\n  \"Sokka\",  NA,      TRUE\n)\n\n\n# export the data to a file\nrio::export(avatar, \"data/avatar.csv\")\n\n# or by importing data from a file\navatar &lt;- rio::import(\"data/avatar.csv\")\n\nTabular data becomes especially important for when we talk about tidy data in Chapter 6, which consists of a set of simple principles for structuring data.\n\nH.2.3.1 Table info\nWe can get information about the table using the following functions.\n\n\nncol(): number of columns\n\nnrow(): number of rows\n\ndim(): the number of rows and number of columns\n\nname(): the column names\n\nglimpse(): the column types\n\n\nnrow(avatar)\nncol(avatar)\ndim(avatar)\nnames(avatar)\nglimpse(avatar)\n\n[1] 3\n[1] 3\n[1] 3 3\n[1] \"name\"     \"bends\"    \"friendly\"\nRows: 3\nColumns: 3\n$ name     &lt;chr&gt; \"Katara\", \"Toph\", \"Sokka\"\n$ bends    &lt;chr&gt; \"water\", \"earth\", NA\n$ friendly &lt;lgl&gt; TRUE, TRUE, TRUE\n\n\n\nH.2.3.2 Accessing rows and columns\nThere are various ways of accessing specific columns or rows from a table. You’ll be learning more about this in Chapter 6 and Chapter 7.\n\nsiblings   &lt;- avatar %&gt;% slice(1, 3) # rows (by number)\nbends      &lt;- avatar %&gt;% pull(2) # column vector (by number)\nfriendly   &lt;- avatar %&gt;% pull(friendly) # column vector (by name)\nbends_name &lt;- avatar %&gt;% select(bends, name) # subset table (by name)\ntoph       &lt;- avatar %&gt;% pull(name) %&gt;% pluck(2) # single cell\n\nThe code below uses base R to produce the same subsets as the functions above. This format is useful to know about, since you might see them in other people’s scripts.\n\n# base R access\n\nsiblings   &lt;- avatar[c(1, 3), ] # rows (by number)\nbends      &lt;- avatar[, 2] # column vector (by number)\nfriendly   &lt;- avatar$friendly  # column vector (by name)\nbends_name &lt;- avatar[, c(\"bends\", \"name\")] # subset table (by name)\ntoph       &lt;- avatar[[2, 1]] # single cell (row, col)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "app-datatypes.html#sec-glossary-datatypes",
    "href": "app-datatypes.html#sec-glossary-datatypes",
    "title": "Appendix H — Data Types",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\nterm\ndefinition\n\n\n\nbase-r\nThe set of R functions that come with a basic installation of R, before you add external packages.\n\n\ncharacter\nA data type representing strings of text.\n\n\ncoercion\nChanging the data type of values in a vector to a single compatible type.\n\n\ndata-type\nThe kind of data represented by an object.\n\n\ndouble\nA data type representing a real decimal number\n\n\nescape\nInclude special characters like ” inside of a string by prefacing them with a backslash.\n\n\nextract-operator\nA symbol used to get values from a container object, such as [, [[, or $\n\n\nfactor\nA data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter\n\n\ninteger\nA data type representing whole numbers.\n\n\nlist\nA container data type that allows items with different data types to be grouped together.\n\n\nlogical\nA data type representing TRUE or FALSE values.\n\n\nnumeric\nA data type representing a real decimal number or integer.\n\n\noperator\nA symbol that performs some mathematical or comparative process.\n\n\ntabular-data\nData in a rectangular table format, where each row has an entry for each column.\n\n\ntidy-data\nA format for data that maps the meaning onto the structure.\n\n\nvector\nA type of data structure that collects values with the same data type, like T/F values, numbers, or strings.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "app-dates.html",
    "href": "app-dates.html",
    "title": "Appendix I — Dates and Times",
    "section": "",
    "text": "I.1 Formats\nWhile there is only one correct way to write date (The ISO 8601 format of “YYYY-MM-DD”), dates can be found in many formats. When you are reading a data file, you might need to specify the date format so it can be read properly. Date format specification uses abbreviations to represent the different ways people can write. the year, month, and day (as well as hours, minutes, and seconds). For example, the date 2023-01-03 is represented by the formatting string \"%Y-%m-%d. The fastest way to find the list of formatting abbreviations is to look in the help for the function col_date().\nRun in the console\n\n?col_date\n# create a table with some different date formats\ndate_formats &lt;- tibble(\n  best = \"2022-01-03\",\n  ok = \"2022 January 3\",\n  bad = \"January 3, 2022\",\n  terrible = \"Mon is 3 22 1\"\n)\n\n# save it as a CSV file\nwrite_csv(date_formats, \"data/date_formats.csv\")\n\n# read it in\ndf &lt;- read_csv(\"data/date_formats.csv\")\n\nRows: 1 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): ok, bad, terrible\ndate (1): best\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nYou can see that only the first column read as a date, and the rest read as characters. You can set the date format using the col_types argument and two helper functions, cols() and col_date().\nct &lt;- cols(ok = col_date(\"%Y %B %d\"),\n           bad = col_date(\"%B %d, %Y\"),\n           terrible = col_date(\"%a is %m %y %d\"))\n\nread_csv(\"data/date_formats.csv\", \n         col_types = ct)\n\n\n\n\nbest\nok\nbad\nterrible\n\n\n2022-01-03\n2022-01-03\n2022-01-03\n2022-03-01",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "app-dates.html#parsing",
    "href": "app-dates.html#parsing",
    "title": "Appendix I — Dates and Times",
    "section": "\nI.2 Parsing",
    "text": "I.2 Parsing\nThe ymd functions can deal with almost all date formats, regardless of the punctuation used in the format. All of the examples below produce a date in the standard format “2022-01-03”.\n\n# year-month-day orders\nymd(\"22 Jan 3\")\nymd(\"2022 January 3rd\")\n\n# month-day-year orders\nmdy(\"January 3, 2022\")\nmdy(\"Jan/03/22\")\n\n# day-month-year orders\ndmy(\"3JAN22\")\ndmy(\"3rd of January in the year 2022\")\n\n\nSee if you can make a date format that one of the parsers can’t handle.\n\nThere are similar functions for date/times, too.\n\nymd_hms(\"2022 Jan 3, 6:05 and 20s pm\")\nmdy_h(\"January 3rd, 2022 at 6pm\")\n\n[1] \"2022-01-03 18:05:20 UTC\"\n[1] \"2022-01-03 18:00:00 UTC\"\n\n\nThe date/time functions can also take a timezone argument. If you don’t specify it, it defaults to “UTC”.\n\nymd_hm(\"2022-01-03 18:05\", tz = \"GMT\")\n\n[1] \"2022-01-03 18:05:00 GMT\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "app-dates.html#get-parts",
    "href": "app-dates.html#get-parts",
    "title": "Appendix I — Dates and Times",
    "section": "\nI.3 Get Parts",
    "text": "I.3 Get Parts\nYou frequently need to extract parts of a date/time for plotting. The following functions extract specific parts of a date or datetime object. This is a godsend for those of us who never have a clue what week of the year it is today.\n\n# get the date and time when this function is run\nnow &lt;- now(tzone = \"GMT\")\n\n# get separate parts\ntime_parts &lt;- list(\n  second  = second(now),\n  minute  = minute(now),\n  hour    = hour(now),\n  day     = day(now),  # day of the month (same as mday())\n  wday    = wday(now), # day of the week\n  yday    = yday(now), # day of the year\n  week    = week(now),\n  isoweek = isoweek(now), # ISO 8501 week calendar (Monday start)\n  epiweek = epiweek(now), # CDC epidemiological week (Sunday Start)\n  month   = month(now),\n  year    = year(now),\n  tz      = tz(now)\n)\n\nstr(time_parts)\n\nList of 12\n $ second : num 23.6\n $ minute : int 40\n $ hour   : int 13\n $ day    : int 20\n $ wday   : num 6\n $ yday   : num 264\n $ week   : num 38\n $ isoweek: num 38\n $ epiweek: num 38\n $ month  : num 9\n $ year   : num 2024\n $ tz     : chr \"GMT\"\n\n\nThe month() and wday() functions can return factor labels.\n\njan1 &lt;- ymd(20220101)\nwday(jan1, label = TRUE)\nwday(jan1, label = TRUE, abbr = TRUE)\nmonth(jan1, label = TRUE)\nmonth(jan1, label = TRUE, abbr = TRUE)\n\n[1] Sat\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n[1] Sat\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n[1] Jan\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n[1] Jan\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\n\nWhat day of the week were you born?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nbirthdate &lt;- ymd(19761118) # put your own birthdate here\nwday(birthdate, label = TRUE)\n\n[1] Thu\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "app-dates.html#date-arithmetic",
    "href": "app-dates.html#date-arithmetic",
    "title": "Appendix I — Dates and Times",
    "section": "\nI.4 Date Arithmetic",
    "text": "I.4 Date Arithmetic\nYou can add and subtract dates. For example, you can get the dates two weeks from today by adding weeks(2) to today(). You can probably guess how to add and subtract seconds, minutes, days, months, and years.\n\ntoday() + weeks(1)\n\n[1] \"2024-09-27\"\n\n\n\nWhat day of the week will your 100th birthday be?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nbirthdate &lt;- ymd(19761118) # put your own birthdate here\ncentennial &lt;- birthdate + years(100)\nwday(centennial, label = TRUE, abbr = FALSE)\n\n[1] Wednesday\n7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhat do you think will happen if you subtract one month from March 31st? You get NA, since February doesn’t have a 31st day.\n\nymd(20220331) - months(1)\n\n[1] NA\n\n\nUse the special date operators %m+% and %m-% to add and subtract months without risking an impossible date.\n\nymd(20220331) %m-% months(1)\n\n[1] \"2022-02-28\"\n\n\n\n\n\nI.4.1 First and last of month\nFor things like billing, you might need to find the first or last days of the current, previous, or next month. The rollback() and rollforward() functions are easier than trying to parse dates.\n\nd &lt;- ymd(\"2022-01-24\")\nrollback(d)                          # last day of the previous month\nrollforward(d)                       # last day of the current month\nrollback(d, roll_to_first = TRUE)    # first day of the current month\nrollforward(d, roll_to_first = TRUE) # first day of the next month\n\n[1] \"2021-12-31\"\n[1] \"2022-01-31\"\n[1] \"2022-01-01\"\n[1] \"2022-02-01\"\n\n\n\nI.4.2 Rounding\nYou can round dates and times to the nearest unit. This can be useful when you have, for example, time measured to the nearest second, but want to group data by the nearest hour, rather than extract the hour component.\n\nymd_hm(\"2022-01-24 10:25\") %&gt;% round_date(unit = \"hour\")\nymd_hm(\"2022-01-24 10:30\") %&gt;% round_date(unit = \"hour\")\nymd_hm(\"2022-01-24 10:35\") %&gt;% round_date(unit = \"hour\")\n\n[1] \"2022-01-24 10:00:00 UTC\"\n[1] \"2022-01-24 11:00:00 UTC\"\n[1] \"2022-01-24 11:00:00 UTC\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "app-dates.html#internationalisation",
    "href": "app-dates.html#internationalisation",
    "title": "Appendix I — Dates and Times",
    "section": "\nI.5 Internationalisation",
    "text": "I.5 Internationalisation\nYou may need to work with dates from a different locale than your computer’s defaults, such as dates written in French or Russian. Or your computer may have a non-English locale. Set the locale argument to the relevant language code.\n\nymd(\"2022 January 24\", locale = \"en_GB\")\nymd(\"2022 Janvier 24\", locale = \"fr_FR\")\nwday(\"2022-01-03\", label = TRUE, locale = \"ru_RU\")\n\n[1] \"2022-01-24\"\n[1] \"2022-01-24\"\n[1] пн\nLevels: вс &lt; пн &lt; вт &lt; ср &lt; чт &lt; пт &lt; сб\n\n\nSome of the locale functions only work on unix-based machines, like Macs or machines running linux.\n\n# check your own locale; doesn't work for Windows\nlocale()\n\n&lt;locale&gt;\nNumbers:  123,456.78\nFormats:  %AD / %AT\nTimezone: UTC\nEncoding: UTF-8\n&lt;date_names&gt;\nDays:   Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday\n        (Thu), Friday (Fri), Saturday (Sat)\nMonths: January (Jan), February (Feb), March (Mar), April (Apr), May (May),\n        June (Jun), July (Jul), August (Aug), September (Sep), October\n        (Oct), November (Nov), December (Dec)\nAM/PM:  AM/PM\n\n\n\n# check which locales are available on your computer\n# doesn't work for Windows\nsystem(\"locale -a\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "app-dates.html#example",
    "href": "app-dates.html#example",
    "title": "Appendix I — Dates and Times",
    "section": "\nI.6 Example",
    "text": "I.6 Example\nLet’s work through some examples with downloaded tweets from the class data.\n\n# read all metrics files in data/tweets/\ntweets &lt;- list.files(\n  path = \"data/tweets\", \n  pattern = \"^tweet_activity_metrics\",\n  full.names = TRUE\n) %&gt;%\n  map_df(read_csv) %&gt;%\n  select(!starts_with(\"promoted\"))\n\nThe time column is already in date/time (POSIXct) format, but what if we wanted to plot tweets by hour for each day of the week?\n\ntweets %&gt;%\n  mutate(weekday = wday(time, label = TRUE),\n         hour = hour(time)) %&gt;%\n  ggplot(aes(x = hour, fill = weekday)) +\n  geom_bar(size = 1, alpha = 0.5, show.legend = FALSE) +\n  facet_grid(~weekday) +\n  scale_fill_manual(values = rainbow(7)) +\n  scale_x_continuous(breaks = seq(0, 24, 4))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nA nice side-effect of using the lubridate function to get days of the week or months of the year is that the results are an ordered factor, so display correctly in a plot. Let’s display the months in Greek (if that’s available on your system).\n\ntweets %&gt;%\n  mutate(month = month(time, label = TRUE, abbr = FALSE, locale = \"el_GR.UTF-8\")) %&gt;%\n  ggplot(aes(x = month, fill = month)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = NULL, guide = guide_axis(n.dodge=2))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Dates and Times</span>"
    ]
  },
  {
    "objectID": "app-styling.html",
    "href": "app-styling.html",
    "title": "Appendix J — Styling Plots",
    "section": "",
    "text": "J.1 Aesthetics",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Styling Plots</span>"
    ]
  },
  {
    "objectID": "app-styling.html#aesthetics",
    "href": "app-styling.html#aesthetics",
    "title": "Appendix J — Styling Plots",
    "section": "",
    "text": "J.1.1 Colour/Fill\nThe colour argument changes the point and line colour, while the fill argument changes the interior colour of shapes. Type colours() into the console to see a list of all the named colours in R. Alternatively, you can use hexadecimal colours like \"#FF8000\" or the rgb() function to set red, green, and blue values on a scale from 0 to 1.\nHover over a colour to see its R name.\n\n\n\nblack\n\n\ngray1\n\n\ngray2\n\n\ngray3\n\n\ngray4\n\n\ngray5\n\n\ngray6\n\n\ngray7\n\n\ngray8\n\n\ngray9\n\n\ngray10\n\n\ngray11\n\n\ngray12\n\n\ngray13\n\n\ngray14\n\n\ngray15\n\n\ngray16\n\n\ngray17\n\n\ngray18\n\n\ngray19\n\n\ngray20\n\n\ngray21\n\n\ngray22\n\n\ngray23\n\n\ngray24\n\n\ngray25\n\n\ngray26\n\n\ngray27\n\n\ngray28\n\n\ngray29\n\n\ngray30\n\n\ngray31\n\n\ngray32\n\n\ngray33\n\n\ngray34\n\n\ngray35\n\n\ngray36\n\n\ngray37\n\n\ngray38\n\n\ngray39\n\n\ngray40\n\n\ndimgray\n\n\ngray42\n\n\ngray43\n\n\ngray44\n\n\ngray45\n\n\ngray46\n\n\ngray47\n\n\ngray48\n\n\ngray49\n\n\ngray50\n\n\ngray51\n\n\ngray52\n\n\ngray53\n\n\ngray54\n\n\ngray55\n\n\ngray56\n\n\ngray57\n\n\ngray58\n\n\ngray59\n\n\ngray60\n\n\ngray61\n\n\ngray62\n\n\ngray63\n\n\ngray64\n\n\ngray65\n\n\ndarkgray\n\n\ngray66\n\n\ngray67\n\n\ngray68\n\n\ngray69\n\n\ngray70\n\n\ngray71\n\n\ngray72\n\n\ngray73\n\n\ngray74\n\n\ngray\n\n\ngray75\n\n\ngray76\n\n\ngray77\n\n\ngray78\n\n\ngray79\n\n\ngray80\n\n\ngray81\n\n\ngray82\n\n\ngray83\n\n\nlightgray\n\n\ngray84\n\n\ngray85\n\n\ngainsboro\n\n\ngray86\n\n\ngray87\n\n\ngray88\n\n\ngray89\n\n\ngray90\n\n\ngray91\n\n\ngray92\n\n\ngray93\n\n\ngray94\n\n\ngray95\n\n\ngray96\n\n\ngray97\n\n\ngray98\n\n\ngray99\n\n\nwhite\n\n\nsnow4\n\n\nsnow3\n\n\nsnow2\n\n\nsnow\n\n\nrosybrown4\n\n\nrosybrown\n\n\nrosybrown3\n\n\nrosybrown2\n\n\nrosybrown1\n\n\nlightcoral\n\n\nindianred\n\n\nindianred4\n\n\nindianred2\n\n\nindianred1\n\n\nindianred3\n\n\nbrown4\n\n\nbrown\n\n\nbrown3\n\n\nbrown2\n\n\nbrown1\n\n\nfirebrick4\n\n\nfirebrick\n\n\nfirebrick3\n\n\nfirebrick1\n\n\nfirebrick2\n\n\ndarkred\n\n\nred3\n\n\nred2\n\n\nred\n\n\nmistyrose3\n\n\nmistyrose4\n\n\nmistyrose2\n\n\nmistyrose\n\n\nsalmon\n\n\ntomato3\n\n\ncoral4\n\n\ncoral3\n\n\ncoral2\n\n\ncoral1\n\n\ntomato2\n\n\ntomato\n\n\ntomato4\n\n\ndarksalmon\n\n\nsalmon4\n\n\nsalmon3\n\n\nsalmon2\n\n\nsalmon1\n\n\ncoral\n\n\norangered4\n\n\norangered3\n\n\norangered2\n\n\nlightsalmon3\n\n\nlightsalmon2\n\n\nlightsalmon\n\n\nlightsalmon4\n\n\nsienna\n\n\nsienna3\n\n\nsienna2\n\n\nsienna1\n\n\nsienna4\n\n\norangered\n\n\nseashell4\n\n\nseashell3\n\n\nseashell2\n\n\nseashell\n\n\nchocolate4\n\n\nchocolate3\n\n\nchocolate\n\n\nchocolate2\n\n\nchocolate1\n\n\nlinen\n\n\npeachpuff4\n\n\npeachpuff3\n\n\npeachpuff2\n\n\npeachpuff\n\n\nsandybrown\n\n\ntan4\n\n\nperu\n\n\ntan2\n\n\ntan1\n\n\ndarkorange4\n\n\ndarkorange3\n\n\ndarkorange2\n\n\ndarkorange1\n\n\nantiquewhite3\n\n\nantiquewhite2\n\n\nantiquewhite1\n\n\nbisque4\n\n\nbisque3\n\n\nbisque2\n\n\nbisque\n\n\nburlywood4\n\n\nburlywood3\n\n\nburlywood\n\n\nburlywood2\n\n\nburlywood1\n\n\ndarkorange\n\n\nantiquewhite4\n\n\nantiquewhite\n\n\npapayawhip\n\n\nblanchedalmond\n\n\nnavajowhite4\n\n\nnavajowhite3\n\n\nnavajowhite2\n\n\nnavajowhite\n\n\ntan\n\n\nfloralwhite\n\n\noldlace\n\n\nwheat4\n\n\nwheat3\n\n\nwheat2\n\n\nwheat\n\n\nwheat1\n\n\nmoccasin\n\n\norange4\n\n\norange3\n\n\norange2\n\n\norange\n\n\ngoldenrod\n\n\ngoldenrod1\n\n\ngoldenrod4\n\n\ngoldenrod3\n\n\ngoldenrod2\n\n\ndarkgoldenrod4\n\n\ndarkgoldenrod\n\n\ndarkgoldenrod3\n\n\ndarkgoldenrod2\n\n\ndarkgoldenrod1\n\n\ncornsilk\n\n\ncornsilk4\n\n\ncornsilk3\n\n\ncornsilk2\n\n\nlightgoldenrod4\n\n\nlightgoldenrod3\n\n\nlightgoldenrod\n\n\nlightgoldenrod2\n\n\nlightgoldenrod1\n\n\ngold4\n\n\ngold3\n\n\ngold2\n\n\ngold\n\n\nlemonchiffon4\n\n\nlemonchiffon3\n\n\nlemonchiffon2\n\n\nlemonchiffon\n\n\npalegoldenrod\n\n\nkhaki\n\n\ndarkkhaki\n\n\nkhaki4\n\n\nkhaki3\n\n\nkhaki2\n\n\nkhaki1\n\n\nivory4\n\n\nivory3\n\n\nivory2\n\n\nivory\n\n\nbeige\n\n\nlightyellow4\n\n\nlightyellow3\n\n\nlightyellow2\n\n\nlightyellow\n\n\nlightgoldenrodyellow\n\n\nyellow4\n\n\nyellow3\n\n\nyellow2\n\n\nyellow\n\n\nolivedrab\n\n\nolivedrab4\n\n\nolivedrab3\n\n\nolivedrab2\n\n\nolivedrab1\n\n\ndarkolivegreen\n\n\ndarkolivegreen4\n\n\ndarkolivegreen3\n\n\ndarkolivegreen2\n\n\ndarkolivegreen1\n\n\ngreenyellow\n\n\nchartreuse4\n\n\nchartreuse3\n\n\nchartreuse2\n\n\nlawngreen\n\n\nchartreuse\n\n\nhoneydew4\n\n\nhoneydew3\n\n\nhoneydew2\n\n\nhoneydew\n\n\ndarkseagreen4\n\n\ndarkseagreen\n\n\ndarkseagreen3\n\n\ndarkseagreen2\n\n\ndarkseagreen1\n\n\nlightgreen\n\n\npalegreen\n\n\npalegreen4\n\n\npalegreen3\n\n\npalegreen1\n\n\nforestgreen\n\n\nlimegreen\n\n\ndarkgreen\n\n\ngreen4\n\n\ngreen3\n\n\ngreen2\n\n\ngreen\n\n\nmediumseagreen\n\n\nseagreen\n\n\nseagreen3\n\n\nseagreen2\n\n\nseagreen1\n\n\nmintcream\n\n\nspringgreen4\n\n\nspringgreen3\n\n\nspringgreen2\n\n\nspringgreen\n\n\naquamarine3\n\n\naquamarine2\n\n\naquamarine\n\n\nmediumspringgreen\n\n\naquamarine4\n\n\nturquoise\n\n\nmediumturquoise\n\n\nlightseagreen\n\n\nazure4\n\n\nazure3\n\n\nazure2\n\n\nazure\n\n\nlightcyan4\n\n\nlightcyan3\n\n\nlightcyan2\n\n\nlightcyan\n\n\npaleturquoise\n\n\npaleturquoise4\n\n\npaleturquoise3\n\n\npaleturquoise2\n\n\npaleturquoise1\n\n\ndarkslategray\n\n\ndarkslategray4\n\n\ndarkslategray3\n\n\ndarkslategray2\n\n\ndarkslategray1\n\n\ncyan4\n\n\ncyan3\n\n\ndarkturquoise\n\n\ncyan2\n\n\ncyan\n\n\ncadetblue4\n\n\ncadetblue\n\n\nturquoise4\n\n\nturquoise3\n\n\nturquoise2\n\n\nturquoise1\n\n\npowderblue\n\n\ncadetblue3\n\n\ncadetblue2\n\n\ncadetblue1\n\n\nlightblue4\n\n\nlightblue3\n\n\nlightblue\n\n\nlightblue2\n\n\nlightblue1\n\n\ndeepskyblue4\n\n\ndeepskyblue3\n\n\ndeepskyblue2\n\n\ndeepskyblue\n\n\nskyblue\n\n\nlightskyblue4\n\n\nlightskyblue3\n\n\nlightskyblue2\n\n\nlightskyblue1\n\n\nlightskyblue\n\n\nskyblue4\n\n\nskyblue3\n\n\nskyblue2\n\n\nskyblue1\n\n\naliceblue\n\n\nslategray\n\n\nlightslategray\n\n\nslategray3\n\n\nslategray2\n\n\nslategray1\n\n\nsteelblue4\n\n\nsteelblue\n\n\nsteelblue3\n\n\nsteelblue2\n\n\nsteelblue1\n\n\ndodgerblue4\n\n\ndodgerblue3\n\n\ndodgerblue2\n\n\ndodgerblue\n\n\nlightsteelblue4\n\n\nlightsteelblue3\n\n\nlightsteelblue\n\n\nlightsteelblue2\n\n\nlightsteelblue1\n\n\nslategray4\n\n\ncornflowerblue\n\n\nroyalblue\n\n\nroyalblue4\n\n\nroyalblue3\n\n\nroyalblue2\n\n\nroyalblue1\n\n\nghostwhite\n\n\nlavender\n\n\nmidnightblue\n\n\nnavy\n\n\nblue4\n\n\nblue3\n\n\nblue2\n\n\nblue\n\n\ndarkslateblue\n\n\nslateblue\n\n\nmediumslateblue\n\n\nlightslateblue\n\n\nslateblue1\n\n\nslateblue4\n\n\nslateblue3\n\n\nslateblue2\n\n\nmediumpurple4\n\n\nmediumpurple3\n\n\nmediumpurple\n\n\nmediumpurple2\n\n\nmediumpurple1\n\n\npurple4\n\n\npurple3\n\n\nblueviolet\n\n\npurple1\n\n\npurple2\n\n\npurple\n\n\ndarkorchid\n\n\ndarkorchid4\n\n\ndarkorchid3\n\n\ndarkorchid2\n\n\ndarkorchid1\n\n\ndarkviolet\n\n\nmediumorchid4\n\n\nmediumorchid3\n\n\nmediumorchid\n\n\nmediumorchid2\n\n\nmediumorchid1\n\n\nthistle4\n\n\nthistle3\n\n\nthistle\n\n\nthistle2\n\n\nthistle1\n\n\nplum4\n\n\nplum3\n\n\nplum2\n\n\nplum1\n\n\nplum\n\n\nviolet\n\n\ndarkmagenta\n\n\nmagenta3\n\n\nmagenta2\n\n\nmagenta\n\n\norchid4\n\n\norchid3\n\n\norchid\n\n\norchid2\n\n\norchid1\n\n\nmaroon4\n\n\nvioletred\n\n\nmaroon3\n\n\nmaroon2\n\n\nmaroon1\n\n\nmediumvioletred\n\n\ndeeppink3\n\n\ndeeppink2\n\n\ndeeppink\n\n\ndeeppink4\n\n\nhotpink2\n\n\nhotpink1\n\n\nhotpink4\n\n\nhotpink\n\n\nvioletred4\n\n\nvioletred3\n\n\nvioletred2\n\n\nvioletred1\n\n\nhotpink3\n\n\nlavenderblush4\n\n\nlavenderblush3\n\n\nlavenderblush2\n\n\nlavenderblush\n\n\nmaroon\n\n\npalevioletred4\n\n\npalevioletred3\n\n\npalevioletred\n\n\npalevioletred2\n\n\npalevioletred1\n\n\npink4\n\n\npink3\n\n\npink2\n\n\npink1\n\n\npink\n\n\nlightpink\n\n\nlightpink4\n\n\nlightpink3\n\n\nlightpink2\n\n\nlightpink1\n\n\nJ.1.2 Alpha\nThe alpha argument changes transparency (0 = totally transparent, 1 = totally opaque).\n\n\n\n\nVarying alpha values.\n\n\n\n\nJ.1.3 Shape\nThe shape argument changes the shape of points.\n\n\n\n\nThe 25 shape values\n\n\n\n\nJ.1.4 Linetype\nYou can probably guess what the linetype argument does.\n\n\n\n\nThe 6 linetype values at different sizes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Styling Plots</span>"
    ]
  },
  {
    "objectID": "app-styling.html#palettes",
    "href": "app-styling.html#palettes",
    "title": "Appendix J — Styling Plots",
    "section": "\nJ.2 Palettes",
    "text": "J.2 Palettes\nDiscrete palettes change depending on the number of categories.\n\n\n\n\nDefault discrete palette with different numbers of levels.\n\n\n\n\nJ.2.1 Viridis Palettes\nViridis palettes are very good for colourblind-safe and greyscale-safe plots. The work with any number of categories, but are best for larger numbers of categories or continuous colours.\n\nJ.2.1.1 Discrete Viridis Palettes\nSet discrete viridis colours with scale_colour_viridis_d() or scale_fill_viridis_d() and set the option argument to one of the options below. Set direction = -1 to reverse the order of colours.\n\n\n\n\nDiscrete viridis palettes.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the end colour is too light for your plot or the start colour too dark, you can set the begin and end arguments to values between 0 and 1, such as scale_colour_viridis_c(begin = .1, end = .9).\n\n\n\nJ.2.1.2 Continuous Viridis Palettes\nSet continuous viridis colours with scale_colour_viridis_c() or scale_fill_viridis_c() and set the option argument to one of the options below. Set direction = -1 to reverse the order of colours.\n\n\n\n\nContinuous viridis palettes.\n\n\n\n\nJ.2.2 Brewer Palettes\nBrewer palettes give you a lot of control over plot colour and fill. You set them with scale_color_brewer() or scale_fill_brewer() and set the palette argument to one of the palettes below. Set direction = -1 to reverse the order of colours.\n\nJ.2.2.1 Qualitative Brewer Palettes\nThese palettes are good for categorical data with up to 8 categories (some palettes can handle up to 12). The “Paired” palette is useful if your categories are arranged in pairs.\n\n\n\n\nQualitative brewer palettes.\n\n\n\n\nJ.2.2.2 Sequential Brewer Palettes\nThese palettes are good for up to 9 ordinal categories with a lot of categories.\n\n\n\n\nSequential brewer palettes.\n\n\n\n\nJ.2.2.3 Diverging Brewer Palettes\nThese palettes are good for ordinal categories with up to 11 levels where the centre level is a neutral or baseline category and the levels above and below it differ in an important way, such as agree versus disagree options.\n\n\n\n\nDiverging brewer palettes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Styling Plots</span>"
    ]
  },
  {
    "objectID": "app-styling.html#sec-themes-appendix",
    "href": "app-styling.html#sec-themes-appendix",
    "title": "Appendix J — Styling Plots",
    "section": "\nJ.3 Themes",
    "text": "J.3 Themes\nggplot2 has 8 built-in themes that you can add to a plot like plot + theme_bw() or set as the default theme at the top of your script like theme_set(theme_bw()).\n\n\n\n\n{ggplot2} themes.\n\n\n\n\nJ.3.1 ggthemes\nYou can get more themes from add-on packages, like ggthemes”, “https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/”). Most of the themes also have customscale_functions likescale_colour_economist()`. Their website has extensive examples and instructions for alternate or dark versions of these themes.\n\n\n\n\n{ggthemes} themes.\n\n\n\n\nJ.3.2 Fonts\nYou can customise the fonts used in themes. All computers should be able to recognise the families “sans”, “serif”, and “mono”, and some computers will be able to access other installed fonts by name.\n\nsans &lt;- g + theme_bw(base_family = \"sans\") + \n  ggtitle(\"Sans\")\nserif &lt;- g + theme_bw(base_family = \"serif\") + \n  ggtitle(\"Serif\")\nmono &lt;- g + theme_bw(base_family = \"mono\") + \n  ggtitle(\"Mono\")\nfont &lt;- g + theme_bw(base_family = \"Comic Sans MS\") + \n  ggtitle(\"Comic Sans MS\")\n\nsans + serif + mono + font + plot_layout(nrow = 1)\n\n\n\nDifferent fonts.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are working on a Windows machine and get the error “font family not found in Windows font database”, you may need to explicitly map the fonts. In your setup code chunk, add the following code, which should fix the error. You may need to do this for any fonts that you specify.\n\n\nThe showtext package is a flexible way to add fonts.\nIf you have a .ttf file from a font site, like Font Squirrel, you can load the file directly using font_add(). Set regular as the path to the file for the regular version of the font, and optionally add other versions. Set the family to the name you want to use for the font. You will need to include any local font files if you are sharing your script with others.\n\nlibrary(showtext)\n\n# font from https://www.fontsquirrel.com/fonts/SF-Cartoonist-Hand\n\nfont_add(\n  regular = \"fonts/cartoonist/SF_Cartoonist_Hand.ttf\",\n  bold = \"fonts/cartoonist/SF_Cartoonist_Hand_Bold.ttf\",\n  italic = \"fonts/cartoonist/SF_Cartoonist_Hand_Italic.ttf\",\n  bolditalic = \"fonts/cartoonist/SF_Cartoonist_Hand_Bold_Italic.ttf\",\n  family = \"cartoonist\" \n)\n\nTo download fonts directly from Google fonts, use the function font_add_google(), set the name to the exact name from the site, and the family to the name you want to use for the font.\n\n# download fonts from Google\nfont_add_google(name = \"Courgette\", family = \"courgette\")\nfont_add_google(name = \"Poiret One\", family = \"poiret\")\n\nAfter you’ve added fonts from local files or Google, you need to make them available to R using showtext_auto(). You will have to do these steps in each script where you want to use the custom fonts.\n\nshowtext_auto() # load the fonts\n\nTo change the fonts used overall in a plot, use the theme() function and set text to element_text(family = \"new_font_family\").\n\na &lt;- g + theme(text = element_text(family = \"courgette\")) +\n  ggtitle(\"Courgette\")\nb &lt;- g + theme(text = element_text(family = \"cartoonist\")) +\n  ggtitle(\"Cartoonist Hand\")\nc &lt;- g + theme(text = element_text(family = \"poiret\")) +\n  ggtitle(\"Poiret One\")\n\na + b + c\n\n\n\nCustom Fonts.\n\n\n\nTo set the fonts for individual elements in the plot, you need to find the specific argument for that element. You can use the argument face to choose “bold”, “italic”, or “bolditalic” versions, if they are available.\n\ng + ggtitle(\"Cartoonist Hand\") +\n  theme(\n    title = element_text(family = \"cartoonist\", face = \"bold\"),\n    strip.text = element_text(family = \"cartoonist\", face = \"italic\"),\n    axis.text = element_text(family = \"sans\")\n  )\n\n\n\nMultiple custom fonts on the same plot.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Styling Plots</span>"
    ]
  },
  {
    "objectID": "app-webpage.html",
    "href": "app-webpage.html",
    "title": "Appendix K — Webpages",
    "section": "",
    "text": "K.1 Create a webpage",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Webpages</span>"
    ]
  },
  {
    "objectID": "app-webpage.html#sec-webpage-create",
    "href": "app-webpage.html#sec-webpage-create",
    "title": "Appendix K — Webpages",
    "section": "",
    "text": "K.1.1 Create a project\n\nChoose New Project... from the File menu (don’t save any workspaces)\nChoose New Directory &gt; Simple R Markdown Website\n\nSet your project name to “mywebpage”\n\nK.1.2 Site header\nThis is where you can set options like whether to show a table of contents and what the navigation bar will look like. We’ll edit this later to add a section menu.\n\nOpen the file _site.yml\n\nReplace the text with the following:\nname: \"mywebpage\"  \nauthor: \"YOUR NAME\"  \noutput_dir: \"docs\"  \noutput:  \n  html_document:  \n    self_contained: no  \n    theme: \n      version: 4\n      bootswatch: yeti \nnavbar:  \n  title: \"My First Webpage\"  \n    left:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n\nSave the file (do not change the name)\n\nK.1.3 Edit the pages\nEdit the text in the index.Rmd and about.Rmd pages. You can use R markdown, including code chunks.\n\nK.1.4 Render the site\nIn the upper right “Build” pane, click on the “Build website” hammer icon. This will render the website and automatically open it in a browser window. Alternatively, type the following into the Console pane:\n\nbrowseURL(rmarkdown::render_site(encoding = 'UTF-8'))\n\nIf you accidentally close the website and want to look at it again, you don’t have to re-render it. Click on the docs directory in the Files tab of the lower right pane, then click on index.html and choose View in Web Browser.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Webpages</span>"
    ]
  },
  {
    "objectID": "app-webpage.html#sec-webpage-pages",
    "href": "app-webpage.html#sec-webpage-pages",
    "title": "Appendix K — Webpages",
    "section": "\nK.2 Add pages",
    "text": "K.2 Add pages\n\nCreate a new .Rmd file for each webpage\nAdd content to the webpages using R Markdown\nRe-render the site\n\nIf you include linked content like image files, make sure they are copied to your main project directory and linked using relative paths.\nTo get your webpage online, copy the contents of the docs directory to a web server. If you don’t have access to a web server, you can make free websites using a GitHub repository and GitHub Pages).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Webpages</span>"
    ]
  },
  {
    "objectID": "app-webpage.html#sec-webpage-styles",
    "href": "app-webpage.html#sec-webpage-styles",
    "title": "Appendix K — Webpages",
    "section": "\nK.3 Styles",
    "text": "K.3 Styles\nYou can change the appearance of your website by changing the theme in the _site.yml file (see Appendix J), but the instructions below will help you to customise things even further.\n\nK.3.1 Add custom styles\nYou can add a custom style sheet (a document that determines how each element of your website should look) by adding the line css: style.css under html_document: in the _site.yml file.\noutput:  \n  html_document:  \n    self_contained: no  \n    theme: \n      version: 4\n      bootswatch: readable\n    css: style.css\nThen you need to create a file named style.css and add your custom styles there. The web has thousands of guides to CSS, but codeacademy has great interactive tutorials for learning html, css, and even more advanced web coding like javascript.\nHowever, the basics of css are easy to learn and it’s best to just start playing around with it. Add the following text to your style.css file and re-render the website.\n\nK.3.2 Change global fonts and colours\nbody {\n  font-size: 2em;\n  font-family: \"Times New Roman\";\n  color: white;\n  background-color: #660000;\n}\n\n\nThis will make the text on your website larger, a different font, and change the text and background colours.\n\nThe theme you’re using might have css that blocks the styles you’re trying to change. You can add !important before the end colon to override that.\n\nK.3.3 Change certain elements\nMaybe you only want to change the font colour for your headings, not the rest of the text. You can apply a style to a specific element type by specifying the element name before the curly brackets.\nh1, h2, h3 {\n  text-align: center;\n  color: hsl(0, 100%, 20%);\n}\n\nh3 {\n  font-style: italic;\n}\n\np {\n  border: 1px solid green;\n  padding: 10px;\n  line-height: 2;\n}\n\nul {\n  border: 3px dotted red;\n  border-radius: 10px;\n  padding: 10px 30px;\n}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Webpages</span>"
    ]
  },
  {
    "objectID": "app-webpage.html#example-using-the-styles-above",
    "href": "app-webpage.html#example-using-the-styles-above",
    "title": "Appendix K — Webpages",
    "section": "\nK.4 Example using the styles above",
    "text": "K.4 Example using the styles above\nThe CSS above changes the styles for three levels of headers (h2, h3, h4) and sets the third level to italics.\n\nK.4.1 Level 3 header\nIt also gives paragraphs (p) a green border and double-spacing.\n\nK.4.1.1 Level 4 header\nUnordered Lists (ul) get:\n\ndotted red border\nround corners\nincreased padding on top (10px) and sides (30px)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Webpages</span>"
    ]
  }
]